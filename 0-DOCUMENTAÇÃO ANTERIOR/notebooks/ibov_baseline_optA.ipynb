{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59be0813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ibov_baseline_optA cell ready. Run run_optA(n_splits=5) to execute.\n"
     ]
    }
   ],
   "source": [
    "# ibov_baseline_optA: 3-class XGBoost baseline (argmax decision after calibration)\n",
    "# Instruções: copie/cole esta célula inteira em um notebook novo e execute no kernel do .venv.\n",
    "# Saídas: metrics_ibov_optA.csv, partitions_ibov.csv, feature_list_d{h}.txt, PROVENANCE_IBOV.txt append.\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json, logging, hashlib, platform, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef, f1_score\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "GOLD_DIR = Path('/home/wrm/BOLSA_2026/gold/IBOV')\n",
    "GOLD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "GOLD_PARQUET = GOLD_DIR / 'gold_ibov_features.parquet'\n",
    "PROV = GOLD_DIR / 'PROVENANCE_IBOV.txt'\n",
    "PARTITIONS_CSV = GOLD_DIR / 'partitions_ibov.csv'\n",
    "METRICS_OPT_A = GOLD_DIR / 'metrics_ibov_optA.csv'\n",
    "CALIB_PNG_DIR = GOLD_DIR\n",
    "\n",
    "HYPER = {\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'n_estimators': 400,\n",
    "    'learning_rate': 0.08,\n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'hist',\n",
    "}\n",
    "EMBARGO = 5\n",
    "HORIZONS = (1,3,5)\n",
    "\n",
    "def sha256_of_file(p: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with p.open('rb') as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b''):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def load_gold(p: Path) -> pd.DataFrame:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(p)\n",
    "    return pd.read_parquet(p)\n",
    "\n",
    "def select_features_and_save(df: pd.DataFrame):\n",
    "    banned_prefix = ('r_d','y_d','k_d')\n",
    "    banned_exact = {'date'}\n",
    "    banned_contains = {'ticker'}\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    features = [c for c in numeric_cols if not any(c.startswith(bp) for bp in banned_prefix) and c not in banned_exact and not any(b in c.lower() for b in banned_contains)]\n",
    "    for h in HORIZONS:\n",
    "        (GOLD_DIR / f'feature_list_d{h}.txt').write_text('\\n'.join(features))\n",
    "    return {h: features for h in HORIZONS}\n",
    "\n",
    "def make_partitions(df: pd.DataFrame, n_splits=5):\n",
    "    dates = pd.to_datetime(df['date']).sort_values().unique()\n",
    "    n = len(dates)\n",
    "    k = min(max(4, n_splits), 6)\n",
    "    block_size = max(1, n // k)\n",
    "    parts = []\n",
    "    for i in range(k):\n",
    "        test_start_idx = i * block_size\n",
    "        test_end_idx = min(n-1, (i+1)*block_size -1) if i < k-1 else n-1\n",
    "        test_start = dates[test_start_idx]\n",
    "        test_end = dates[test_end_idx]\n",
    "        train_start = dates[0]\n",
    "        train_end = dates[max(0, test_start_idx-1)]\n",
    "        embargo_start = dates[max(0, test_start_idx-EMBARGO)] if test_start_idx-EMBARGO >=0 else train_end\n",
    "        embargo_end = train_end\n",
    "        parts.append({'block_id': i+1, 'train_start': str(train_start.date()), 'train_end': str(train_end.date()), 'embargo_start': str(embargo_start.date()) if embargo_start is not None else None, 'embargo_end': str(embargo_end.date()) if embargo_end is not None else None, 'test_start': str(test_start.date()), 'test_end': str(test_end.date())})\n",
    "    pdf = pd.DataFrame(parts)\n",
    "    pdf.to_csv(PARTITIONS_CSV, index=False)\n",
    "    logging.info(f'Partitions saved: {PARTITIONS_CSV}')\n",
    "    return pdf\n",
    "\n",
    "def compute_class_weights(y):\n",
    "    classes = np.unique(y.dropna())\n",
    "    try:\n",
    "        cw = compute_class_weight(class_weight='balanced', classes=classes, y=y.dropna())\n",
    "        m = {c: float(w) for c,w in zip(classes, cw)}\n",
    "    except Exception:\n",
    "        m = {c: 1.0 for c in classes}\n",
    "    if 1 in m:\n",
    "        m[1] = m[1] * 1.15\n",
    "    return m\n",
    "\n",
    "def choose_calibrator_and_fit(clf, X_cal, y_cal):\n",
    "    best = (None, None, float('inf'))\n",
    "    for method in ['sigmoid','isotonic']:\n",
    "        try:\n",
    "            cal = CalibratedClassifierCV(clf, cv='prefit', method=method)\n",
    "            cal.fit(X_cal, y_cal)\n",
    "            probs = cal.predict_proba(X_cal)\n",
    "            eps = 1e-15\n",
    "            ll = -np.mean(np.log(np.maximum(eps, probs[np.arange(len(y_cal)), y_cal])))\n",
    "            if ll < best[2]:\n",
    "                best = (method, cal, ll)\n",
    "        except Exception as e:\n",
    "            logging.debug(f'Calibration {method} failed: {e}')\n",
    "    if best[0] is None:\n",
    "        return None, None\n",
    "    return best[0], best[1]\n",
    "\n",
    "def train_and_eval_optA(df, feat_map, partitions):\n",
    "    rows = []\n",
    "    for h in HORIZONS:\n",
    "        features = feat_map[h]\n",
    "        for _, prow in partitions.iterrows():\n",
    "            bid = int(prow['block_id'])\n",
    "            test_mask = (pd.to_datetime(df['date']).dt.date >= pd.to_datetime(prow['test_start']).date()) & (pd.to_datetime(df['date']).dt.date <= pd.to_datetime(prow['test_end']).date())\n",
    "            train_mask = (pd.to_datetime(df['date']).dt.date >= pd.to_datetime(prow['train_start']).date()) & (pd.to_datetime(df['date']).dt.date <= pd.to_datetime(prow['train_end']).date())\n",
    "            X_tr = df.loc[train_mask, features].copy()\n",
    "            y_tr = df.loc[train_mask, f'y_d{h}_cls'].copy()\n",
    "            X_te = df.loc[test_mask, features].copy()\n",
    "            y_te = df.loc[test_mask, f'y_d{h}_cls'].copy()\n",
    "            if y_tr.dropna().empty or y_te.dropna().empty:\n",
    "                logging.warning(f'Empty train/test for horizon {h} block {bid} — skipping')\n",
    "                continue\n",
    "            cw = compute_class_weights(y_tr)\n",
    "            notna_tr = y_tr.notna()\n",
    "            X_tr_enc = X_tr.loc[notna_tr]\n",
    "            y_tr_enc = y_tr.loc[notna_tr].astype(int)\n",
    "            le = LabelEncoder()\n",
    "            y_tr_le = le.fit_transform(y_tr_enc)\n",
    "            sample_w = y_tr.loc[notna_tr].map(lambda v: cw.get(v,1.0)).values\n",
    "            if len(np.unique(y_tr_le)) < 2:\n",
    "                logging.warning(f'Not enough classes in train for h{h} b{bid} — skipping')\n",
    "                continue\n",
    "            clf = xgb.XGBClassifier(**HYPER, objective='multi:softprob', num_class=len(le.classes_))\n",
    "            clf.fit(X_tr_enc, y_tr_le, sample_weight=sample_w)\n",
    "            # calibration\n",
    "            if len(X_tr_enc) < 50:\n",
    "                calibrated = clf\n",
    "                cal_name = 'none'\n",
    "            else:\n",
    "                Xtr_sub, Xcal, ytr_sub, ycal = train_test_split(X_tr_enc, y_tr_le, test_size=0.1, random_state=SEED, stratify=y_tr_le)\n",
    "                clf_inner = xgb.XGBClassifier(**HYPER, objective='multi:softprob', num_class=len(le.classes_))\n",
    "                clf_inner.fit(Xtr_sub, ytr_sub)\n",
    "                method, cal = choose_calibrator_and_fit(clf_inner, Xcal, ycal)\n",
    "                if cal is None:\n",
    "                    calibrated = clf_inner\n",
    "                    cal_name = 'none'\n",
    "                else:\n",
    "                    calibrated = cal\n",
    "                    cal_name = method\n",
    "            # prepare test rows where label seen in train\n",
    "            test_notna = y_te.notna()\n",
    "            X_te_enc = X_te.loc[test_notna] if not X_te.empty else X_te\n",
    "            y_te_orig = y_te.loc[test_notna].astype(int) if not y_te.empty else y_te\n",
    "            # map only labels present in train\n",
    "            mapping = {orig: enc for enc, orig in enumerate(le.classes_)}\n",
    "            keep = []\n",
    "            for i,v in enumerate(y_te_orig.values if len(y_te_orig)>0 else []):\n",
    "                if v in mapping:\n",
    "                    keep.append(i)\n",
    "            if len(keep) == 0:\n",
    "                logging.warning(f'No test rows with seen labels for h{h} b{bid} — skipping')\n",
    "                continue\n",
    "            X_te_enc = X_te_enc.iloc[keep]\n",
    "            y_te_aligned = y_te_orig.iloc[keep].values\n",
    "            probs = calibrated.predict_proba(X_te_enc)\n",
    "            # argmax decision over all classes (decoded back to original labels)\n",
    "            # calibrated.classes_ are encoded classes; map back using le.inverse_transform\n",
    "            pred_enc = np.argmax(probs, axis=1)\n",
    "            pred_orig = le.inverse_transform(pred_enc)\n",
    "            # diagnostics: proportion of zeros predicted\n",
    "            prop_zero = float((pred_orig == 0).mean())\n",
    "            acc_bal = balanced_accuracy_score(y_te_aligned, pred_orig)\n",
    "            try:\n",
    "                mcc = matthews_corrcoef(y_te_aligned, pred_orig)\n",
    "            except Exception:\n",
    "                mcc = float('nan')\n",
    "            f1_up = f1_score(y_te_aligned, pred_orig, labels=[1], average='macro', zero_division=0)\n",
    "            f1_down = f1_score(y_te_aligned, pred_orig, labels=[-1], average='macro', zero_division=0)\n",
    "            rows.append({'horizon': h, 'block_id': bid, 'acc_bal': float(acc_bal), 'mcc_macro': float(mcc), 'f1_up': float(f1_up), 'f1_down': float(f1_down), 'n_test': int(len(y_te_aligned)), 'prop_zero': prop_zero, 'calibrator': cal_name})\n",
    "            # optional calibration plot for 'up' class if present\n",
    "            try:\n",
    "                if 1 in mapping:\n",
    "                    idx_up = list(calibrated.classes_).index(mapping[1]) if mapping[1] in list(calibrated.classes_) else None\n",
    "                    if idx_up is not None:\n",
    "                        p_up = probs[:, idx_up]\n",
    "                        frac_pos, mean_pred = calibration_curve((y_te_aligned == 1).astype(int), p_up, n_bins=10)\n",
    "                        plt.figure()\n",
    "                        plt.plot(mean_pred, frac_pos, marker='o')\n",
    "                        plt.plot([0,1],[0,1], linestyle='--', color='k')\n",
    "                        plt.xlabel('Mean predicted prob (up)')\n",
    "                        plt.ylabel('Fraction of positives')\n",
    "                        plt.title(f'Calibration up: h{h} block{bid}')\n",
    "                        png = CALIB_PNG_DIR / f'calibration_optA_d{h}_block{bid}.png'\n",
    "                        plt.savefig(png)\n",
    "                        plt.close()\n",
    "            except Exception as e:\n",
    "                logging.debug(f'Cal plot failed: {e}')\n",
    "\n",
    "    # write metrics and provenance\n",
    "    pd.DataFrame(rows).to_csv(METRICS_OPT_A, index=False)\n",
    "    prov = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'mode': 'OptA argmax 3-classes',\n",
    "        'seed': SEED,\n",
    "        'hyperparameters': HYPER,\n",
    "        'class_weight_note': 'positive class weight *1.15 if present',\n",
    "        'metrics_file': str(METRICS_OPT_A),\n",
    "        'partitions_file': str(PARTITIONS_CSV),\n",
    "        'gold_sha256': sha256_of_file(GOLD_PARQUET) if GOLD_PARQUET.exists() else None,\n",
    "        'python': sys.version,\n",
    "        'platform': platform.platform(),\n",
    "    }\n",
    "    line = f\"[{datetime.now().isoformat()}] OPTA_BASELINE: {json.dumps(prov, default=str)}\\n\"\n",
    "    PROV.write_text(PROV.read_text() + line if PROV.exists() else line)\n",
    "    logging.info(f'OPT-A metrics saved: {METRICS_OPT_A}')\n",
    "    logging.info(f'Provenance appended: {PROV}')\n",
    "\n",
    "# Runner helper\n",
    "def run_optA(n_splits=5):\n",
    "    df = load_gold(GOLD_PARQUET)\n",
    "    if not PARTITIONS_CSV.exists():\n",
    "        make_partitions(df, n_splits=n_splits)\n",
    "    parts = pd.read_csv(PARTITIONS_CSV)\n",
    "    feat_map = select_features_and_save(df)\n",
    "    train_and_eval_optA(df, feat_map, parts)\n",
    "\n",
    "print('ibov_baseline_optA cell ready. Run run_optA(n_splits=5) to execute.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba4cb9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Not enough classes in train for h1 b1 — skipping\n",
      "WARNING: Not enough classes in train for h3 b1 — skipping\n",
      "WARNING: Not enough classes in train for h3 b1 — skipping\n",
      "WARNING: Not enough classes in train for h5 b1 — skipping\n",
      "WARNING: Not enough classes in train for h5 b1 — skipping\n",
      "INFO: OPT-A metrics saved: /home/wrm/BOLSA_2026/gold/IBOV/metrics_ibov_optA.csv\n",
      "INFO: Provenance appended: /home/wrm/BOLSA_2026/gold/IBOV/PROVENANCE_IBOV.txt\n",
      "INFO: OPT-A metrics saved: /home/wrm/BOLSA_2026/gold/IBOV/metrics_ibov_optA.csv\n",
      "INFO: Provenance appended: /home/wrm/BOLSA_2026/gold/IBOV/PROVENANCE_IBOV.txt\n"
     ]
    }
   ],
   "source": [
    "run_optA(n_splits=5) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project: BOLSA_2026)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
