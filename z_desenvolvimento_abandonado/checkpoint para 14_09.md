Perfeito ‚Äî alinhando com sua prefer√™ncia (dire√ß√£o do movimento > valor exato):

## O LSTM ainda √© a melhor t√©cnica?

Para **EOD di√°rio com 15 lags** e sem features ricas, **nem sempre**. Em tabular ‚Äúrasa‚Äù (lags, retornos, rolantes), **√°rvores impulsionadas** (LightGBM/XGBoost/CatBoost) costumam:

- aprender rela√ß√µes n√£o lineares simples com **menos overfitting**,
    
- treinar **muito mais r√°pido**,
    
- entregar **boa acur√°cia direcional** com as features certas.
    

O **LSTM** volta a valer quando:

- h√° **sequ√™ncias longas**/multi-resolu√ß√£o (intraday ‚Üí di√°rio),
    
- queremos **multi-horizontes acoplados** com depend√™ncia temporal expl√≠cita,
    
- usamos **ex√≥genas sequenciais** (ex.: Brent minuto, d√≥lar, futuros).
    

Como voc√™ quer **acertar dire√ß√£o**, recomendo **duas trilhas** em paralelo e compar√°veis:

---

## Trilha A ‚Äî Regress√£o em retorno (valor relativo) e sinal derivado

**Alvo:** rt+1=Closet+1‚àíClosetClosetr_{t+1} = \frac{Close_{t+1} - Close_t}{Close_t} (pode usar **log-return** tamb√©m).  
**Modelo:** LSTM **ou** LightGBM Regressor.  
**Loss:** Huber (robusto).  
**Treino:** s√≥ **dias de preg√£o** (sem FF).  
**Escalas:** X ‚Üí MinMax (ou Standard), y ‚Üí Standard (para r).  
**Decis√£o de dire√ß√£o:** `sign( rÃÇ_{t+1} )` com **limiar morto** ‚à£r^‚à£<Œµ|rÃÇ|<Œµ ‚Üí ‚Äúneutro‚Äù.  
**M√©trica principal:** **acur√°cia direcional** (‚Üë/‚Üì), **MCC**, **precision/recall** por classe (‚Üë e ‚Üì).  
**Por que ajuda?** O modelo aprende **varia√ß√£o**, n√£o n√≠vel ‚Äî reduz a ‚Äúpersist√™ncia‚Äù.

---

## Trilha B ‚Äî Classifica√ß√£o direcional pura (sobe/cai[/neutro])

**Alvo:** ycls‚àà{‚àí1,+1}y_{cls} \in \{-1, +1\} (opcional classe 0 para |r| muito pequeno).  
**Modelo:** **LightGBMClassifier** (baseline fort√≠ssimo) e um **cabe√ßalho de classifica√ß√£o** em cima do LSTM (multitarefa).  
**Loss:** BCE (ou **Focal Loss** se ‚Üë/‚Üì desbalanceados).  
**Pondera√ß√£o:** peso maior para amostras com |r| alto (menos ‚Äúru√≠do‚Äù).  
**Threshold tuning:** escolher o limiar de prob. que **maximiza MCC** ou o **retorno esperado** sob custos.  
**Por que ajuda?** Otimiza diretamente o que voc√™ prioriza: **dire√ß√£o**.

---

## Extra ‚Äî Multitarefa (o ‚Äúmeio-termo‚Äù que costumo preferir)

Um √∫nico modelo com **duas sa√≠das**:

- **Cabe√ßa regress√£o** ‚Üí rt+1r_{t+1} (Huber).
    
- **Cabe√ßa classifica√ß√£o** ‚Üí 1[rt+1>0]\mathbb{1}[r_{t+1}>0] (BCE/Focal).
    
- **Loss total:** L=ŒªregLHuber+ŒªclsLBCE\mathcal{L} = \lambda_{reg}\mathcal{L}_{Huber} + \lambda_{cls}\mathcal{L}_{BCE}.  
    Isso for√ßa o modelo a captar **valor relativo** e **dire√ß√£o** simultaneamente ‚Äî costuma melhorar o acerto ‚Üë/‚Üì sem sacrificar muito a coer√™ncia do valor.
    

---

## Regras de dados (para evitar vieses que vimos)

- **Treinar s√≥ preg√µes**; para calend√°rio, **dias sem preg√£o = regra Œî=0** fora do treino.
    
- **Corte temporal recente** (ex.: 2 anos) para aderir ao **regime atual**.
    
- **Walk-forward ‚Äúpurged‚Äù** (expanding window + embargo) ao avaliar.
    
- **Features √∫teis** (mesmo para √°rvores):
    
    - retornos rt‚àíkr_{t-k}, **retornos cumulativos** (5/10/20d),
        
    - **volatilidade rolante**, ATR, **z-scores** de retornos,
        
    - raz√µes Close/MAnClose/MA_{n}, **RSI** simples,
        
    - (opcional) ex√≥genas: Brent, USD/BRL, IBOV, setor.
        

---

## Recomendo como pr√≥ximo passo (objetivo: ganhar na dire√ß√£o)

1. **Criar alvo** rt+1r_{t+1} (e vers√£o com classe ‚Üë/‚Üì/0 com œµ\epsilon).
    
2. **Treinar LightGBMClassifier** (baseline) para dire√ß√£o em **preg√µes 2y**.
    
3. Em paralelo, **LSTM multitarefa** (r + classe) nas mesmas janelas/sets.
    
4. **Avaliar**: acur√°cia direcional, **MCC**, ROC-AUC, precision/recall ‚Üë/‚Üì, e uma **curva acerto vs. limiar**.
    
5. **Escolher limiar** para maximizar **utilidade** (ex.: acerto de altas, ou retorno esperado com custos).
    

Se topar, eu j√° te entrego a **ETAPA E** com:

- (i) preparo do dataset em **retornos** + r√≥tulo ‚Üë/‚Üì/0,
    
- (ii) **LightGBMClassifier** baseline,
    
- (iii) relat√≥rio de **acur√°cia direcional / MCC / matriz de confus√£o**,
    
- e (iv) a mesma avalia√ß√£o aplicada sobre um **LSTM multitarefa** (cabe√ßas g√™meas).
  
  
  
  Entendido. Aqui vai o **resumo de contexto para abertura do novo chat / notebook**, j√° preparado para que voc√™ (ou o modelo) recomece sabendo de onde partir:

---

## Checkpoint de Contexto ‚Äî Projeto Multiativo (PETR4 e demais 22 tickers)

1. **Estrutura de diret√≥rios no Google Drive**
    
    - Diret√≥rio raiz do novo projeto:
        
        ```
        /content/drive/Shared drives/BOLSA_2026/a_projeto_multiativo/LSTM/
        ```
        
    - Subpastas principais:
        
        - `bronze/<TICKER>/...` ‚Üí dados brutos de leil√£o (√∫ltimos 5 anos).
            
        - `datasets_preparados/` ‚Üí vers√µes filtradas/transformadas (ex.: preg√£o, calend√°rio 2 anos).
            
        - `experimentos/` ‚Üí artefatos de treino/teste (modelos, hist√≥ricos, predi√ß√µes, relat√≥rios).
            
        - `manifests/` ‚Üí metadados de runs e SSOT.
            
2. **Dados dispon√≠veis**
    
    - **Base ‚Äúantiga‚Äù calendarizada completa com FF:**
        
        ```
        /content/drive/Shared drives/BOLSA_2026/Projeto_LSTM_Multiativo/gold/PETR4/gold_lag15.parquet
        ```
        
        (equivalente ao usado no notebook original, d√° R¬≤ ‚âà0,96 mas com vi√©s de persist√™ncia).
        
    - **Bases derivadas preparadas recentemente:**
        
        ```
        /content/drive/Shared drives/BOLSA_2026/a_projeto_multiativo/LSTM/datasets_preparados/PETR4_d1_delta_pregao.parquet
        /content/drive/Shared drives/BOLSA_2026/a_projeto_multiativo/LSTM/datasets_preparados/PETR4_d1_calendario_2anos.parquet
        ```
        
        - `delta_pregao`: s√≥ dias de preg√£o, alvo Œî‚â†0.
            
        - `calendario_2anos`: recorte temporal para regime recente.
            
3. **√öltimos resultados**
    
    - **D+1 Œî-preg√£o:** R¬≤ ~0.997, mas n√£o superou na√Øve; acerto direcional ~46% (modelo colapsou para ‚Äúqueda constante‚Äù).
        
    - **Calend√°rio 2 anos:** R¬≤ mais realista (~0.74), baseline na√Øve ainda superior.
        
    - **Notebook antigo (base completa com FF):** inflava R¬≤ (~0.96) mas falhava na dire√ß√£o.
        
4. **Pr√≥xima linha de pesquisa**
    
    - **Novo alvo:** retorno percentual
        
        rt+1=Closet+1‚àíClosetClosetr_{t+1} = \frac{Close_{t+1} - Close_t}{Close_t}
    - **Decis√£o:** dire√ß√£o = `sign(r_{t+1})` (‚Üë / ‚Üì), opcional classe ‚Äúneutro‚Äù para |r| < Œµ.
        
    - **Modelagem recomendada:**
        
        - Baseline: **LightGBMClassifier** (r√°pido, robusto).
            
        - Alternativa: **LSTM multitarefa** (sa√≠da regress√£o de r + sa√≠da classifica√ß√£o ‚Üë/‚Üì).
            
    - **M√©trica central:** acur√°cia direcional, MCC, precision/recall ‚Üë/‚Üì.
        
    - **Regime de dados:** preg√µes dos √∫ltimos 2 anos ‚Üí mais aderente ao presente.
        

---

üëâ Esse √© o **estado inicial que o novo chat deve saber**: onde est√£o os dados, como foram tratados at√© agora, e qual √© o pr√≥ximo objetivo (sair do valor absoluto para o retorno percentual, focando na dire√ß√£o).

---

Quer que eu j√° escreva tamb√©m um **esqueleto inicial de notebook (Markdown + c√©lulas)** para voc√™ colar no novo ambiente e come√ßar direto dessa base?