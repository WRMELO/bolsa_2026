{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21553759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG df.columns: ['date', 'adj_close_^bvsp', 'close_^bvsp', 'high_^bvsp', 'low_^bvsp', 'open_^bvsp', 'volume_^bvsp']\n",
      "DEBUG type(df.columns): <class 'pandas.core.indexes.base.Index'>\n",
      "DEBUG sample rows (up to 3):\n",
      "[{'date': Timestamp('2012-01-03 00:00:00'), 'adj_close_^bvsp': 59265.0, 'close_^bvsp': 59265.0, 'high_^bvsp': 59288.0, 'low_^bvsp': 57836.0, 'open_^bvsp': 57836.0, 'volume_^bvsp': 3083000}, {'date': Timestamp('2012-01-04 00:00:00'), 'adj_close_^bvsp': 59365.0, 'close_^bvsp': 59365.0, 'high_^bvsp': 59519.0, 'low_^bvsp': 58558.0, 'open_^bvsp': 59263.0, 'volume_^bvsp': 2252000}, {'date': Timestamp('2012-01-05 00:00:00'), 'adj_close_^bvsp': 58546.0, 'close_^bvsp': 58546.0, 'high_^bvsp': 59354.0, 'low_^bvsp': 57963.0, 'open_^bvsp': 59354.0, 'volume_^bvsp': 2351200}]\n",
      "Bronze target parquet: /home/wrm/BOLSA_2026/dados_originais/IBOV_2012-01-03_2025-09-17.parquet\n",
      "Exists: True\n",
      "Rows, cols: (3398, 8)\n",
      "Columns: ['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'ticker']\n",
      "Dataframe date range: 2012-01-03 -> 2025-09-17\n",
      "Duplicate rows by date: 0\n",
      "NaNs per column: {'date': 0, 'open': 3398, 'high': 3398, 'low': 3398, 'close': 3398, 'adj_close': 0, 'volume': 3398, 'ticker': 0}\n",
      "File sha256: fe5a67cfeed73a480672ff151c54ac61ef1df1bed5985b142a8a458b0f978496\n",
      "Manifest path: /home/wrm/BOLSA_2026/dados_originais/manifesto_dados_originais_bronze_ibov.csv\n",
      "Saved bronze parquet and updated manifest: /home/wrm/BOLSA_2026/dados_originais/IBOV_2012-01-03_2025-09-17.parquet /home/wrm/BOLSA_2026/dados_originais/manifesto_dados_originais_bronze_ibov.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_181680/2662639054.py:204: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  run_ts = datetime.utcnow().replace(tzinfo=timezone.utc).isoformat()\n"
     ]
    }
   ],
   "source": [
    "# célula Jupyter atualizada — Baixa IBOV (^BVSP) desde 2012-01-01, salva Bronze parquet e atualiza manifesto\n",
    "# Mudanças principais:\n",
    "# - Achata MultiIndex de colunas (junta níveis com \"_\") e depois normaliza para snake_case (1a).\n",
    "# - Se coluna 'date' inexistente ou inteiramente NaT, PARA e imprime diagnóstico sem salvar (2a).\n",
    "# - Adiciona prints diagnósticos de df.columns e tipo antes da deduplicação (3sim).\n",
    "# - Se parquet alvo existir, adiciona sufixo timestamp para não sobrescrever (4b).\n",
    "# - Pode ser inserida no notebook (5sim).\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# ---- Helpers ----\n",
    "def sha256_of_file(path, chunk_size=65536):\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def ensure_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def flatten_multiindex_columns(df):\n",
    "    # If columns are MultiIndex, join levels with \"_\" and coerce to strings\n",
    "    cols = df.columns\n",
    "    if isinstance(cols, pd.MultiIndex):\n",
    "        new_cols = []\n",
    "        for tup in cols:\n",
    "            parts = [str(p) for p in tup if (p is not None and str(p) != \"\")]\n",
    "            new_cols.append(\"_\".join(parts) if parts else \"\")\n",
    "        df.columns = new_cols\n",
    "    else:\n",
    "        # ensure all column names are strings\n",
    "        df.columns = [str(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def to_snake_case_name(s):\n",
    "    # minimal snake_case normalizer: remove spaces, replace hyphens, lower, replace multiple _ with single\n",
    "    import re\n",
    "    s = s.strip()\n",
    "    s = s.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    s = re.sub(r\"[:/\\\\]\", \"_\", s)\n",
    "    s = re.sub(r\"__+\", \"_\", s)\n",
    "    s = s.lower()\n",
    "    return s\n",
    "\n",
    "def to_snake_case_cols(df):\n",
    "    df = df.copy()\n",
    "    new_cols = {}\n",
    "    for c in df.columns:\n",
    "        new_cols[c] = to_snake_case_name(c).replace(\"adj_close\", \"adj_close\")  # keep adj_close if already\n",
    "    return df.rename(columns=new_cols)\n",
    "\n",
    "def csv_quote(value):\n",
    "    s = str(value)\n",
    "    if any(ch in s for ch in ['\"', ',', '\\n']):\n",
    "        s = '\"' + s.replace('\"', '\"\"') + '\"'\n",
    "    return s\n",
    "\n",
    "def timestamp_suffix():\n",
    "    return datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ---- Params / Paths ----\n",
    "ROOT = Path(\"/home/wrm/BOLSA_2026\")\n",
    "TARGET_DIR = ROOT / \"dados_originais\"\n",
    "MANIFEST_PATH = TARGET_DIR / \"manifesto_dados_originais_bronze_ibov.csv\"\n",
    "TICKER = \"^BVSP\"\n",
    "REQ_START = \"2012-01-01\"\n",
    "today_utc = pd.Timestamp.utcnow().normalize()\n",
    "REQ_END = (today_utc + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")  # exclusive end\n",
    "\n",
    "ensure_dir(TARGET_DIR)\n",
    "\n",
    "# ---- Download ----\n",
    "df_raw = yf.download(TICKER, start=REQ_START, end=REQ_END, auto_adjust=False, progress=False)\n",
    "\n",
    "# ---- Process & Validate ----\n",
    "if isinstance(df_raw, pd.DataFrame) and not df_raw.empty:\n",
    "    # Make a working copy\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # If columns are MultiIndex, flatten them by joining levels with \"_\" (1a)\n",
    "    df = flatten_multiindex_columns(df)\n",
    "\n",
    "    # Reset index if index is DatetimeIndex and there's no explicit date column\n",
    "    if not any(pd.api.types.is_datetime64_any_dtype(df[col]) for col in df.columns):\n",
    "        # try reset_index to extract index as 'Date' column\n",
    "        df = df.reset_index()\n",
    "\n",
    "    # Normalize column names to snake_case after flattening\n",
    "    df = to_snake_case_cols(df)\n",
    "\n",
    "    # Diagnostic prints authorized (3sim)\n",
    "    print(\"DEBUG df.columns:\", list(df.columns))\n",
    "    print(\"DEBUG type(df.columns):\", type(df.columns))\n",
    "    print(\"DEBUG sample rows (up to 3):\")\n",
    "    try:\n",
    "        print(df.head(3).to_dict(orient=\"records\"))\n",
    "    except Exception:\n",
    "        print(\"DEBUG: cannot show sample rows\")\n",
    "\n",
    "    # Robustly ensure 'date' column exists:\n",
    "    if \"date\" not in df.columns:\n",
    "        # Try common alternatives\n",
    "        if \"index\" in df.columns:\n",
    "            df = df.rename(columns={\"index\": \"date\"})\n",
    "        elif \"date_local\" in df.columns:\n",
    "            df = df.rename(columns={\"date_local\": \"date\"})\n",
    "        else:\n",
    "            # try to find any datetime-like column\n",
    "            found = False\n",
    "            for c in df.columns:\n",
    "                try:\n",
    "                    if pd.api.types.is_datetime64_any_dtype(df[c]) or pd.api.types.is_datetime64_ns_dtype(df[c]):\n",
    "                        df = df.rename(columns={c: \"date\"})\n",
    "                        found = True\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if not found:\n",
    "                # try original index from df_raw\n",
    "                try:\n",
    "                    idx = df_raw.index\n",
    "                    if pd.api.types.is_datetime64_any_dtype(idx) or getattr(idx, \"dtype\", None) is not None:\n",
    "                        df.insert(0, \"date\", pd.to_datetime(idx))\n",
    "                    else:\n",
    "                        df.insert(0, \"date\", pd.NaT)\n",
    "                except Exception:\n",
    "                    df.insert(0, \"date\", pd.NaT)\n",
    "\n",
    "    # Coerce date dtype (safe: errors -> NaT)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    # If date column missing entirely or all NaT -> stop and print diagnostic (2a)\n",
    "    if \"date\" not in df.columns or df[\"date\"].dropna().empty:\n",
    "        print(\"ERROR: 'date' column missing or all NaT after processing. Aborting write.\")\n",
    "        print(\"DIAGNOSTIC df.columns:\", list(df.columns))\n",
    "        print(\"DIAGNOSTIC head (5):\")\n",
    "        try:\n",
    "            print(df.head(5).to_dict(orient=\"records\"))\n",
    "        except Exception:\n",
    "            print(\"DIAGNOSTIC: cannot show head\")\n",
    "        print(\"Manifest path (no write):\", MANIFEST_PATH)\n",
    "    else:\n",
    "        # Ensure required numeric columns exist\n",
    "        for c in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
    "            if c not in df.columns:\n",
    "                df[c] = pd.NA\n",
    "\n",
    "        # Ensure adj_close exists\n",
    "        if \"adj_close\" not in df.columns:\n",
    "            # Try variants from flattened names like 'adj_close_^bvsp' etc.\n",
    "            adj_candidates = [col for col in df.columns if \"adj\" in col and \"close\" in col]\n",
    "            if adj_candidates:\n",
    "                df = df.rename(columns={adj_candidates[0]: \"adj_close\"})\n",
    "            else:\n",
    "                df[\"adj_close\"] = pd.NA\n",
    "\n",
    "        # Ensure ticker column and set value\n",
    "        df[\"ticker\"] = TICKER\n",
    "\n",
    "        # Keep only expected schema order\n",
    "        cols_required = [\"date\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\", \"ticker\"]\n",
    "        df = df[[c for c in cols_required if c in df.columns]]\n",
    "\n",
    "        # Sort by date ascending (NaT to the end)\n",
    "        df = df.sort_values(\"date\", ascending=True, na_position=\"last\").reset_index(drop=True)\n",
    "\n",
    "        # Count duplicates by date and drop keeping last\n",
    "        before_rows = len(df)\n",
    "        duplicates_mask = df.duplicated(subset=[\"date\"], keep=\"last\")\n",
    "        duplicates_count = int(duplicates_mask.sum())\n",
    "        if duplicates_count > 0:\n",
    "            df = df.drop_duplicates(subset=[\"date\"], keep=\"last\").reset_index(drop=True)\n",
    "        after_rows = len(df)\n",
    "\n",
    "        # NaNs per column\n",
    "        nan_counts = {col: int(df[col].isna().sum()) for col in df.columns}\n",
    "\n",
    "        # Date range (skip NaT)\n",
    "        valid_dates = df[\"date\"].dropna()\n",
    "        date_min = valid_dates.min() if not valid_dates.empty else pd.NaT\n",
    "        date_max = valid_dates.max() if not valid_dates.empty else pd.NaT\n",
    "        date_min_str = pd.to_datetime(date_min).strftime(\"%Y-%m-%d\") if pd.notna(date_min) else \"na\"\n",
    "        date_max_str = pd.to_datetime(date_max).strftime(\"%Y-%m-%d\") if pd.notna(date_max) else \"na\"\n",
    "\n",
    "        # Parquet filename and save (snappy). If exists, add timestamp suffix (4b)\n",
    "        parquet_filename = f\"IBOV_{date_min_str}_{date_max_str}.parquet\"\n",
    "        parquet_path = TARGET_DIR / parquet_filename\n",
    "        if parquet_path.exists():\n",
    "            parquet_path = TARGET_DIR / f\"IBOV_{date_min_str}_{date_max_str}_{timestamp_suffix()}.parquet\"\n",
    "\n",
    "        df.to_parquet(parquet_path, engine=\"pyarrow\", compression=\"snappy\", index=False)\n",
    "\n",
    "        # SHA256\n",
    "        file_sha256 = sha256_of_file(parquet_path)\n",
    "\n",
    "        # Manifest entry\n",
    "        run_ts = datetime.utcnow().replace(tzinfo=timezone.utc).isoformat()\n",
    "        manifest_fields = [\n",
    "            \"run_ts\",\n",
    "            \"ticker\",\n",
    "            \"start_req\",\n",
    "            \"end_req\",\n",
    "            \"date_min\",\n",
    "            \"date_max\",\n",
    "            \"rows\",\n",
    "            \"cols\",\n",
    "            \"duplicates_dropped\",\n",
    "            \"nan_counts_json\",\n",
    "            \"parquet_path\",\n",
    "            \"parquet_sha256\"\n",
    "        ]\n",
    "        manifest_row = {\n",
    "            \"run_ts\": run_ts,\n",
    "            \"ticker\": TICKER,\n",
    "            \"start_req\": REQ_START,\n",
    "            \"end_req\": today_utc.strftime(\"%Y-%m-%d\"),\n",
    "            \"date_min\": date_min_str,\n",
    "            \"date_max\": date_max_str,\n",
    "            \"rows\": after_rows,\n",
    "            \"cols\": len(df.columns),\n",
    "            \"duplicates_dropped\": duplicates_count,\n",
    "            \"nan_counts_json\": json.dumps(nan_counts, ensure_ascii=False),\n",
    "            \"parquet_path\": str(parquet_path),\n",
    "            \"parquet_sha256\": file_sha256\n",
    "        }\n",
    "\n",
    "        # Append to manifest (create with header if missing)\n",
    "        manifest_exists = MANIFEST_PATH.exists()\n",
    "        if not manifest_exists:\n",
    "            with open(MANIFEST_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\",\".join(manifest_fields) + \"\\n\")\n",
    "        with open(MANIFEST_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "            row_values = [manifest_row[field] for field in manifest_fields]\n",
    "            row_quoted = [csv_quote(v) for v in row_values]\n",
    "            f.write(\",\".join(row_quoted) + \"\\n\")\n",
    "\n",
    "        # Required prints (exact lines)\n",
    "        print(f\"Bronze target parquet: {parquet_path}\")\n",
    "        print(f\"Exists: {parquet_path.exists()}\")\n",
    "        print(f\"Rows, cols: ({after_rows}, {len(df.columns)})\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        print(f\"Dataframe date range: {date_min_str} -> {date_max_str}\")\n",
    "        print(f\"Duplicate rows by date: {duplicates_count}\")\n",
    "        print(f\"NaNs per column: {nan_counts}\")\n",
    "        print(f\"File sha256: {file_sha256}\")\n",
    "        print(f\"Manifest path: {MANIFEST_PATH}\")\n",
    "        print(f\"Saved bronze parquet and updated manifest: {parquet_path} {MANIFEST_PATH}\")\n",
    "\n",
    "else:\n",
    "    # No data downloaded — prints required fallback\n",
    "    print(\"Bronze target parquet: None\")\n",
    "    print(\"Exists: False\")\n",
    "    print(\"Rows, cols: (0, 0)\")\n",
    "    print(\"Columns: []\")\n",
    "    print(\"Dataframe date range: None -> None\")\n",
    "    print(\"Duplicate rows by date: 0\")\n",
    "    print(\"NaNs per column: {}\")\n",
    "    print(\"File sha256: None\")\n",
    "    print(f\"Manifest path: {MANIFEST_PATH}\")\n",
    "    print(\"Saved bronze parquet and updated manifest: None None\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project: BOLSA_2026)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
