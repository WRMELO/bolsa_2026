{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53286f24",
   "metadata": {},
   "source": [
    "# Instrução 1A-REV3 — Coleta direta Yahoo Chart → Bronze (dry_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b6539e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== PROVEDOR E TENTATIVAS ========\n",
      "{\n",
      "  \"provider_used\": \"yahoo-chart\",\n",
      "  \"rows_returned\": 3400\n",
      "}\n",
      "[\n",
      "  {\n",
      "    \"provider\": \"yahoo-chart\",\n",
      "    \"attempt\": 1,\n",
      "    \"ok\": true,\n",
      "    \"rows\": 3400,\n",
      "    \"exception_message\": null\n",
      "  }\n",
      "]\n",
      "\n",
      "======== SCHEMA (EXATO) ========\n",
      "{\n",
      "  \"columns_expected\": [\n",
      "    \"date\",\n",
      "    \"open\",\n",
      "    \"high\",\n",
      "    \"low\",\n",
      "    \"close\",\n",
      "    \"volume\",\n",
      "    \"ticker\"\n",
      "  ],\n",
      "  \"columns_obtained\": [\n",
      "    \"date\",\n",
      "    \"open\",\n",
      "    \"high\",\n",
      "    \"low\",\n",
      "    \"close\",\n",
      "    \"volume\",\n",
      "    \"ticker\"\n",
      "  ],\n",
      "  \"dtypes_obtained\": {\n",
      "    \"date\": \"datetime64[ns]\",\n",
      "    \"open\": \"float64\",\n",
      "    \"high\": \"float64\",\n",
      "    \"low\": \"float64\",\n",
      "    \"close\": \"float64\",\n",
      "    \"volume\": \"int64\",\n",
      "    \"ticker\": \"string\"\n",
      "  },\n",
      "  \"nulls_percent\": {\n",
      "    \"date\": 0.0,\n",
      "    \"open\": 0.0,\n",
      "    \"high\": 0.0,\n",
      "    \"low\": 0.0,\n",
      "    \"close\": 0.0,\n",
      "    \"volume\": 0.0,\n",
      "    \"ticker\": 0.0\n",
      "  },\n",
      "  \"ticker_dtype_is_string\": true,\n",
      "  \"ticker_nulls_percent\": 0.0\n",
      "}\n",
      "\n",
      "======== INTERVALO TEMPORAL (com tolerâncias) ========\n",
      "{\n",
      "  \"required_start\": \"2012-01-01 00:00:00\",\n",
      "  \"start_tolerance_max\": \"2012-01-06 00:00:00\",\n",
      "  \"required_end_min\": \"2025-09-16 00:00:00\",\n",
      "  \"date_min\": \"2012-01-03 00:00:00\",\n",
      "  \"date_max\": \"2025-09-19 00:00:00\",\n",
      "  \"start_verdict\": \"OK\",\n",
      "  \"end_verdict\": \"OK\"\n",
      "}\n",
      "\n",
      "======== QUALIDADE ========\n",
      "{\n",
      "  \"percent_nulls\": {\n",
      "    \"date\": 0.0,\n",
      "    \"open\": 0.0,\n",
      "    \"high\": 0.0,\n",
      "    \"low\": 0.0,\n",
      "    \"close\": 0.0,\n",
      "    \"volume\": 0.0,\n",
      "    \"ticker\": 0.0\n",
      "  },\n",
      "  \"duplicates_by_date\": 0,\n",
      "  \"constraints\": {\n",
      "    \"nulls_must_be_zero_in\": {\n",
      "      \"date\": true,\n",
      "      \"close\": true,\n",
      "      \"ticker\": true\n",
      "    },\n",
      "    \"duplicates_by_date_must_be_zero\": true,\n",
      "    \"min_rows_required\": 2500,\n",
      "    \"date_monotonic_increasing\": true\n",
      "  }\n",
      "}\n",
      "\n",
      "======== AMOSTRA — HEAD(10) ========\n",
      "      date   close  volume ticker\n",
      "2012-01-03 59265.0 3083000  ^BVSP\n",
      "2012-01-04 59365.0 2252000  ^BVSP\n",
      "2012-01-05 58546.0 2351200  ^BVSP\n",
      "2012-01-06 58600.0 1659200  ^BVSP\n",
      "2012-01-09 59083.0 2244600  ^BVSP\n",
      "2012-01-10 59806.0 2689200  ^BVSP\n",
      "2012-01-11 59962.0 2245200  ^BVSP\n",
      "2012-01-12 59921.0 2145600  ^BVSP\n",
      "2012-01-13 59147.0 5624200  ^BVSP\n",
      "2012-01-16 59956.0 1705000  ^BVSP\n",
      "\n",
      "======== AMOSTRA — TAIL(10) ========\n",
      "      date         close  volume ticker\n",
      "2025-09-08 141792.000000 7440900  ^BVSP\n",
      "2025-09-09 141618.000000 7481900  ^BVSP\n",
      "2025-09-10 142349.000000 7138700  ^BVSP\n",
      "2025-09-11 143151.000000 7570400  ^BVSP\n",
      "2025-09-12 142272.000000 6388600  ^BVSP\n",
      "2025-09-15 143547.000000 6614000  ^BVSP\n",
      "2025-09-16 144062.000000 8478200  ^BVSP\n",
      "2025-09-17 145594.000000 9604400  ^BVSP\n",
      "2025-09-18 145500.000000 8372000  ^BVSP\n",
      "2025-09-19 145808.515625       0  ^BVSP\n",
      "\n",
      "======== CONTAGENS ========\n",
      "{\n",
      "  \"rows_before_cleaning\": 3408,\n",
      "  \"rows_dropped_ohlc\": 8,\n",
      "  \"rows_after_cleaning\": 3400,\n",
      "  \"unique_days\": 3400,\n",
      "  \"days_with_volume_zero\": 32,\n",
      "  \"final_rows\": 3400\n",
      "}\n",
      "\n",
      "======== PLANO DE PERSISTÊNCIA (SIMULADO) ========\n",
      "{\n",
      "  \"dry_run\": false,\n",
      "  \"parquet_target\": \"/home/wrm/BOLSA_2026/bronze/IBOV.parquet\",\n",
      "  \"partitions\": [\n",
      "    \"year=2012\",\n",
      "    \"year=2013\",\n",
      "    \"year=2014\",\n",
      "    \"year=2015\",\n",
      "    \"year=2016\",\n",
      "    \"year=2017\",\n",
      "    \"year=2018\",\n",
      "    \"year=2019\",\n",
      "    \"year=2020\",\n",
      "    \"year=2021\",\n",
      "    \"year=2022\",\n",
      "    \"year=2023\",\n",
      "    \"year=2024\",\n",
      "    \"year=2025\"\n",
      "  ],\n",
      "  \"manifesto_path\": \"/home/wrm/BOLSA_2026/manifestos/bronze_ibov_manifesto.csv\",\n",
      "  \"manifesto_header\": \"timestamp,ticker,rows_total,date_min,date_max,columns_json,partitions_json,target_path\",\n",
      "  \"manifesto_row_sample\": \"2025-09-19T10:22:32.389706-03:00,^BVSP,3400,2012-01-03 00:00:00,2025-09-19 00:00:00,[\\\"date\\\", \\\"open\\\", \\\"high\\\", \\\"low\\\", \\\"close\\\", \\\"volume\\\", \\\"ticker\\\"],[\\\"year=2012\\\", \\\"year=2013\\\", \\\"year=2014\\\", \\\"year=2015\\\", \\\"year=2016\\\", \\\"year=2017\\\", \\\"year=2018\\\", \\\"year=2019\\\", \\\"year=2020\\\", \\\"year=2021\\\", \\\"year=2022\\\", \\\"year=2023\\\", \\\"year=2024\\\", \\\"year=2025\\\"],/home/wrm/BOLSA_2026/bronze/IBOV.parquet\",\n",
      "  \"nota\": \"Nenhuma escrita realizada em dry_run=True.\"\n",
      "}\n",
      "\n",
      "======== CHECKLIST ========\n",
      "{\n",
      "  \"provider_attempts_listed\": \"ok\",\n",
      "  \"schema_columns_and_dtypes_exact\": \"ok\",\n",
      "  \"interval_tolerance_verdicts\": \"ok\",\n",
      "  \"quality_nulls_and_duplicates\": \"ok\",\n",
      "  \"sample_head_tail_presented\": \"ok\",\n",
      "  \"counts_included\": \"ok\",\n",
      "  \"persistence_plan_simulated\": \"ok\"\n",
      "}\n",
      "\n",
      "======== ESTRUTURA DO RESULTADO (info) ========\n",
      "{\n",
      "  \"ticker\": \"^BVSP\",\n",
      "  \"periodo\": {\n",
      "    \"start\": \"2012-01-01 00:00:00\",\n",
      "    \"end\": \"2025-09-19 00:00:00\"\n",
      "  },\n",
      "  \"dry_run\": false,\n",
      "  \"timestamp_execucao\": \"2025-09-19T10:22:32.389706-03:00\",\n",
      "  \"dataframe_name\": \"bronze_ibov\",\n",
      "  \"columns\": [\n",
      "    \"date\",\n",
      "    \"open\",\n",
      "    \"high\",\n",
      "    \"low\",\n",
      "    \"close\",\n",
      "    \"volume\",\n",
      "    \"ticker\"\n",
      "  ],\n",
      "  \"dtypes\": {\n",
      "    \"date\": \"datetime64[ns]\",\n",
      "    \"open\": \"float64\",\n",
      "    \"high\": \"float64\",\n",
      "    \"low\": \"float64\",\n",
      "    \"close\": \"float64\",\n",
      "    \"volume\": \"int64\",\n",
      "    \"ticker\": \"string\"\n",
      "  },\n",
      "  \"provider_used\": \"yahoo-chart\",\n",
      "  \"status\": \"sucesso\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Instrução 1A-REV3 — Coleta direta Yahoo Chart → Bronze (dry_run)\n",
    "# Regras:\n",
    "# - Bloco único, auto-contido.\n",
    "# - dry_run=True (sem persistência).\n",
    "# - Provedores em ordem: Yahoo Chart -> yfinance -> Stooq.\n",
    "# - Sem dados sintéticos.\n",
    "# - Mensagens normativas: VALIDATION_ERROR / CHECKLIST_FAILURE.\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tseries.offsets import BusinessDay as BDay\n",
    "\n",
    "# =========================\n",
    "# Parâmetros\n",
    "# =========================\n",
    "ROOT_DIR = Path(\"/home/wrm/BOLSA_2026\").resolve()\n",
    "DRY_RUN = False\n",
    "TICKER = \"^BVSP\"\n",
    "\n",
    "START_DATE_UTC = pd.Timestamp(\"2012-01-01\", tz=\"UTC\")\n",
    "NOW_UTC = pd.Timestamp(datetime.now(timezone.utc))\n",
    "END_DATE_UTC = NOW_UTC.normalize()  # 00:00 UTC de hoje\n",
    "PERIOD2_NOW_UTC = NOW_UTC  # para Yahoo Chart, usar timestamp \"agora\"\n",
    "\n",
    "PARQUET_TARGET = ROOT_DIR / \"bronze\" / \"IBOV.parquet\"\n",
    "MANIFESTO_TARGET = ROOT_DIR / \"manifestos\" / \"bronze_ibov_manifesto.csv\"\n",
    "\n",
    "EXPECTED_COLUMNS = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"ticker\"]\n",
    "EXPECTED_DTYPES = {\n",
    "    \"date\": \"datetime64[ns]\",\n",
    "    \"open\": \"float64\",\n",
    "    \"high\": \"float64\",\n",
    "    \"low\": \"float64\",\n",
    "    \"close\": \"float64\",\n",
    "    \"volume\": \"int64\",\n",
    "    \"ticker\": \"string\",\n",
    "}\n",
    "\n",
    "AGORA = datetime.now().astimezone()\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "def print_section(title: str):\n",
    "    print(\"\\n\" + \"=\" * 8 + f\" {title} \" + \"=\" * 8)\n",
    "\n",
    "def dtypes_signature(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    return {c: str(df.dtypes[c]) for c in df.columns}\n",
    "\n",
    "def percent_nulls(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    total = len(df)\n",
    "    if total == 0:\n",
    "        return {c: 100.0 for c in df.columns}\n",
    "    return {c: float(df[c].isna().sum()) * 100.0 / float(total) for c in df.columns}\n",
    "\n",
    "def to_unix_seconds(ts: pd.Timestamp) -> int:\n",
    "    if ts.tzinfo is None:\n",
    "        ts = ts.tz_localize(\"UTC\")\n",
    "    else:\n",
    "        ts = ts.tz_convert(\"UTC\")\n",
    "    return int(ts.timestamp())\n",
    "\n",
    "def bronze_normalize(\n",
    "    df_pre: pd.DataFrame,\n",
    "    ticker: str,\n",
    "    start_utc: pd.Timestamp,\n",
    "    end_utc: pd.Timestamp\n",
    ") -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    df_pre: espera colunas ['date','open','high','low','close','volume'] (date pode ser datetime ou epoch já convertido)\n",
    "    Retorna df_final no schema Bronze + contagens de limpeza.\n",
    "    \"\"\"\n",
    "    df = df_pre.copy()\n",
    "\n",
    "    # Garantir colunas\n",
    "    for c in [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
    "        if c not in df.columns:\n",
    "            raise RuntimeError(f\"SCHEMA_ERROR: coluna ausente em df_pre: {c}\")\n",
    "\n",
    "    # Date -> datetime naive normalizado 00:00\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True).dt.tz_localize(None).dt.normalize()\n",
    "\n",
    "    # Tipos numéricos\n",
    "    for c in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Contagens antes da limpeza\n",
    "    rows_before_cleaning = int(len(df))\n",
    "\n",
    "    # Remover linhas com qualquer OHLC nulo\n",
    "    mask_ohlc_notna = (~df[\"open\"].isna()) & (~df[\"high\"].isna()) & (~df[\"low\"].isna()) & (~df[\"close\"].isna())\n",
    "    df = df[mask_ohlc_notna].copy()\n",
    "    rows_after_cleaning = int(len(df))\n",
    "    rows_dropped_ohlc = int(rows_before_cleaning - rows_after_cleaning)\n",
    "\n",
    "    # Volume: NaN -> 0, int64\n",
    "    df[\"volume\"] = pd.to_numeric(df[\"volume\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "\n",
    "    # Forçar dtype float64 para OHLC\n",
    "    for c in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "        df[c] = df[c].astype(\"float64\")\n",
    "\n",
    "    # ticker\n",
    "    df[\"ticker\"] = pd.Series([ticker] * len(df), dtype=\"string\").astype(\"string\")\n",
    "\n",
    "    # Filtrar intervalo [start, end]\n",
    "    start_naive = start_utc.tz_convert(None).tz_localize(None) if start_utc.tzinfo is not None else start_utc\n",
    "    end_naive = end_utc.tz_convert(None).tz_localize(None) if end_utc.tzinfo is not None else end_utc\n",
    "    df = df[(df[\"date\"] >= start_naive) & (df[\"date\"] <= end_naive)].copy()\n",
    "\n",
    "    # Ordenar, deduplicar por date\n",
    "    df = df.sort_values(\"date\").drop_duplicates(subset=[\"date\"], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "    # Reordenar colunas\n",
    "    df = df[[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"ticker\"]]\n",
    "\n",
    "    stats = {\n",
    "        \"rows_before_cleaning\": rows_before_cleaning,\n",
    "        \"rows_after_cleaning\": rows_after_cleaning,\n",
    "        \"rows_dropped_ohlc\": rows_dropped_ohlc,\n",
    "    }\n",
    "    return df, stats\n",
    "\n",
    "# =========================\n",
    "# Provedores\n",
    "# =========================\n",
    "def fetch_yahoo_chart_direct(\n",
    "    ticker: str,\n",
    "    start_utc: pd.Timestamp,\n",
    "    period2_now_utc: pd.Timestamp,\n",
    "    retries: int = 2,\n",
    "    backoff_seconds: List[float] = [0.8, 1.6]\n",
    ") -> Tuple[Optional[pd.DataFrame], Dict[str, int], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Coleta direto do endpoint Chart do Yahoo.\n",
    "    Retorna (df_final, stats, attempts).\n",
    "    \"\"\"\n",
    "    attempts: List[Dict[str, Any]] = []\n",
    "    df_final: Optional[pd.DataFrame] = None\n",
    "    stats: Dict[str, int] = {\"rows_before_cleaning\": 0, \"rows_after_cleaning\": 0, \"rows_dropped_ohlc\": 0}\n",
    "\n",
    "    base_url = \"https://query2.finance.yahoo.com/v8/finance/chart/%5EBVSP\"\n",
    "    params = {\n",
    "        \"period1\": str(to_unix_seconds(start_utc)),\n",
    "        \"period2\": str(to_unix_seconds(period2_now_utc)),\n",
    "        \"interval\": \"1d\",\n",
    "        \"events\": \"history\",\n",
    "        \"includeAdjustedClose\": \"false\",\n",
    "    }\n",
    "\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            # Prefer requests se disponível; caso contrário, urllib\n",
    "            try:\n",
    "                import requests  # type: ignore\n",
    "                r = requests.get(base_url, params=params, headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) Python\"}, timeout=5)\n",
    "                status_code = r.status_code\n",
    "                if status_code < 200 or status_code >= 400:\n",
    "                    raise RuntimeError(f\"HTTP_STATUS_{status_code}\")\n",
    "                data = r.json()\n",
    "            except Exception as e_req:\n",
    "                # fallback para urllib\n",
    "                try:\n",
    "                    from urllib.parse import urlencode\n",
    "                    from urllib.request import Request, urlopen\n",
    "                    url = base_url + \"?\" + urlencode(params)\n",
    "                    req = Request(url, headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) Python\"})\n",
    "                    with urlopen(req, timeout=6) as resp:\n",
    "                        status_code = getattr(resp, \"status\", 200)\n",
    "                        raw = resp.read()\n",
    "                    data = json.loads(raw.decode(\"utf-8\"))\n",
    "                except Exception as e_url:\n",
    "                    raise RuntimeError(f\"HTTP_ERROR: {e_req} | URLLIB_FALLBACK: {e_url}\")\n",
    "\n",
    "            # Parse esperado\n",
    "            if \"chart\" not in data:\n",
    "                raise RuntimeError(\"PARSE_ERROR: chave 'chart' ausente\")\n",
    "            chart = data[\"chart\"]\n",
    "            if chart.get(\"error\"):\n",
    "                raise RuntimeError(f\"REMOTE_ERROR: {chart.get('error')}\")\n",
    "            results = chart.get(\"result\", [])\n",
    "            if not results:\n",
    "                raise RuntimeError(\"PARSE_ERROR: 'result' vazio\")\n",
    "            res0 = results[0]\n",
    "            ts = res0.get(\"timestamp\", [])\n",
    "            inds = res0.get(\"indicators\", {})\n",
    "            quotes = inds.get(\"quote\", [])\n",
    "            if not quotes:\n",
    "                raise RuntimeError(\"PARSE_ERROR: 'quote[0]' ausente\")\n",
    "            q0 = quotes[0]\n",
    "            opens = q0.get(\"open\", [])\n",
    "            highs = q0.get(\"high\", [])\n",
    "            lows = q0.get(\"low\", [])\n",
    "            closes = q0.get(\"close\", [])\n",
    "            vols = q0.get(\"volume\", [])\n",
    "\n",
    "            n = min(len(ts), len(opens), len(highs), len(lows), len(closes), len(vols))\n",
    "            if n == 0:\n",
    "                raise RuntimeError(\"DATA_EMPTY_ERROR: listas vazias\")\n",
    "            # Construir DataFrame posicional\n",
    "            df_pre = pd.DataFrame({\n",
    "                \"date\": pd.to_datetime(ts[:n], unit=\"s\", utc=True),\n",
    "                \"open\": opens[:n],\n",
    "                \"high\": highs[:n],\n",
    "                \"low\": lows[:n],\n",
    "                \"close\": closes[:n],\n",
    "                \"volume\": vols[:n],\n",
    "            })\n",
    "            # Normalizar Bronze com limpeza\n",
    "            df_norm, stats = bronze_normalize(df_pre, ticker, START_DATE_UTC, END_DATE_UTC)\n",
    "            attempts.append({\"provider\": \"yahoo-chart\", \"attempt\": i + 1, \"ok\": True, \"rows\": int(len(df_norm)), \"exception_message\": None})\n",
    "            df_final = df_norm\n",
    "            return df_final, stats, attempts\n",
    "        except Exception as e:\n",
    "            attempts.append({\"provider\": \"yahoo-chart\", \"attempt\": i + 1, \"ok\": False, \"rows\": 0, \"exception_message\": str(e)})\n",
    "            if i < retries - 1:\n",
    "                time.sleep(backoff_seconds[min(i, len(backoff_seconds) - 1)])\n",
    "\n",
    "    return None, stats, attempts\n",
    "\n",
    "def fetch_with_yfinance(\n",
    "    ticker: str,\n",
    "    start_utc: pd.Timestamp,\n",
    "    end_utc: pd.Timestamp,\n",
    "    retries: int = 2,\n",
    "    backoff_seconds: List[float] = [0.8, 1.6]\n",
    ") -> Tuple[Optional[pd.DataFrame], Dict[str, int], List[Dict[str, Any]]]:\n",
    "    attempts: List[Dict[str, Any]] = []\n",
    "    stats: Dict[str, int] = {\"rows_before_cleaning\": 0, \"rows_after_cleaning\": 0, \"rows_dropped_ohlc\": 0}\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            try:\n",
    "                import yfinance as yf  # type: ignore\n",
    "            except Exception as e_imp:\n",
    "                attempts.append({\"provider\": \"yfinance\", \"attempt\": i + 1, \"ok\": False, \"rows\": 0, \"exception_message\": f\"IMPORT_ERROR: {e_imp}\"})\n",
    "                break\n",
    "            try:\n",
    "                start_str = start_utc.tz_localize(None).date().isoformat() if start_utc.tzinfo else start_utc.date().isoformat()\n",
    "                end_inc = (end_utc + pd.Timedelta(days=1))  # end-exclusive\n",
    "                end_str = end_inc.tz_localize(None).date().isoformat() if end_inc.tzinfo else end_inc.date().isoformat()\n",
    "                df_raw = yf.download(\n",
    "                    tickers=ticker,\n",
    "                    start=start_str,\n",
    "                    end=end_str,\n",
    "                    interval=\"1d\",\n",
    "                    auto_adjust=False,\n",
    "                    progress=False,\n",
    "                    threads=True\n",
    "                )\n",
    "                if df_raw is None or df_raw.empty:\n",
    "                    raise RuntimeError(\"DATA_EMPTY_ERROR: yfinance retornou vazio\")\n",
    "                # Mapear colunas\n",
    "                df_raw = df_raw.copy()\n",
    "                # Lidar com MultiIndex simples: se colunas são ('Open',), etc.\n",
    "                if isinstance(df_raw.columns, pd.MultiIndex):\n",
    "                    try:\n",
    "                        df_raw.columns = [c[-1] if isinstance(c, tuple) else c for c in df_raw.columns.to_list()]\n",
    "                    except Exception:\n",
    "                        df_raw.columns = df_raw.columns.get_level_values(-1)\n",
    "                rename_map = {\"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\", \"Volume\": \"volume\",\n",
    "                              \"open\": \"open\", \"high\": \"high\", \"low\": \"low\", \"close\": \"close\", \"volume\": \"volume\"}\n",
    "                df_raw = df_raw.rename(columns=rename_map)\n",
    "                need = {\"open\", \"high\", \"low\", \"close\", \"volume\"}\n",
    "                if not need.issubset(set(df_raw.columns)):\n",
    "                    missing = sorted(list(need - set(df_raw.columns)))\n",
    "                    raise RuntimeError(f\"SCHEMA_ERROR: faltam colunas em yfinance: {missing}\")\n",
    "                df_pre = df_raw.reset_index().rename(columns={\"Date\": \"date\", \"Datetime\": \"date\"})\n",
    "                if \"date\" not in df_pre.columns:\n",
    "                    # se índice for datetime e não houver 'date' após reset\n",
    "                    df_pre = df_raw.copy()\n",
    "                    df_pre[\"date\"] = df_pre.index\n",
    "                    df_pre = df_pre.reset_index(drop=True)\n",
    "                df_pre = df_pre[[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "                df_norm, stats = bronze_normalize(df_pre, ticker, START_DATE_UTC, END_DATE_UTC)\n",
    "                attempts.append({\"provider\": \"yfinance\", \"attempt\": i + 1, \"ok\": True, \"rows\": int(len(df_norm)), \"exception_message\": None})\n",
    "                return df_norm, stats, attempts\n",
    "            except Exception as e_dl:\n",
    "                attempts.append({\"provider\": \"yfinance\", \"attempt\": i + 1, \"ok\": False, \"rows\": 0, \"exception_message\": str(e_dl)})\n",
    "                if i < retries - 1:\n",
    "                    time.sleep(backoff_seconds[min(i, len(backoff_seconds) - 1)])\n",
    "        except Exception as e:\n",
    "            attempts.append({\"provider\": \"yfinance\", \"attempt\": i + 1, \"ok\": False, \"rows\": 0, \"exception_message\": str(e)})\n",
    "            break\n",
    "    return None, stats, attempts\n",
    "\n",
    "def fetch_with_stooq(\n",
    "    ticker: str,\n",
    "    start_utc: pd.Timestamp,\n",
    "    end_utc: pd.Timestamp,\n",
    "    retries: int = 1\n",
    ") -> Tuple[Optional[pd.DataFrame], Dict[str, int], List[Dict[str, Any]]]:\n",
    "    attempts: List[Dict[str, Any]] = []\n",
    "    stats: Dict[str, int] = {\"rows_before_cleaning\": 0, \"rows_after_cleaning\": 0, \"rows_dropped_ohlc\": 0}\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            try:\n",
    "                from pandas_datareader import data as dr  # type: ignore\n",
    "            except Exception as e_imp:\n",
    "                attempts.append({\"provider\": \"stooq\", \"attempt\": i + 1, \"ok\": False, \"rows\": 0, \"exception_message\": f\"IMPORT_ERROR: {e_imp}\"})\n",
    "                break\n",
    "            try:\n",
    "                candidates = [ticker, ticker.replace(\"^\", \"\"), ticker.replace(\"^\", \"\").lower()]\n",
    "                df_raw = None\n",
    "                last_exc = None\n",
    "                for tk in candidates:\n",
    "                    try:\n",
    "                        df_raw = dr.DataReader(tk, \"stooq\", start=start_utc.tz_localize(None), end=end_utc.tz_localize(None))\n",
    "                        if df_raw is not None and not df_raw.empty:\n",
    "                            break\n",
    "                    except Exception as e2:\n",
    "                        last_exc = e2\n",
    "                        continue\n",
    "                if df_raw is None or df_raw.empty:\n",
    "                    raise RuntimeError(f\"STOOQ_EMPTY: {last_exc}\") if last_exc else RuntimeError(\"STOOQ_EMPTY: retorno vazio\")\n",
    "                # Stooq costuma vir com colunas minúsculas ou 'Open/High/...'\n",
    "                df_raw = df_raw.sort_index()\n",
    "                if isinstance(df_raw.columns, pd.MultiIndex):\n",
    "                    try:\n",
    "                        df_raw.columns = [c[-1] if isinstance(c, tuple) else c for c in df_raw.columns.to_list()]\n",
    "                    except Exception:\n",
    "                        df_raw.columns = df_raw.columns.get_level_values(-1)\n",
    "                rename_map = {\"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\", \"Volume\": \"volume\",\n",
    "                              \"open\": \"open\", \"high\": \"high\", \"low\": \"low\", \"close\": \"close\", \"volume\": \"volume\"}\n",
    "                df_raw = df_raw.rename(columns=rename_map)\n",
    "                need = {\"open\", \"high\", \"low\", \"close\", \"volume\"}\n",
    "                if not need.issubset(set(df_raw.columns)):\n",
    "                    missing = sorted(list(need - set(df_raw.columns)))\n",
    "                    raise RuntimeError(f\"SCHEMA_ERROR: faltam colunas em stooq: {missing}\")\n",
    "                df_pre = df_raw.copy()\n",
    "                df_pre[\"date\"] = df_pre.index\n",
    "                df_pre = df_pre.reset_index(drop=True)\n",
    "                df_pre = df_pre[[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "                df_norm, stats = bronze_normalize(df_pre, ticker, START_DATE_UTC, END_DATE_UTC)\n",
    "                attempts.append({\"provider\": \"stooq\", \"attempt\": i + 1, \"ok\": True, \"rows\": int(len(df_norm)), \"exception_message\": None})\n",
    "                return df_norm, stats, attempts\n",
    "            except Exception as e_dl:\n",
    "                attempts.append({\"provider\": \"stooq\", \"attempt\": i + 1, \"ok\": False, \"rows\": 0, \"exception_message\": str(e_dl)})\n",
    "                break\n",
    "        except Exception as e:\n",
    "            attempts.append({\"provider\": \"stooq\", \"attempt\": i + 1, \"ok\": False, \"rows\": 0, \"exception_message\": str(e)})\n",
    "            break\n",
    "    return None, stats, attempts\n",
    "\n",
    "# =========================\n",
    "# Validações & Plano\n",
    "# =========================\n",
    "def validate_schema(df: pd.DataFrame) -> List[str]:\n",
    "    erros = []\n",
    "    if list(df.columns) != EXPECTED_COLUMNS:\n",
    "        erros.append(f\"VALIDATION_ERROR: schema de colunas incorreto. Esperado={EXPECTED_COLUMNS} Obtido={list(df.columns)}\")\n",
    "    dts = dtypes_signature(df)\n",
    "    for c, dt_expected in EXPECTED_DTYPES.items():\n",
    "        if c not in dts:\n",
    "            erros.append(f\"VALIDATION_ERROR: coluna ausente no DataFrame: {c}\")\n",
    "            continue\n",
    "        got = dts[c]\n",
    "        if c == \"ticker\":\n",
    "            if not got.startswith(\"string\"):\n",
    "                erros.append(f\"VALIDATION_ERROR: dtype incorreto para ticker. Esperado=string Obtido={got}\")\n",
    "        else:\n",
    "            if got != dt_expected:\n",
    "                erros.append(f\"VALIDATION_ERROR: dtype incorreto para {c}. Esperado={dt_expected} Obtido={got}\")\n",
    "    if df[\"ticker\"].isna().any():\n",
    "        erros.append(\"VALIDATION_ERROR: ticker contém valores nulos (deve ser 0%).\")\n",
    "    return erros\n",
    "\n",
    "def validate_quality(df: pd.DataFrame) -> List[str]:\n",
    "    erros = []\n",
    "    if len(df) < 2500:\n",
    "        erros.append(f\"VALIDATION_ERROR: cobertura insuficiente — linhas={len(df)} (< 2500)\")\n",
    "    pn = percent_nulls(df)\n",
    "    for col in [\"date\", \"close\", \"ticker\"]:\n",
    "        if round(pn.get(col, 100.0), 6) != 0.0:\n",
    "            erros.append(f\"VALIDATION_ERROR: % nulos em {col} deve ser 0%, obtido={pn.get(col, 100.0):.6f}%\")\n",
    "    dups = int(df.duplicated(subset=[\"date\"]).sum())\n",
    "    if dups != 0:\n",
    "        erros.append(f\"VALIDATION_ERROR: duplicatas por date detectadas (= {dups})\")\n",
    "    if not df[\"date\"].is_monotonic_increasing:\n",
    "        erros.append(\"VALIDATION_ERROR: coluna date não é monotônica crescente.\")\n",
    "    return erros\n",
    "\n",
    "def validate_interval_with_tolerance(df: pd.DataFrame, start_utc: pd.Timestamp) -> Tuple[List[str], Dict[str, Any]]:\n",
    "    erros = []\n",
    "    if df.empty:\n",
    "        return [\"VALIDATION_ERROR: DataFrame vazio após ingestão.\"], {\"date_min\": None, \"date_max\": None, \"start_verdict\": \"FAIL\", \"end_verdict\": \"FAIL\"}\n",
    "    dmin = pd.to_datetime(df[\"date\"].min())\n",
    "    dmax = pd.to_datetime(df[\"date\"].max())\n",
    "    required_start = start_utc.tz_convert(None).tz_localize(None) if start_utc.tzinfo else start_utc\n",
    "    start_tol_max = (required_start + BDay(5)).to_pydatetime().date()\n",
    "    start_ok = dmin <= pd.Timestamp(start_tol_max).to_pydatetime()\n",
    "    if not start_ok:\n",
    "        erros.append(f\"VALIDATION_ERROR: date.min ({dmin.date().isoformat()}) > tolerância de início ({start_tol_max.isoformat()})\")\n",
    "    required_end_min = (pd.Timestamp(datetime.now(timezone.utc)).normalize() - pd.Timedelta(days=3)).tz_localize(None)\n",
    "    end_ok = dmax >= required_end_min\n",
    "    if not end_ok:\n",
    "        erros.append(f\"VALIDATION_ERROR: date.max ({dmax.date().isoformat()}) < requerido mínimo ({required_end_min.date().isoformat()}) (tolerância 3 dias)\")\n",
    "    info = {\n",
    "        \"date_min\": dmin,\n",
    "        \"date_max\": dmax,\n",
    "        \"required_start\": required_start,\n",
    "        \"start_tolerance_max\": pd.Timestamp(start_tol_max),\n",
    "        \"required_end_min\": required_end_min,\n",
    "        \"start_verdict\": \"OK\" if start_ok else \"FAIL\",\n",
    "        \"end_verdict\": \"OK\" if end_ok else \"FAIL\",\n",
    "    }\n",
    "    return erros, info\n",
    "\n",
    "def build_persistence_plan(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    years = sorted(pd.to_datetime(df[\"date\"]).dt.year.unique().tolist())\n",
    "    partitions = [f\"year={y}\" for y in years]\n",
    "    manifesto_header = [\"timestamp\", \"ticker\", \"rows_total\", \"date_min\", \"date_max\", \"columns_json\", \"partitions_json\", \"target_path\"]\n",
    "    manifesto_row = [\n",
    "        AGORA.isoformat(),\n",
    "        TICKER,\n",
    "        int(len(df)),\n",
    "        str(pd.to_datetime(df[\"date\"]).min()),\n",
    "        str(pd.to_datetime(df[\"date\"]).max()),\n",
    "        json.dumps(EXPECTED_COLUMNS, ensure_ascii=False),\n",
    "        json.dumps(partitions, ensure_ascii=False),\n",
    "        str(PARQUET_TARGET),\n",
    "    ]\n",
    "    return {\n",
    "        \"parquet_target\": str(PARQUET_TARGET),\n",
    "        \"partitions\": partitions,\n",
    "        \"manifesto_path\": str(MANIFESTO_TARGET),\n",
    "        \"manifesto_header\": \",\".join(manifesto_header),\n",
    "        \"manifesto_row_sample\": \",\".join([str(x) for x in manifesto_row]),\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# Execução Principal\n",
    "# =========================\n",
    "def main():\n",
    "    provider_attempts: List[Dict[str, Any]] = []\n",
    "    erros_normativos: List[str] = []\n",
    "\n",
    "    bronze_ibov: Optional[pd.DataFrame] = None\n",
    "    used_provider: Optional[str] = None\n",
    "    cleaning_stats: Dict[str, int] = {\"rows_before_cleaning\": 0, \"rows_after_cleaning\": 0, \"rows_dropped_ohlc\": 0}\n",
    "\n",
    "    # P1: Yahoo Chart\n",
    "    df_yc, stats_yc, attempts_yc = fetch_yahoo_chart_direct(TICKER, START_DATE_UTC, PERIOD2_NOW_UTC)\n",
    "    provider_attempts.extend(attempts_yc)\n",
    "    if df_yc is not None and not df_yc.empty:\n",
    "        bronze_ibov = df_yc\n",
    "        used_provider = \"yahoo-chart\"\n",
    "        cleaning_stats = stats_yc\n",
    "    else:\n",
    "        # P2: yfinance (apenas se P1 falhar)\n",
    "        df_yf, stats_yf, attempts_yf = fetch_with_yfinance(TICKER, START_DATE_UTC, END_DATE_UTC)\n",
    "        provider_attempts.extend(attempts_yf)\n",
    "        if df_yf is not None and not df_yf.empty:\n",
    "            bronze_ibov = df_yf\n",
    "            used_provider = \"yfinance\"\n",
    "            cleaning_stats = stats_yf\n",
    "        else:\n",
    "            # P3: stooq (apenas se P1 e P2 falharem)\n",
    "            df_stq, stats_stq, attempts_stq = fetch_with_stooq(TICKER, START_DATE_UTC, END_DATE_UTC)\n",
    "            provider_attempts.extend(attempts_stq)\n",
    "            if df_stq is not None and not df_stq.empty:\n",
    "                bronze_ibov = df_stq\n",
    "                used_provider = \"stooq\"\n",
    "                cleaning_stats = stats_stq\n",
    "\n",
    "    # Se todos falharem\n",
    "    if bronze_ibov is None or bronze_ibov.empty:\n",
    "        print_section(\"PROVEDORES E TENTATIVAS\")\n",
    "        print(json.dumps(provider_attempts, ensure_ascii=False, indent=2))\n",
    "        # Selecionar a exceção mais informativa (última não-ok com mensagem)\n",
    "        last_err = None\n",
    "        for att in reversed(provider_attempts):\n",
    "            if not att.get(\"ok\") and att.get(\"exception_message\"):\n",
    "                last_err = att.get(\"exception_message\")\n",
    "                break\n",
    "        print(f\"VALIDATION_ERROR: PROVIDERS_EXHAUSTED — {last_err if last_err else 'sem mensagem detalhada.'}\")\n",
    "        print_section(\"CHECKLIST\")\n",
    "        checklist = {\n",
    "            \"provider_attempts_listed\": \"ok\",\n",
    "            \"schema_columns_and_dtypes_exact\": \"falha\",\n",
    "            \"interval_tolerance_verdicts\": \"falha\",\n",
    "            \"quality_nulls_and_duplicates\": \"falha\",\n",
    "            \"sample_head_tail_presented\": \"falha\",\n",
    "            \"counts_included\": \"falha\",\n",
    "            \"persistence_plan_simulated\": \"ok\",\n",
    "        }\n",
    "        print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "        for k, v in checklist.items():\n",
    "            if v != \"ok\":\n",
    "                print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "        print_section(\"DÚVIDAS OBJETIVAS\")\n",
    "        print(\"- Rede pode estar bloqueada para Yahoo/Stooq? Há Proxy que devamos configurar?\")\n",
    "        print(\"- Deseja fornecer outro provedor (AlphaVantage/Polygon) com chave?\")\n",
    "        print(\"- Autoriza aumentar timeouts/backoff e tentar novamente?\")\n",
    "        return\n",
    "\n",
    "    # Reforço de tipos/order e ticker\n",
    "    bronze_ibov = bronze_ibov.copy()\n",
    "    bronze_ibov[\"date\"] = pd.to_datetime(bronze_ibov[\"date\"], errors=\"coerce\").dt.normalize()\n",
    "    for c in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "        bronze_ibov[c] = pd.to_numeric(bronze_ibov[c], errors=\"coerce\").astype(\"float64\")\n",
    "    bronze_ibov[\"volume\"] = pd.to_numeric(bronze_ibov[\"volume\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "    bronze_ibov[\"ticker\"] = pd.Series([TICKER] * len(bronze_ibov), dtype=\"string\").astype(\"string\")\n",
    "    bronze_ibov = bronze_ibov[EXPECTED_COLUMNS].sort_values(\"date\").drop_duplicates(subset=[\"date\"], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "    # Validações\n",
    "    schema_errors = validate_schema(bronze_ibov)\n",
    "    qual_errors = validate_quality(bronze_ibov)\n",
    "    interval_errors, interval_info = validate_interval_with_tolerance(bronze_ibov, START_DATE_UTC)\n",
    "    erros_normativos.extend(schema_errors + qual_errors + interval_errors)\n",
    "\n",
    "    # Métricas\n",
    "    total_linhas = int(len(bronze_ibov))\n",
    "    dias_unicos = int(bronze_ibov[\"date\"].nunique()) if total_linhas > 0 else 0\n",
    "    dias_vol_zero = int((bronze_ibov[\"volume\"] == 0).sum()) if total_linhas > 0 else 0\n",
    "    pct_nulos = percent_nulls(bronze_ibov)\n",
    "    dups_by_date = int(bronze_ibov.duplicated(subset=[\"date\"]).sum())\n",
    "\n",
    "    # Plano de persistência (simulado)\n",
    "    persist_plan = build_persistence_plan(bronze_ibov)\n",
    "\n",
    "    # Relatórios\n",
    "    print_section(\"PROVEDOR E TENTATIVAS\")\n",
    "    print(json.dumps({\"provider_used\": used_provider, \"rows_returned\": total_linhas}, ensure_ascii=False, indent=2))\n",
    "    print(json.dumps(provider_attempts, ensure_ascii=False, indent=2))\n",
    "\n",
    "    print_section(\"SCHEMA (EXATO)\")\n",
    "    schema_out = {\n",
    "        \"columns_expected\": EXPECTED_COLUMNS,\n",
    "        \"columns_obtained\": list(bronze_ibov.columns),\n",
    "        \"dtypes_obtained\": dtypes_signature(bronze_ibov),\n",
    "        \"nulls_percent\": {k: round(v, 6) for k, v in pct_nulos.items()},\n",
    "        \"ticker_dtype_is_string\": str(bronze_ibov.dtypes[\"ticker\"]).startswith(\"string\"),\n",
    "        \"ticker_nulls_percent\": round(pct_nulos.get(\"ticker\", 100.0), 6),\n",
    "    }\n",
    "    print(json.dumps(schema_out, ensure_ascii=False, indent=2))\n",
    "\n",
    "    print_section(\"INTERVALO TEMPORAL (com tolerâncias)\")\n",
    "    interval_out = {\n",
    "        \"required_start\": str(interval_info[\"required_start\"]) if interval_info[\"date_min\"] is not None else None,\n",
    "        \"start_tolerance_max\": str(interval_info[\"start_tolerance_max\"]) if interval_info[\"date_min\"] is not None else None,\n",
    "        \"required_end_min\": str(interval_info[\"required_end_min\"]) if interval_info[\"date_max\"] is not None else None,\n",
    "        \"date_min\": str(pd.to_datetime(interval_info[\"date_min\"])) if interval_info[\"date_min\"] is not None else None,\n",
    "        \"date_max\": str(pd.to_datetime(interval_info[\"date_max\"])) if interval_info[\"date_max\"] is not None else None,\n",
    "        \"start_verdict\": interval_info.get(\"start_verdict\", \"FAIL\"),\n",
    "        \"end_verdict\": interval_info.get(\"end_verdict\", \"FAIL\"),\n",
    "    }\n",
    "    print(json.dumps(interval_out, ensure_ascii=False, indent=2))\n",
    "\n",
    "    print_section(\"QUALIDADE\")\n",
    "    qualidade_out = {\n",
    "        \"percent_nulls\": {k: round(v, 6) for k, v in pct_nulos.items()},\n",
    "        \"duplicates_by_date\": dups_by_date,\n",
    "        \"constraints\": {\n",
    "            \"nulls_must_be_zero_in\": {\"date\": True, \"close\": True, \"ticker\": True},\n",
    "            \"duplicates_by_date_must_be_zero\": True,\n",
    "            \"min_rows_required\": 2500,\n",
    "            \"date_monotonic_increasing\": True\n",
    "        }\n",
    "    }\n",
    "    print(json.dumps(qualidade_out, ensure_ascii=False, indent=2))\n",
    "\n",
    "    print_section(\"AMOSTRA — HEAD(10)\")\n",
    "    print(bronze_ibov[[\"date\", \"close\", \"volume\", \"ticker\"]].head(10).to_string(index=False))\n",
    "\n",
    "    print_section(\"AMOSTRA — TAIL(10)\")\n",
    "    print(bronze_ibov[[\"date\", \"close\", \"volume\", \"ticker\"]].tail(10).to_string(index=False))\n",
    "\n",
    "    print_section(\"CONTAGENS\")\n",
    "    print(json.dumps({\n",
    "        \"rows_before_cleaning\": cleaning_stats.get(\"rows_before_cleaning\", 0),\n",
    "        \"rows_dropped_ohlc\": cleaning_stats.get(\"rows_dropped_ohlc\", 0),\n",
    "        \"rows_after_cleaning\": cleaning_stats.get(\"rows_after_cleaning\", 0),\n",
    "        \"unique_days\": dias_unicos,\n",
    "        \"days_with_volume_zero\": dias_vol_zero,\n",
    "        \"final_rows\": total_linhas\n",
    "    }, ensure_ascii=False, indent=2))\n",
    "\n",
    "    print_section(\"PLANO DE PERSISTÊNCIA (SIMULADO)\")\n",
    "    print(json.dumps({\n",
    "        \"dry_run\": DRY_RUN,\n",
    "        \"parquet_target\": persist_plan[\"parquet_target\"],\n",
    "        \"partitions\": persist_plan[\"partitions\"],\n",
    "        \"manifesto_path\": persist_plan[\"manifesto_path\"],\n",
    "        \"manifesto_header\": persist_plan[\"manifesto_header\"],\n",
    "        \"manifesto_row_sample\": persist_plan[\"manifesto_row_sample\"],\n",
    "        \"nota\": \"Nenhuma escrita realizada em dry_run=True.\"\n",
    "    }, ensure_ascii=False, indent=2))\n",
    "\n",
    "    # Erros normativos (se houver)\n",
    "    if erros_normativos:\n",
    "        print_section(\"ERROS NORMATIVOS\")\n",
    "        seen = set()\n",
    "        ordered = []\n",
    "        for e in erros_normativos:\n",
    "            if e not in seen:\n",
    "                seen.add(e)\n",
    "                ordered.append(e)\n",
    "        for e in ordered:\n",
    "            if not (str(e).startswith(\"VALIDATION_ERROR\") or str(e).startswith(\"CHECKLIST_FAILURE\")):\n",
    "                print(f\"VALIDATION_ERROR: {e}\")\n",
    "            else:\n",
    "                print(e)\n",
    "\n",
    "    # Checklist\n",
    "    print_section(\"CHECKLIST\")\n",
    "    schema_ok = (len(schema_errors := schema_errors if 'schema_errors' in locals() else validate_schema(bronze_ibov)) == 0)  # revalida se necessário\n",
    "    interval_ok = (len(interval_errors) == 0 and interval_info.get(\"start_verdict\") == \"OK\" and interval_info.get(\"end_verdict\") == \"OK\")\n",
    "    quality_ok = (len(qual_errors := qual_errors if 'qual_errors' in locals() else validate_quality(bronze_ibov)) == 0)\n",
    "    sample_ok = (total_linhas > 0)\n",
    "    counts_ok = True  # contagens sempre apresentadas\n",
    "    attempts_ok = True\n",
    "    plan_ok = True\n",
    "\n",
    "    checklist = {\n",
    "        \"provider_attempts_listed\": \"ok\" if attempts_ok else \"falha\",\n",
    "        \"schema_columns_and_dtypes_exact\": \"ok\" if schema_ok else \"falha\",\n",
    "        \"interval_tolerance_verdicts\": \"ok\" if interval_ok else \"falha\",\n",
    "        \"quality_nulls_and_duplicates\": \"ok\" if quality_ok else \"falha\",\n",
    "        \"sample_head_tail_presented\": \"ok\" if sample_ok else \"falha\",\n",
    "        \"counts_included\": \"ok\" if counts_ok else \"falha\",\n",
    "        \"persistence_plan_simulated\": \"ok\" if plan_ok else \"falha\",\n",
    "    }\n",
    "    print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "    for k, v in checklist.items():\n",
    "        if v != \"ok\":\n",
    "            print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "\n",
    "    # Estrutura do Resultado (info)\n",
    "    print_section(\"ESTRUTURA DO RESULTADO (info)\")\n",
    "    resultado = {\n",
    "        \"ticker\": TICKER,\n",
    "        \"periodo\": {\"start\": str(START_DATE_UTC.tz_localize(None)), \"end\": str(END_DATE_UTC.tz_localize(None))},\n",
    "        \"dry_run\": DRY_RUN,\n",
    "        \"timestamp_execucao\": AGORA.isoformat(),\n",
    "        \"dataframe_name\": \"bronze_ibov\",\n",
    "        \"columns\": EXPECTED_COLUMNS,\n",
    "        \"dtypes\": dtypes_signature(bronze_ibov),\n",
    "        \"provider_used\": used_provider,\n",
    "        \"status\": \"sucesso\" if not erros_normativos and all(v == \"ok\" for v in checklist.values()) else \"falha\"\n",
    "    }\n",
    "    print(json.dumps(resultado, ensure_ascii=False, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Contrato\n",
    "    # - Coleta direta Yahoo Chart (requests/stdlib) → yfinance → stooq (sem dados sintéticos)\n",
    "    # - Normalização Bronze e validações: schema, qualidade, tolerâncias de calendário\n",
    "    # - Planos de persistência (simulados), checklist e mensagens normativas\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26593eb",
   "metadata": {},
   "source": [
    "# Instrução 1B-RETRY — Persistir Bronze “valendo” (escrita real + manifesto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92d2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== PREFLIGHT QUALITY ========\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"details\": {\n",
      "    \"percent_nulls\": {\n",
      "      \"date\": 0.0,\n",
      "      \"open\": 0.0,\n",
      "      \"high\": 0.0,\n",
      "      \"low\": 0.0,\n",
      "      \"close\": 0.0,\n",
      "      \"volume\": 0.0,\n",
      "      \"ticker\": 0.0\n",
      "    },\n",
      "    \"duplicates_by_date\": 0,\n",
      "    \"date_min\": \"2012-01-03 00:00:00\",\n",
      "    \"date_max\": \"2025-09-19 00:00:00\"\n",
      "  },\n",
      "  \"errors\": []\n",
      "}\n",
      "\n",
      "======== PARQUET WRITE ========\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"years_written\": [\n",
      "    2012,\n",
      "    2013,\n",
      "    2014,\n",
      "    2015,\n",
      "    2016,\n",
      "    2017,\n",
      "    2018,\n",
      "    2019,\n",
      "    2020,\n",
      "    2021,\n",
      "    2022,\n",
      "    2023,\n",
      "    2024,\n",
      "    2025\n",
      "  ],\n",
      "  \"files_per_partition\": {\n",
      "    \"2012\": 1,\n",
      "    \"2013\": 1,\n",
      "    \"2014\": 1,\n",
      "    \"2015\": 1,\n",
      "    \"2016\": 1,\n",
      "    \"2017\": 1,\n",
      "    \"2018\": 1,\n",
      "    \"2019\": 1,\n",
      "    \"2020\": 1,\n",
      "    \"2021\": 1,\n",
      "    \"2022\": 1,\n",
      "    \"2023\": 1,\n",
      "    \"2024\": 1,\n",
      "    \"2025\": 1\n",
      "  }\n",
      "}\n",
      "\n",
      "======== POST-WRITE VERIFICATION ========\n",
      "{\n",
      "  \"rows_total\": 3400,\n",
      "  \"min_date\": \"2012-01-03 00:00:00\",\n",
      "  \"max_date\": \"2025-09-19 00:00:00\"\n",
      "}\n",
      "\n",
      "======== MANIFESTO APPEND ========\n",
      "2025-09-19T10:24:34.637271-03:00,^BVSP,3400,2012-01-03 00:00:00,2025-09-19 00:00:00,[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"ticker\"],[\"year=2012\", \"year=2013\", \"year=2014\", \"year=2015\", \"year=2016\", \"year=2017\", \"year=2018\", \"year=2019\", \"year=2020\", \"year=2021\", \"year=2022\", \"year=2023\", \"year=2024\", \"year=2025\"],/home/wrm/BOLSA_2026/bronze/IBOV.parquet\n",
      "\n",
      "======== CHECKLIST ========\n",
      "{\n",
      "  \"preflight_quality_ok\": \"ok\",\n",
      "  \"parquet_write_summary\": \"ok\",\n",
      "  \"post_write_verification\": \"ok\",\n",
      "  \"manifesto_append_ok\": \"ok\"\n",
      "}\n",
      "\n",
      "======== PREFLIGHT QUALITY ========\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"details\": {\n",
      "    \"percent_nulls\": {\n",
      "      \"date\": 0.0,\n",
      "      \"open\": 0.0,\n",
      "      \"high\": 0.0,\n",
      "      \"low\": 0.0,\n",
      "      \"close\": 0.0,\n",
      "      \"volume\": 0.0,\n",
      "      \"ticker\": 0.0\n",
      "    },\n",
      "    \"duplicates_by_date\": 0,\n",
      "    \"date_min\": \"2012-01-03 00:00:00\",\n",
      "    \"date_max\": \"2025-09-19 00:00:00\"\n",
      "  },\n",
      "  \"errors\": []\n",
      "}\n",
      "\n",
      "======== PARQUET WRITE ========\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"years_written\": [\n",
      "    2012,\n",
      "    2013,\n",
      "    2014,\n",
      "    2015,\n",
      "    2016,\n",
      "    2017,\n",
      "    2018,\n",
      "    2019,\n",
      "    2020,\n",
      "    2021,\n",
      "    2022,\n",
      "    2023,\n",
      "    2024,\n",
      "    2025\n",
      "  ],\n",
      "  \"files_per_partition\": {\n",
      "    \"2012\": 1,\n",
      "    \"2013\": 1,\n",
      "    \"2014\": 1,\n",
      "    \"2015\": 1,\n",
      "    \"2016\": 1,\n",
      "    \"2017\": 1,\n",
      "    \"2018\": 1,\n",
      "    \"2019\": 1,\n",
      "    \"2020\": 1,\n",
      "    \"2021\": 1,\n",
      "    \"2022\": 1,\n",
      "    \"2023\": 1,\n",
      "    \"2024\": 1,\n",
      "    \"2025\": 1\n",
      "  }\n",
      "}\n",
      "\n",
      "======== POST-WRITE VERIFICATION ========\n",
      "{\n",
      "  \"rows_total\": 3400,\n",
      "  \"min_date\": \"2012-01-03 00:00:00\",\n",
      "  \"max_date\": \"2025-09-19 00:00:00\"\n",
      "}\n",
      "\n",
      "======== MANIFESTO APPEND ========\n",
      "2025-09-19T10:24:34.637271-03:00,^BVSP,3400,2012-01-03 00:00:00,2025-09-19 00:00:00,[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"ticker\"],[\"year=2012\", \"year=2013\", \"year=2014\", \"year=2015\", \"year=2016\", \"year=2017\", \"year=2018\", \"year=2019\", \"year=2020\", \"year=2021\", \"year=2022\", \"year=2023\", \"year=2024\", \"year=2025\"],/home/wrm/BOLSA_2026/bronze/IBOV.parquet\n",
      "\n",
      "======== CHECKLIST ========\n",
      "{\n",
      "  \"preflight_quality_ok\": \"ok\",\n",
      "  \"parquet_write_summary\": \"ok\",\n",
      "  \"post_write_verification\": \"ok\",\n",
      "  \"manifesto_append_ok\": \"ok\"\n",
      "}\n",
      "\n",
      "======== PREFLIGHT QUALITY ========\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"details\": {\n",
      "    \"percent_nulls\": {\n",
      "      \"date\": 0.0,\n",
      "      \"open\": 0.0,\n",
      "      \"high\": 0.0,\n",
      "      \"low\": 0.0,\n",
      "      \"close\": 0.0,\n",
      "      \"volume\": 0.0,\n",
      "      \"ticker\": 0.0\n",
      "    },\n",
      "    \"duplicates_by_date\": 0,\n",
      "    \"date_min\": \"2012-01-03 00:00:00\",\n",
      "    \"date_max\": \"2025-09-19 00:00:00\"\n",
      "  },\n",
      "  \"errors\": []\n",
      "}\n",
      "\n",
      "======== PARQUET WRITE ========\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"years_written\": [\n",
      "    2012,\n",
      "    2013,\n",
      "    2014,\n",
      "    2015,\n",
      "    2016,\n",
      "    2017,\n",
      "    2018,\n",
      "    2019,\n",
      "    2020,\n",
      "    2021,\n",
      "    2022,\n",
      "    2023,\n",
      "    2024,\n",
      "    2025\n",
      "  ],\n",
      "  \"files_per_partition\": {\n",
      "    \"2012\": 1,\n",
      "    \"2013\": 1,\n",
      "    \"2014\": 1,\n",
      "    \"2015\": 1,\n",
      "    \"2016\": 1,\n",
      "    \"2017\": 1,\n",
      "    \"2018\": 1,\n",
      "    \"2019\": 1,\n",
      "    \"2020\": 1,\n",
      "    \"2021\": 1,\n",
      "    \"2022\": 1,\n",
      "    \"2023\": 1,\n",
      "    \"2024\": 1,\n",
      "    \"2025\": 1\n",
      "  }\n",
      "}\n",
      "\n",
      "======== POST-WRITE VERIFICATION ========\n",
      "{\n",
      "  \"rows_total\": 3400,\n",
      "  \"min_date\": \"2012-01-03 00:00:00\",\n",
      "  \"max_date\": \"2025-09-19 00:00:00\"\n",
      "}\n",
      "\n",
      "======== MANIFESTO APPEND ========\n",
      "2025-09-19T10:24:34.637271-03:00,^BVSP,3400,2012-01-03 00:00:00,2025-09-19 00:00:00,[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"ticker\"],[\"year=2012\", \"year=2013\", \"year=2014\", \"year=2015\", \"year=2016\", \"year=2017\", \"year=2018\", \"year=2019\", \"year=2020\", \"year=2021\", \"year=2022\", \"year=2023\", \"year=2024\", \"year=2025\"],/home/wrm/BOLSA_2026/bronze/IBOV.parquet\n",
      "\n",
      "======== CHECKLIST ========\n",
      "{\n",
      "  \"preflight_quality_ok\": \"ok\",\n",
      "  \"parquet_write_summary\": \"ok\",\n",
      "  \"post_write_verification\": \"ok\",\n",
      "  \"manifesto_append_ok\": \"ok\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Instrução 1B-RETRY — Persistir Bronze “valendo” (escrita real + manifesto)\n",
    "# Regras:\n",
    "# - Bloco único, auto-contido.\n",
    "# - dry_run desligado nesta célula (escrita real).\n",
    "# - Usar bronze_ibov em memória; se ausente, reingestar silenciosamente (Yahoo Chart → yfinance → stooq).\n",
    "# - Parquet particionado por year=YYYY, compressão snappy, overwrite-by-partition.\n",
    "# - Manifesto: criar se faltar; adicionar linha com metadados (sem hashes).\n",
    "# - Mensagens normativas: VALIDATION_ERROR / CHECKLIST_FAILURE.\n",
    "# - Em 2 falhas consecutivas, parar e emitir dúvidas objetivas.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# Parâmetros (SSOT)\n",
    "# =========================\n",
    "ROOT_DIR = Path(\"/home/wrm/BOLSA_2026\").resolve()\n",
    "PARQUET_TARGET = ROOT_DIR / \"bronze\" / \"IBOV.parquet\"  # diretório de dataset particionado (hive: year=YYYY)\n",
    "MANIFESTO_PATH = ROOT_DIR / \"manifestos\" / \"bronze_ibov_manifesto.csv\"\n",
    "TICKER = \"^BVSP\"\n",
    "DRY_RUN = False  # escrita real nesta etapa\n",
    "\n",
    "EXPECTED_COLUMNS = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"ticker\"]\n",
    "AGORA_TZ = datetime.now().astimezone()\n",
    "START_DATE_UTC = pd.Timestamp(\"2012-01-01\", tz=\"UTC\")\n",
    "NOW_UTC = pd.Timestamp(datetime.now(timezone.utc))\n",
    "END_DATE_UTC = NOW_UTC.normalize()\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "def print_section(title: str):\n",
    "    print(\"\\n\" + \"=\" * 8 + f\" {title} \" + \"=\" * 8)\n",
    "\n",
    "def dtypes_signature(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    return {c: str(df.dtypes[c]) for c in df.columns}\n",
    "\n",
    "def percent_nulls(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    total = len(df)\n",
    "    if total == 0:\n",
    "        return {c: 100.0 for c in df.columns}\n",
    "    return {c: float(df[c].isna().sum()) * 100.0 / float(total) for c in df.columns}\n",
    "\n",
    "def to_unix_seconds(ts: pd.Timestamp) -> int:\n",
    "    if ts.tzinfo is None:\n",
    "        ts = ts.tz_localize(\"UTC\")\n",
    "    else:\n",
    "        ts = ts.tz_convert(\"UTC\")\n",
    "    return int(ts.timestamp())\n",
    "\n",
    "def bronze_normalize(df_pre: pd.DataFrame, ticker: str) -> pd.DataFrame:\n",
    "    # Espera colunas: date, open, high, low, close, volume\n",
    "    need = {\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"}\n",
    "    if not need.issubset(df_pre.columns):\n",
    "        missing = sorted(list(need - set(df_pre.columns)))\n",
    "        raise RuntimeError(f\"SCHEMA_ERROR: colunas ausentes: {missing}\")\n",
    "    df = df_pre.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True).dt.tz_localize(None).dt.normalize()\n",
    "    for c in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float64\")\n",
    "    df[\"volume\"] = pd.to_numeric(df[\"volume\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "    df[\"ticker\"] = pd.Series([ticker] * len(df), dtype=\"string\").astype(\"string\")\n",
    "    # limpeza: remover OHLC nulos\n",
    "    mask_ohlc = (~df[\"open\"].isna()) & (~df[\"high\"].isna()) & (~df[\"low\"].isna()) & (~df[\"close\"].isna())\n",
    "    df = df[mask_ohlc].copy()\n",
    "    # ordenar, deduplicar por date\n",
    "    df = df.sort_values(\"date\").drop_duplicates(subset=[\"date\"], keep=\"last\").reset_index(drop=True)\n",
    "    # filtrar intervalo#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Instrução 1B-RETRY — Persistir Bronze “valendo” (escrita real + manifesto)\n",
    "# Regras:\n",
    "# - Bloco único, auto-contido.\n",
    "# - dry_run desligado nesta célula (escrita real).\n",
    "# - Usar bronze_ibov em memória; se ausente, reingestar silenciosamente (Yahoo Chart → yfinance → stooq).\n",
    "# - Parquet particionado por year=YYYY, compressão snappy, overwrite-by-partition.\n",
    "# - Manifesto: criar se faltar; adicionar linha com metadados (sem hashes).\n",
    "# - Mensagens normativas: VALIDATION_ERROR / CHECKLIST_FAILURE.\n",
    "# - Em 2 falhas consecutivas, parar e emitir dúvidas objetivas.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# Parâmetros (SSOT)\n",
    "# =========================\n",
    "ROOT_DIR = Path(\"/home/wrm/BOLSA_2026\").resolve()\n",
    "PARQUET_TARGET = ROOT_DIR / \"bronze\" / \"IBOV.parquet\"  # diretório de dataset particionado (hive: year=YYYY)\n",
    "MANIFESTO_PATH = ROOT_DIR / \"manifestos\" / \"bronze_ibov_manifesto.csv\"\n",
    "TICKER = \"^BVSP\"\n",
    "DRY_RUN = False  # escrita real nesta etapa\n",
    "\n",
    "EXPECTED_COLUMNS = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"ticker\"]\n",
    "AGORA_TZ = datetime.now().astimezone()\n",
    "START_DATE_UTC = pd.Timestamp(\"2012-01-01\", tz=\"UTC\")\n",
    "NOW_UTC = pd.Timestamp(datetime.now(timezone.utc))\n",
    "END_DATE_UTC = NOW_UTC.normalize()\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "def print_section(title: str):\n",
    "    print(\"\\n\" + \"=\" * 8 + f\" {title} \" + \"=\" * 8)\n",
    "\n",
    "def dtypes_signature(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    return {c: str(df.dtypes[c]) for c in df.columns}\n",
    "\n",
    "def percent_nulls(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    total = len(df)\n",
    "    if total == 0:\n",
    "        return {c: 100.0 for c in df.columns}\n",
    "    return {c: float(df[c].isna().sum()) * 100.0 / float(total) for c in df.columns}\n",
    "\n",
    "def to_unix_seconds(ts: pd.Timestamp) -> int:\n",
    "    if ts.tzinfo is None:\n",
    "        ts = ts.tz_localize(\"UTC\")\n",
    "    else:\n",
    "        ts = ts.tz_convert(\"UTC\")\n",
    "    return int(ts.timestamp())\n",
    "\n",
    "def bronze_normalize(df_pre: pd.DataFrame, ticker: str) -> pd.DataFrame:\n",
    "    # Espera colunas: date, open, high, low, close, volume\n",
    "    need = {\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"}\n",
    "    if not need.issubset(df_pre.columns):\n",
    "        missing = sorted(list(need - set(df_pre.columns)))\n",
    "        raise RuntimeError(f\"SCHEMA_ERROR: colunas ausentes: {missing}\")\n",
    "    df = df_pre.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True).dt.tz_localize(None).dt.normalize()\n",
    "    for c in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float64\")\n",
    "    df[\"volume\"] = pd.to_numeric(df[\"volume\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "    df[\"ticker\"] = pd.Series([ticker] * len(df), dtype=\"string\").astype(\"string\")\n",
    "    # limpeza: remover OHLC nulos\n",
    "    mask_ohlc = (~df[\"open\"].isna()) & (~df[\"high\"].isna()) & (~df[\"low\"].isna()) & (~df[\"close\"].isna())\n",
    "    df = df[mask_ohlc].copy()\n",
    "    # ordenar, deduplicar por date\n",
    "    df = df.sort_values(\"date\").drop_duplicates(subset=[\"date\"], keep=\"last\").reset_index(drop=True)\n",
    "    # filtrar intervalo\n",
    "    start_naive = START_DATE_UTC.tz_convert(None).tz_localize(None) if START_DATE_UTC.tzinfo else START_DATE_UTC\n",
    "    end_naive = END_DATE_UTC.tz_convert(None).tz_localize(None) if END_DATE_UTC.tzinfo else END_DATE_UTC\n",
    "    df = df[(df[\"date\"] >= start_naive) & (df[\"date\"] <= end_naive)].copy()\n",
    "    # coluna e ordem final\n",
    "    df = df[EXPECTED_COLUMNS]\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# Reingestão silenciosa (apenas se bronze_ibov não existir)\n",
    "# =========================\n",
    "def fetch_yahoo_chart_silent(ticker: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        base_url = \"https://query2.finance.yahoo.com/v8/finance/chart/%5EBVSP\"\n",
    "        params = {\n",
    "            \"period1\": str(to_unix_seconds(START_DATE_UTC)),\n",
    "            \"period2\": str(to_unix_seconds(NOW_UTC)),\n",
    "            \"interval\": \"1d\",\n",
    "            \"events\": \"history\",\n",
    "            \"includeAdjustedClose\": \"false\",\n",
    "        }\n",
    "        try:\n",
    "            import requests  # type: ignore\n",
    "            r = requests.get(base_url, params=params, headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) Python\"}, timeout=6)\n",
    "            if r.status_code < 200 or r.status_code >= 400:\n",
    "                return None\n",
    "            data = r.json()\n",
    "        except Exception:\n",
    "            from urllib.parse import urlencode\n",
    "            from urllib.request import Request, urlopen\n",
    "            url = base_url + \"?\" + urlencode(params)\n",
    "            req = Request(url, headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) Python\"})\n",
    "            with urlopen(req, timeout=8) as resp:\n",
    "                raw = resp.read()\n",
    "            import json as _json\n",
    "            data = _json.loads(raw.decode(\"utf-8\"))\n",
    "        if \"chart\" not in data or not data[\"chart\"].get(\"result\"):\n",
    "            return None\n",
    "        res0 = data[\"chart\"][\"result\"][0]\n",
    "        ts = res0.get(\"timestamp\", []) or []\n",
    "        q = (res0.get(\"indicators\", {}) or {}).get(\"quote\", []) or []\n",
    "        if not q:\n",
    "            return None\n",
    "        q0 = q[0]\n",
    "        opens = q0.get(\"open\", []) or []\n",
    "        highs = q0.get(\"high\", []) or []\n",
    "        lows = q0.get(\"low\", []) or []\n",
    "        closes = q0.get(\"close\", []) or []\n",
    "        vols = q0.get(\"volume\", []) or []\n",
    "        n = min(len(ts), len(opens), len(highs), len(lows), len(closes), len(vols))\n",
    "        if n == 0:\n",
    "            return None\n",
    "        df_pre = pd.DataFrame({\n",
    "            \"date\": pd.to_datetime(ts[:n], unit=\"s\", utc=True),\n",
    "            \"open\": opens[:n],\n",
    "            \"high\": highs[:n],\n",
    "            \"low\": lows[:n],\n",
    "            \"close\": closes[:n],\n",
    "            \"volume\": vols[:n],\n",
    "        })\n",
    "        return bronze_normalize(df_pre, ticker)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fetch_yfinance_silent(ticker: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        try:\n",
    "            import yfinance as yf  # type: ignore\n",
    "        except Exception:\n",
    "            return None\n",
    "        start_str = START_DATE_UTC.tz_localize(None).date().isoformat() if START_DATE_UTC.tzinfo else START_DATE_UTC.date().isoformat()\n",
    "        end_inc = (END_DATE_UTC + pd.Timedelta(days=1))\n",
    "        end_str = end_inc.tz_localize(None).date().isoformat() if end_inc.tzinfo else end_inc.date().isoformat()\n",
    "        df_raw = yf.download(tickers=ticker, start=start_str, end=end_str, interval=\"1d\", auto_adjust=False, progress=False, threads=True)\n",
    "        if df_raw is None or df_raw.empty:\n",
    "            return None\n",
    "        if isinstance(df_raw.columns, pd.MultiIndex):\n",
    "            try:\n",
    "                df_raw.columns = [c[-1] if isinstance(c, tuple) else c for c in df_raw.columns.to_list()]\n",
    "            except Exception:\n",
    "                df_raw.columns = df_raw.columns.get_level_values(-1)\n",
    "        rename_map = {\"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\", \"Volume\": \"volume\",\n",
    "                      \"open\": \"open\", \"high\": \"high\", \"low\": \"low\", \"close\": \"close\", \"volume\": \"volume\"}\n",
    "        df_raw = df_raw.rename(columns=rename_map)\n",
    "        need = {\"open\", \"high\", \"low\", \"close\", \"volume\"}\n",
    "        if not need.issubset(set(df_raw.columns)):\n",
    "            return None\n",
    "        df_pre = df_raw.reset_index().rename(columns={\"Date\": \"date\", \"Datetime\": \"date\"})\n",
    "        if \"date\" not in df_pre.columns:\n",
    "            df_pre = df_raw.copy()\n",
    "            df_pre[\"date\"] = df_pre.index\n",
    "            df_pre = df_pre.reset_index(drop=True)\n",
    "        df_pre = df_pre[[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "        return bronze_normalize(df_pre, ticker)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fetch_stooq_silent(ticker: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        try:\n",
    "            from pandas_datareader import data as dr  # type: ignore\n",
    "        except Exception:\n",
    "            return None\n",
    "        candidates = [ticker, ticker.replace(\"^\", \"\"), ticker.replace(\"^\", \"\").lower()]\n",
    "        df_raw = None\n",
    "        last_exc = None\n",
    "        for tk in candidates:\n",
    "            try:\n",
    "                df_raw = dr.DataReader(tk, \"stooq\", start=START_DATE_UTC.tz_localize(None), end=END_DATE_UTC.tz_localize(None))\n",
    "                if df_raw is not None and not df_raw.empty:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                last_exc = e\n",
    "                continue\n",
    "        if df_raw is None or df_raw.empty:\n",
    "            return None\n",
    "        df_raw = df_raw.sort_index()\n",
    "        if isinstance(df_raw.columns, pd.MultiIndex):\n",
    "            try:\n",
    "                df_raw.columns = [c[-1] if isinstance(c, tuple) else c for c in df_raw.columns.to_list()]\n",
    "            except Exception:\n",
    "                df_raw.columns = df_raw.columns.get_level_values(-1)\n",
    "        rename_map = {\"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\", \"Volume\": \"volume\",\n",
    "                      \"open\": \"open\", \"high\": \"high\", \"low\": \"low\", \"close\": \"close\", \"volume\": \"volume\"}\n",
    "        df_raw = df_raw.rename(columns=rename_map)\n",
    "        need = {\"open\", \"high\", \"low\", \"close\", \"volume\"}\n",
    "        if not need.issubset(set(df_raw.columns)):\n",
    "            return None\n",
    "        df_pre = df_raw.copy()\n",
    "        df_pre[\"date\"] = df_pre.index\n",
    "        df_pre = df_pre.reset_index(drop=True)[[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "        return bronze_normalize(df_pre, ticker)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def ensure_bronze_in_memory() -> Tuple[Optional[pd.DataFrame], List[str]]:\n",
    "    msgs: List[str] = []\n",
    "    g = globals()\n",
    "    if \"bronze_ibov\" in g and isinstance(g[\"bronze_ibov\"], pd.DataFrame):\n",
    "        df = g[\"bronze_ibov\"].copy()\n",
    "        # Reforçar schema/dtypes\n",
    "        try:\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.normalize()\n",
    "            for c in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "                df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float64\")\n",
    "            df[\"volume\"] = pd.to_numeric(df[\"volume\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "            df[\"ticker\"] = df[\"ticker\"].astype(\"string\")\n",
    "            df = df[EXPECTED_COLUMNS].sort_values(\"date\").drop_duplicates(subset=[\"date\"], keep=\"last\").reset_index(drop=True)\n",
    "            return df, msgs\n",
    "        except Exception as e:\n",
    "            msgs.append(f\"INFO: bronze_ibov em memória com inconsistências — {e}; reingestão silenciosa será tentada.\")\n",
    "    # Reingestão silenciosa\n",
    "    for fn in (fetch_yahoo_chart_silent, fetch_yfinance_silent, fetch_stooq_silent):\n",
    "        df = fn(TICKER)\n",
    "        if df is not None and not df.empty:\n",
    "            return df, msgs\n",
    "    msgs.append(\"VALIDATION_ERROR: PROVIDERS_EXHAUSTED — não foi possível reingestar bronze_ibov.\")\n",
    "    return None, msgs\n",
    "\n",
    "# =========================\n",
    "# Pré-voo de qualidade\n",
    "# =========================\n",
    "def preflight_checks(df: pd.DataFrame) -> Tuple[bool, Dict[str, Any], List[str]]:\n",
    "    errs: List[str] = []\n",
    "    details: Dict[str, Any] = {}\n",
    "    if df is None or df.empty:\n",
    "        errs.append(\"VALIDATION_ERROR: DataFrame vazio.\")\n",
    "    else:\n",
    "        pn = percent_nulls(df)\n",
    "        details[\"percent_nulls\"] = {k: round(v, 6) for k, v in pn.items()}\n",
    "        for col in [\"date\", \"close\", \"ticker\"]:\n",
    "            if round(pn.get(col, 100.0), 6) != 0.0:\n",
    "                errs.append(f\"VALIDATION_ERROR: % nulos em {col} deve ser 0%, obtido={pn.get(col, 100.0):.6f}%\")\n",
    "        dups = int(df.duplicated(subset=[\"date\"]).sum())\n",
    "        details[\"duplicates_by_date\"] = dups\n",
    "        if dups != 0:\n",
    "            errs.append(f\"VALIDATION_ERROR: duplicatas por date detectadas (= {dups})\")\n",
    "        if len(df) < 2500:\n",
    "            errs.append(f\"VALIDATION_ERROR: cobertura insuficiente — linhas={len(df)} (< 2500)\")\n",
    "        dmin = pd.to_datetime(df[\"date\"]).min()\n",
    "        dmax = pd.to_datetime(df[\"date\"]).max()\n",
    "        details[\"date_min\"] = str(dmin)\n",
    "        details[\"date_max\"] = str(dmax)\n",
    "        # Tolerâncias: início ≤ 2012-01-06; fim ≥ hoje(UTC) − 3d\n",
    "        if dmin > pd.Timestamp(\"2012-01-06\"):\n",
    "            errs.append(f\"VALIDATION_ERROR: date.min ({dmin.date().isoformat()}) > tolerância (2012-01-06)\")\n",
    "        required_end_min = (pd.Timestamp(datetime.now(timezone.utc)).normalize() - pd.Timedelta(days=3)).tz_localize(None)\n",
    "        if dmax < required_end_min:\n",
    "            errs.append(f\"VALIDATION_ERROR: date.max ({dmax.date().isoformat()}) < requerido mínimo ({required_end_min.date().isoformat()}) (tolerância 3 dias)\")\n",
    "    return (len(errs) == 0), details, errs\n",
    "\n",
    "# =========================\n",
    "# Escrita Parquet particionado (overwrite-by-partition)\n",
    "# =========================\n",
    "def write_parquet_partitioned(df: pd.DataFrame, base_dir: Path, partition_col: str = \"year\") -> Tuple[bool, Dict[str, Any], List[str]]:\n",
    "    \"\"\"\n",
    "    Escreve com pyarrow.parquet.write_to_dataset, compressão snappy,\n",
    "    particionado por 'year', com existing_data_behavior='delete_matching' (overwrite-by-partition).\n",
    "    Retorna (ok, summary, errors).\n",
    "    \"\"\"\n",
    "    errors: List[str] = []\n",
    "    summary: Dict[str, Any] = {\"years_written\": [], \"files_per_partition\": {}}\n",
    "    try:\n",
    "        import pyarrow as pa  # type: ignore\n",
    "        import pyarrow.parquet as pq  # type: ignore\n",
    "    except Exception as e:\n",
    "        errors.append(f\"VALIDATION_ERROR: MISSING_DEPENDENCY_PYARROW — {e}\")\n",
    "        return False, summary, errors\n",
    "\n",
    "    try:\n",
    "        base_dir.mkdir(parents=True, exist_ok=True)\n",
    "        df2 = df.copy()\n",
    "        years = pd.to_datetime(df2[\"date\"]).dt.year.astype(\"int16\")\n",
    "        df2[partition_col] = years\n",
    "        table = pa.Table.from_pandas(df2, preserve_index=False)\n",
    "        # Escreve dataset\n",
    "        pq.write_to_dataset(\n",
    "            table=table,\n",
    "            root_path=str(base_dir),\n",
    "            partition_cols=[partition_col],\n",
    "            compression=\"snappy\",\n",
    "            existing_data_behavior=\"delete_matching\"  # overwrite-by-partition\n",
    "        )\n",
    "        # Sumário por partição escrita\n",
    "        written_years = sorted(pd.unique(years).astype(int).tolist())\n",
    "        summary[\"years_written\"] = written_years\n",
    "        files_per = {}\n",
    "        for y in written_years:\n",
    "            p = base_dir / f\"{partition_col}={y}\"\n",
    "            cnt = 0\n",
    "            if p.exists() and p.is_dir():\n",
    "                for _, _, files in os.walk(p):\n",
    "                    cnt += sum(1 for f in files if f.endswith(\".parquet\"))\n",
    "            files_per[str(y)] = cnt\n",
    "        summary[\"files_per_partition\"] = files_per\n",
    "        return True, summary, errors\n",
    "    except Exception as e:\n",
    "        errors.append(f\"VALIDATION_ERROR: PARQUET_WRITE_ERROR — {e}\")\n",
    "        return False, summary, errors\n",
    "\n",
    "# =========================\n",
    "# Reabertura pós-escrita\n",
    "# =========================\n",
    "def reopen_dataset_summary(base_dir: Path) -> Tuple[Optional[pd.DataFrame], Dict[str, Any], List[str]]:\n",
    "    errors: List[str] = []\n",
    "    info: Dict[str, Any] = {\"rows_total\": 0, \"min_date\": None, \"max_date\": None}\n",
    "    try:\n",
    "        import pyarrow.dataset as ds  # type: ignore\n",
    "        dataset = ds.dataset(str(base_dir), format=\"parquet\")\n",
    "        table = dataset.to_table()\n",
    "        df = table.to_pandas()\n",
    "    except Exception as e1:\n",
    "        errors.append(f\"READ_ERROR_PA_DS: {e1}\")\n",
    "        try:\n",
    "            df = pd.read_parquet(str(base_dir))\n",
    "        except Exception as e2:\n",
    "            errors.append(f\"READ_ERROR_PD_RP: {e2}\")\n",
    "            return None, info, errors\n",
    "    # Normaliza e resume\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.tz_localize(None).dt.normalize()\n",
    "    info[\"rows_total\"] = int(len(df))\n",
    "    info[\"min_date\"] = str(pd.to_datetime(df[\"date\"]).min()) if \"date\" in df.columns and not df[\"date\"].isna().all() else None\n",
    "    info[\"max_date\"] = str(pd.to_datetime(df[\"date\"]).max()) if \"date\" in df.columns and not df[\"date\"].isna().all() else None\n",
    "    return df, info, errors\n",
    "\n",
    "# =========================\n",
    "# Manifesto (append ou create)\n",
    "# =========================\n",
    "def append_manifesto_row(\n",
    "    manifesto_path: Path,\n",
    "    ticker: str,\n",
    "    df_written: pd.DataFrame,\n",
    "    target_path: Path\n",
    ") -> Tuple[bool, Optional[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Acrescenta uma linha ao manifesto (cria arquivo se não existir).\n",
    "    Retorna (ok, csv_line_printed, errors)\n",
    "    \"\"\"\n",
    "    errors: List[str] = []\n",
    "    try:\n",
    "        manifesto_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        rows_total = int(len(df_written))\n",
    "        date_min = str(pd.to_datetime(df_written[\"date\"]).min())\n",
    "        date_max = str(pd.to_datetime(df_written[\"date\"]).max())\n",
    "        columns_json = json.dumps(EXPECTED_COLUMNS, ensure_ascii=False)\n",
    "        years = sorted(pd.to_datetime(df_written[\"date\"]).dt.year.unique().astype(int).tolist())\n",
    "        partitions = [f\"year={y}\" for y in years]\n",
    "        partitions_json = json.dumps(partitions, ensure_ascii=False)\n",
    "        header = [\"timestamp\", \"ticker\", \"rows_total\", \"date_min\", \"date_max\", \"columns_json\", \"partitions_json\", \"target_path\"]\n",
    "        row = [\n",
    "            AGORA_TZ.isoformat(),\n",
    "            ticker,\n",
    "            rows_total,\n",
    "            date_min,\n",
    "            date_max,\n",
    "            columns_json,\n",
    "            partitions_json,\n",
    "            str(target_path),\n",
    "        ]\n",
    "        # Escrever (append se existir; senão criar com header)\n",
    "        csv_line = \",\".join([str(x) for x in row])\n",
    "        if not manifesto_path.exists():\n",
    "            with open(manifesto_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\",\".join(header) + \"\\n\")\n",
    "                f.write(csv_line + \"\\n\")\n",
    "        else:\n",
    "            with open(manifesto_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(csv_line + \"\\n\")\n",
    "        return True, csv_line, errors\n",
    "    except Exception as e:\n",
    "        errors.append(f\"VALIDATION_ERROR: MANIFESTO_WRITE_ERROR — {e}\")\n",
    "        return False, None, errors\n",
    "\n",
    "# =========================\n",
    "# Execução Principal\n",
    "# =========================\n",
    "def main():\n",
    "    consecutive_errors = 0\n",
    "\n",
    "    # 1) Obter bronze_ibov (memória ou reingestão silenciosa)\n",
    "    bronze_df, ensure_msgs = ensure_bronze_in_memory()\n",
    "    if ensure_msgs:\n",
    "        for m in ensure_msgs:\n",
    "            print(m)\n",
    "    if bronze_df is None or bronze_df.empty:\n",
    "        consecutive_errors += 1\n",
    "        if consecutive_errors >= 2:\n",
    "            print_section(\"DÚVIDAS OBJETIVAS\")\n",
    "            print(\"- Não foi possível obter bronze_ibov em memória e reingestão falhou. Rede está disponível? Provedores autorizados?\")\n",
    "            print(\"- Deseja fornecer um caminho alternativo para leitura do Bronze antes da escrita?\")\n",
    "        print_section(\"CHECKLIST\")\n",
    "        checklist = {\n",
    "            \"preflight_quality_ok\": \"falha\",\n",
    "            \"parquet_write_summary\": \"falha\",\n",
    "            \"post_write_verification\": \"falha\",\n",
    "            \"manifesto_append_ok\": \"falha\"\n",
    "        }\n",
    "        print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "        for k, v in checklist.items():\n",
    "            if v != \"ok\":\n",
    "                print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "        return\n",
    "\n",
    "    # 2) Pré-voo de qualidade\n",
    "    ok_quality, details, q_errs = preflight_checks(bronze_df)\n",
    "    print_section(\"PREFLIGHT QUALITY\")\n",
    "    print(json.dumps({\"ok\": ok_quality, \"details\": details, \"errors\": q_errs}, ensure_ascii=False, indent=2))\n",
    "    if not ok_quality:\n",
    "        for e in q_errs:\n",
    "            print(e)\n",
    "        print(\"VALIDATION_ERROR: Pré-condições não atendidas; escrita abortada.\")\n",
    "        print_section(\"CHECKLIST\")\n",
    "        checklist = {\n",
    "            \"preflight_quality_ok\": \"falha\",\n",
    "            \"parquet_write_summary\": \"falha\",\n",
    "            \"post_write_verification\": \"falha\",\n",
    "            \"manifesto_append_ok\": \"falha\"\n",
    "        }\n",
    "        print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "        for k, v in checklist.items():\n",
    "            if v != \"ok\":\n",
    "                print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "        return\n",
    "\n",
    "    # 3) Escrita Parquet particionado (overwrite-by-partition)\n",
    "    print_section(\"PARQUET WRITE\")\n",
    "    write_ok, write_summary, write_errs = write_parquet_partitioned(bronze_df, PARQUET_TARGET, partition_col=\"year\")\n",
    "    if write_errs:\n",
    "        for e in write_errs:\n",
    "            print(e)\n",
    "    print(json.dumps({\"ok\": write_ok, \"years_written\": write_summary.get(\"years_written\", []), \"files_per_partition\": write_summary.get(\"files_per_partition\", {})}, ensure_ascii=False, indent=2))\n",
    "    if not write_ok:\n",
    "        consecutive_errors += 1\n",
    "    else:\n",
    "        consecutive_errors = 0\n",
    "\n",
    "    if consecutive_errors >= 2:\n",
    "        print(\"VALIDATION_ERROR: Falhas repetidas na escrita do Parquet.\")\n",
    "        print_section(\"DÚVIDAS OBJETIVAS\")\n",
    "        print(\"- Podemos instalar/atualizar pyarrow para habilitar escrita particionada com snappy?\")\n",
    "        print(\"- Há permissões de escrita no diretório alvo?\")\n",
    "        print_section(\"CHECKLIST\")\n",
    "        checklist = {\n",
    "            \"preflight_quality_ok\": \"ok\",\n",
    "            \"parquet_write_summary\": \"falha\",\n",
    "            \"post_write_verification\": \"falha\",\n",
    "            \"manifesto_append_ok\": \"falha\"\n",
    "        }\n",
    "        print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "        for k, v in checklist.items():\n",
    "            if v != \"ok\":\n",
    "                print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "        return\n",
    "\n",
    "    # 4) Pós-escrita: reabrir e verificar\n",
    "    print_section(\"POST-WRITE VERIFICATION\")\n",
    "    df_reopen, reopen_info, reopen_errs = reopen_dataset_summary(PARQUET_TARGET)\n",
    "    if reopen_errs:\n",
    "        for e in reopen_errs:\n",
    "            print(e)\n",
    "    print(json.dumps(reopen_info, ensure_ascii=False, indent=2))\n",
    "    if df_reopen is None or df_reopen.empty:\n",
    "        consecutive_errors += 1\n",
    "    else:\n",
    "        consecutive_errors = 0\n",
    "\n",
    "    if consecutive_errors >= 2:\n",
    "        print(\"VALIDATION_ERROR: Reabertura pós-escrita falhou repetidamente.\")\n",
    "        print_section(\"DÚVIDAS OBJETIVAS\")\n",
    "        print(\"- Podemos confirmar a instalação do engine Parquet (pyarrow) para leitura?\")\n",
    "        print(\"- O dataset contém arquivos corrompidos?\")\n",
    "        print_section(\"CHECKLIST\")\n",
    "        checklist = {\n",
    "            \"preflight_quality_ok\": \"ok\",\n",
    "            \"parquet_write_summary\": \"ok\",\n",
    "            \"post_write_verification\": \"falha\",\n",
    "            \"manifesto_append_ok\": \"falha\"\n",
    "        }\n",
    "        print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "        for k, v in checklist.items():\n",
    "            if v != \"ok\":\n",
    "                print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "        return\n",
    "\n",
    "    # 5) Manifesto: criar se faltar e adicionar linha\n",
    "    print_section(\"MANIFESTO APPEND\")\n",
    "    man_ok, man_line, man_errs = append_manifesto_row(MANIFESTO_PATH, TICKER, bronze_df, PARQUET_TARGET)\n",
    "    if man_errs:\n",
    "        for e in man_errs:\n",
    "            print(e)\n",
    "    if man_ok and man_line:\n",
    "        print(man_line)\n",
    "    else:\n",
    "        print(\"VALIDATION_ERROR: manifesto não atualizado.\")\n",
    "\n",
    "    # 6) Checklist final\n",
    "    print_section(\"CHECKLIST\")\n",
    "    checklist = {\n",
    "        \"preflight_quality_ok\": \"ok\" if ok_quality else \"falha\",\n",
    "        \"parquet_write_summary\": \"ok\" if write_ok else \"falha\",\n",
    "        \"post_write_verification\": \"ok\" if (df_reopen is not None and reopen_info.get(\"rows_total\", 0) > 0) else \"falha\",\n",
    "        \"manifesto_append_ok\": \"ok\" if man_ok else \"falha\"\n",
    "    }\n",
    "    print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "    for k, v in checklist.items():\n",
    "        if v != \"ok\":\n",
    "            print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Contrato:\n",
    "    # - Obtém bronze_ibov (memória ou reingesta silenciosa), valida pré-voo,\n",
    "    # - Escreve Parquet particionado (snappy, overwrite-by-partition),\n",
    "    # - Reabre para verificar, e registra manifesto (append/gera).\n",
    "    main()\n",
    "\n",
    "# =========================\n",
    "# Reingestão silenciosa (apenas se bronze_ibov não existir)\n",
    "# =========================\n",
    "def fetch_yahoo_chart_silent(ticker: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        base_url = \"https://query2.finance.yahoo.com/v8/finance/chart/%5EBVSP\"\n",
    "        params = {\n",
    "            \"period1\": str(to_unix_seconds(START_DATE_UTC)),\n",
    "            \"period2\": str(to_unix_seconds(NOW_UTC)),\n",
    "            \"interval\": \"1d\",\n",
    "            \"events\": \"history\",\n",
    "            \"includeAdjustedClose\": \"false\",\n",
    "        }\n",
    "        try:\n",
    "            import requests  # type: ignore\n",
    "            r = requests.get(base_url, params=params, headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) Python\"}, timeout=6)\n",
    "            if r.status_code < 200 or r.status_code >= 400:\n",
    "                return None\n",
    "            data = r.json()\n",
    "        except Exception:\n",
    "            from urllib.parse import urlencode\n",
    "            from urllib.request import Request, urlopen\n",
    "            url = base_url + \"?\" + urlencode(params)\n",
    "            req = Request(url, headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) Python\"})\n",
    "            with urlopen(req, timeout=8) as resp:\n",
    "                raw = resp.read()\n",
    "            import json as _json\n",
    "            data = _json.loads(raw.decode(\"utf-8\"))\n",
    "        if \"chart\" not in data or not data[\"chart\"].get(\"result\"):\n",
    "            return None\n",
    "        res0 = data[\"chart\"][\"result\"][0]\n",
    "        ts = res0.get(\"timestamp\", []) or []\n",
    "        q = (res0.get(\"indicators\", {}) or {}).get(\"quote\", []) or []\n",
    "        if not q:\n",
    "            return None\n",
    "        q0 = q[0]\n",
    "        opens = q0.get(\"open\", []) or []\n",
    "        highs = q0.get(\"high\", []) or []\n",
    "        lows = q0.get(\"low\", []) or []\n",
    "        closes = q0.get(\"close\", []) or []\n",
    "        vols = q0.get(\"volume\", []) or []\n",
    "        n = min(len(ts), len(opens), len(highs), len(lows), len(closes), len(vols))\n",
    "        if n == 0:\n",
    "            return None\n",
    "        df_pre = pd.DataFrame({\n",
    "            \"date\": pd.to_datetime(ts[:n], unit=\"s\", utc=True),\n",
    "            \"open\": opens[:n],\n",
    "            \"high\": highs[:n],\n",
    "            \"low\": lows[:n],\n",
    "            \"close\": closes[:n],\n",
    "            \"volume\": vols[:n],\n",
    "        })\n",
    "        return bronze_normalize(df_pre, ticker)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fetch_yfinance_silent(ticker: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        try:\n",
    "            import yfinance as yf  # type: ignore\n",
    "        except Exception:\n",
    "            return None\n",
    "        start_str = START_DATE_UTC.tz_localize(None).date().isoformat() if START_DATE_UTC.tzinfo else START_DATE_UTC.date().isoformat()\n",
    "        end_inc = (END_DATE_UTC + pd.Timedelta(days=1))\n",
    "        end_str = end_inc.tz_localize(None).date().isoformat() if end_inc.tzinfo else end_inc.date().isoformat()\n",
    "        df_raw = yf.download(tickers=ticker, start=start_str, end=end_str, interval=\"1d\", auto_adjust=False, progress=False, threads=True)\n",
    "        if df_raw is None or df_raw.empty:\n",
    "            return None\n",
    "        if isinstance(df_raw.columns, pd.MultiIndex):\n",
    "            try:\n",
    "                df_raw.columns = [c[-1] if isinstance(c, tuple) else c for c in df_raw.columns.to_list()]\n",
    "            except Exception:\n",
    "                df_raw.columns = df_raw.columns.get_level_values(-1)\n",
    "        rename_map = {\"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\", \"Volume\": \"volume\",\n",
    "                      \"open\": \"open\", \"high\": \"high\", \"low\": \"low\", \"close\": \"close\", \"volume\": \"volume\"}\n",
    "        df_raw = df_raw.rename(columns=rename_map)\n",
    "        need = {\"open\", \"high\", \"low\", \"close\", \"volume\"}\n",
    "        if not need.issubset(set(df_raw.columns)):\n",
    "            return None\n",
    "        df_pre = df_raw.reset_index().rename(columns={\"Date\": \"date\", \"Datetime\": \"date\"})\n",
    "        if \"date\" not in df_pre.columns:\n",
    "            df_pre = df_raw.copy()\n",
    "            df_pre[\"date\"] = df_pre.index\n",
    "            df_pre = df_pre.reset_index(drop=True)\n",
    "        df_pre = df_pre[[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "        return bronze_normalize(df_pre, ticker)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fetch_stooq_silent(ticker: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        try:\n",
    "            from pandas_datareader import data as dr  # type: ignore\n",
    "        except Exception:\n",
    "            return None\n",
    "        candidates = [ticker, ticker.replace(\"^\", \"\"), ticker.replace(\"^\", \"\").lower()]\n",
    "        df_raw = None\n",
    "        last_exc = None\n",
    "        for tk in candidates:\n",
    "            try:\n",
    "                df_raw = dr.DataReader(tk, \"stooq\", start=START_DATE_UTC.tz_localize(None), end=END_DATE_UTC.tz_localize(None))\n",
    "                if df_raw is not None and not df_raw.empty:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                last_exc = e\n",
    "                continue\n",
    "        if df_raw is None or df_raw.empty:\n",
    "            return None\n",
    "        df_raw = df_raw.sort_index()\n",
    "        if isinstance(df_raw.columns, pd.MultiIndex):\n",
    "            try:\n",
    "                df_raw.columns = [c[-1] if isinstance(c, tuple) else c for c in df_raw.columns.to_list()]\n",
    "            except Exception:\n",
    "                df_raw.columns = df_raw.columns.get_level_values(-1)\n",
    "        rename_map = {\"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\", \"Volume\": \"volume\",\n",
    "                      \"open\": \"open\", \"high\": \"high\", \"low\": \"low\", \"close\": \"close\", \"volume\": \"volume\"}\n",
    "        df_raw = df_raw.rename(columns=rename_map)\n",
    "        need = {\"open\", \"high\", \"low\", \"close\", \"volume\"}\n",
    "        if not need.issubset(set(df_raw.columns)):\n",
    "            return None\n",
    "        df_pre = df_raw.copy()\n",
    "        df_pre[\"date\"] = df_pre.index\n",
    "        df_pre = df_pre.reset_index(drop=True)[[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "        return bronze_normalize(df_pre, ticker)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def ensure_bronze_in_memory() -> Tuple[Optional[pd.DataFrame], List[str]]:\n",
    "    msgs: List[str] = []\n",
    "    g = globals()\n",
    "    if \"bronze_ibov\" in g and isinstance(g[\"bronze_ibov\"], pd.DataFrame):\n",
    "        df = g[\"bronze_ibov\"].copy()\n",
    "        # Reforçar schema/dtypes\n",
    "        try:\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.normalize()\n",
    "            for c in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "                df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float64\")\n",
    "            df[\"volume\"] = pd.to_numeric(df[\"volume\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "            df[\"ticker\"] = df[\"ticker\"].astype(\"string\")\n",
    "            df = df[EXPECTED_COLUMNS].sort_values(\"date\").drop_duplicates(subset=[\"date\"], keep=\"last\").reset_index(drop=True)\n",
    "            return df, msgs\n",
    "        except Exception as e:\n",
    "            msgs.append(f\"INFO: bronze_ibov em memória com inconsistências — {e}; reingestão silenciosa será tentada.\")\n",
    "    # Reingestão silenciosa\n",
    "    for fn in (fetch_yahoo_chart_silent, fetch_yfinance_silent, fetch_stooq_silent):\n",
    "        df = fn(TICKER)\n",
    "        if df is not None and not df.empty:\n",
    "            return df, msgs\n",
    "    msgs.append(\"VALIDATION_ERROR: PROVIDERS_EXHAUSTED — não foi possível reingestar bronze_ibov.\")\n",
    "    return None, msgs\n",
    "\n",
    "# =========================\n",
    "# Pré-voo de qualidade\n",
    "# =========================\n",
    "def preflight_checks(df: pd.DataFrame) -> Tuple[bool, Dict[str, Any], List[str]]:\n",
    "    errs: List[str] = []\n",
    "    details: Dict[str, Any] = {}\n",
    "    if df is None or df.empty:\n",
    "        errs.append(\"VALIDATION_ERROR: DataFrame vazio.\")\n",
    "    else:\n",
    "        pn = percent_nulls(df)\n",
    "        details[\"percent_nulls\"] = {k: round(v, 6) for k, v in pn.items()}\n",
    "        for col in [\"date\", \"close\", \"ticker\"]:\n",
    "            if round(pn.get(col, 100.0), 6) != 0.0:\n",
    "                errs.append(f\"VALIDATION_ERROR: % nulos em {col} deve ser 0%, obtido={pn.get(col, 100.0):.6f}%\")\n",
    "        dups = int(df.duplicated(subset=[\"date\"]).sum())\n",
    "        details[\"duplicates_by_date\"] = dups\n",
    "        if dups != 0:\n",
    "            errs.append(f\"VALIDATION_ERROR: duplicatas por date detectadas (= {dups})\")\n",
    "        if len(df) < 2500:\n",
    "            errs.append(f\"VALIDATION_ERROR: cobertura insuficiente — linhas={len(df)} (< 2500)\")\n",
    "        dmin = pd.to_datetime(df[\"date\"]).min()\n",
    "        dmax = pd.to_datetime(df[\"date\"]).max()\n",
    "        details[\"date_min\"] = str(dmin)\n",
    "        details[\"date_max\"] = str(dmax)\n",
    "        # Tolerâncias: início ≤ 2012-01-06; fim ≥ hoje(UTC) − 3d\n",
    "        if dmin > pd.Timestamp(\"2012-01-06\"):\n",
    "            errs.append(f\"VALIDATION_ERROR: date.min ({dmin.date().isoformat()}) > tolerância (2012-01-06)\")\n",
    "        required_end_min = (pd.Timestamp(datetime.now(timezone.utc)).normalize() - pd.Timedelta(days=3)).tz_localize(None)\n",
    "        if dmax < required_end_min:\n",
    "            errs.append(f\"VALIDATION_ERROR: date.max ({dmax.date().isoformat()}) < requerido mínimo ({required_end_min.date().isoformat()}) (tolerância 3 dias)\")\n",
    "    return (len(errs) == 0), details, errs\n",
    "\n",
    "# =========================\n",
    "# Escrita Parquet particionado (overwrite-by-partition)\n",
    "# =========================\n",
    "def write_parquet_partitioned(df: pd.DataFrame, base_dir: Path, partition_col: str = \"year\") -> Tuple[bool, Dict[str, Any], List[str]]:\n",
    "    \"\"\"\n",
    "    Escreve com pyarrow.parquet.write_to_dataset, compressão snappy,\n",
    "    particionado por 'year', com existing_data_behavior='delete_matching' (overwrite-by-partition).\n",
    "    Retorna (ok, summary, errors).\n",
    "    \"\"\"\n",
    "    errors: List[str] = []\n",
    "    summary: Dict[str, Any] = {\"years_written\": [], \"files_per_partition\": {}}\n",
    "    try:\n",
    "        import pyarrow as pa  # type: ignore\n",
    "        import pyarrow.parquet as pq  # type: ignore\n",
    "    except Exception as e:\n",
    "        errors.append(f\"VALIDATION_ERROR: MISSING_DEPENDENCY_PYARROW — {e}\")\n",
    "        return False, summary, errors\n",
    "\n",
    "    try:\n",
    "        base_dir.mkdir(parents=True, exist_ok=True)\n",
    "        df2 = df.copy()\n",
    "        years = pd.to_datetime(df2[\"date\"]).dt.year.astype(\"int16\")\n",
    "        df2[partition_col] = years\n",
    "        table = pa.Table.from_pandas(df2, preserve_index=False)\n",
    "        # Escreve dataset\n",
    "        pq.write_to_dataset(\n",
    "            table=table,\n",
    "            root_path=str(base_dir),\n",
    "            partition_cols=[partition_col],\n",
    "            compression=\"snappy\",\n",
    "            existing_data_behavior=\"delete_matching\"  # overwrite-by-partition\n",
    "        )\n",
    "        # Sumário por partição escrita\n",
    "        written_years = sorted(pd.unique(years).astype(int).tolist())\n",
    "        summary[\"years_written\"] = written_years\n",
    "        files_per = {}\n",
    "        for y in written_years:\n",
    "            p = base_dir / f\"{partition_col}={y}\"\n",
    "            cnt = 0\n",
    "            if p.exists() and p.is_dir():\n",
    "                for _, _, files in os.walk(p):\n",
    "                    cnt += sum(1 for f in files if f.endswith(\".parquet\"))\n",
    "            files_per[str(y)] = cnt\n",
    "        summary[\"files_per_partition\"] = files_per\n",
    "        return True, summary, errors\n",
    "    except Exception as e:\n",
    "        errors.append(f\"VALIDATION_ERROR: PARQUET_WRITE_ERROR — {e}\")\n",
    "        return False, summary, errors\n",
    "\n",
    "# =========================\n",
    "# Reabertura pós-escrita\n",
    "# =========================\n",
    "def reopen_dataset_summary(base_dir: Path) -> Tuple[Optional[pd.DataFrame], Dict[str, Any], List[str]]:\n",
    "    errors: List[str] = []\n",
    "    info: Dict[str, Any] = {\"rows_total\": 0, \"min_date\": None, \"max_date\": None}\n",
    "    try:\n",
    "        import pyarrow.dataset as ds  # type: ignore\n",
    "        dataset = ds.dataset(str(base_dir), format=\"parquet\")\n",
    "        table = dataset.to_table()\n",
    "        df = table.to_pandas()\n",
    "    except Exception as e1:\n",
    "        errors.append(f\"READ_ERROR_PA_DS: {e1}\")\n",
    "        try:\n",
    "            df = pd.read_parquet(str(base_dir))\n",
    "        except Exception as e2:\n",
    "            errors.append(f\"READ_ERROR_PD_RP: {e2}\")\n",
    "            return None, info, errors\n",
    "    # Normaliza e resume\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.tz_localize(None).dt.normalize()\n",
    "    info[\"rows_total\"] = int(len(df))\n",
    "    info[\"min_date\"] = str(pd.to_datetime(df[\"date\"]).min()) if \"date\" in df.columns and not df[\"date\"].isna().all() else None\n",
    "    info[\"max_date\"] = str(pd.to_datetime(df[\"date\"]).max()) if \"date\" in df.columns and not df[\"date\"].isna().all() else None\n",
    "    return df, info, errors\n",
    "\n",
    "# =========================\n",
    "# Manifesto (append ou create)\n",
    "# =========================\n",
    "def append_manifesto_row(\n",
    "    manifesto_path: Path,\n",
    "    ticker: str,\n",
    "    df_written: pd.DataFrame,\n",
    "    target_path: Path\n",
    ") -> Tuple[bool, Optional[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Acrescenta uma linha ao manifesto (cria arquivo se não existir).\n",
    "    Retorna (ok, csv_line_printed, errors)\n",
    "    \"\"\"\n",
    "    errors: List[str] = []\n",
    "    try:\n",
    "        manifesto_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        rows_total = int(len(df_written))\n",
    "        date_min = str(pd.to_datetime(df_written[\"date\"]).min())\n",
    "        date_max = str(pd.to_datetime(df_written[\"date\"]).max())\n",
    "        columns_json = json.dumps(EXPECTED_COLUMNS, ensure_ascii=False)\n",
    "        years = sorted(pd.to_datetime(df_written[\"date\"]).dt.year.unique().astype(int).tolist())\n",
    "        partitions = [f\"year={y}\" for y in years]\n",
    "        partitions_json = json.dumps(partitions, ensure_ascii=False)\n",
    "        header = [\"timestamp\", \"ticker\", \"rows_total\", \"date_min\", \"date_max\", \"columns_json\", \"partitions_json\", \"target_path\"]\n",
    "        row = [\n",
    "            AGORA_TZ.isoformat(),\n",
    "            ticker,\n",
    "            rows_total,\n",
    "            date_min,\n",
    "            date_max,\n",
    "            columns_json,\n",
    "            partitions_json,\n",
    "            str(target_path),\n",
    "        ]\n",
    "        # Escrever (append se existir; senão criar com header)\n",
    "        csv_line = \",\".join([str(x) for x in row])\n",
    "        if not manifesto_path.exists():\n",
    "            with open(manifesto_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\",\".join(header) + \"\\n\")\n",
    "                f.write(csv_line + \"\\n\")\n",
    "        else:\n",
    "            with open(manifesto_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(csv_line + \"\\n\")\n",
    "        return True, csv_line, errors\n",
    "    except Exception as e:\n",
    "        errors.append(f\"VALIDATION_ERROR: MANIFESTO_WRITE_ERROR — {e}\")\n",
    "        return False, None, errors\n",
    "\n",
    "# =========================\n",
    "# Execução Principal\n",
    "# =========================\n",
    "def main():\n",
    "    consecutive_errors = 0\n",
    "\n",
    "    # 1) Obter bronze_ibov (memória ou reingestão silenciosa)\n",
    "    bronze_df, ensure_msgs = ensure_bronze_in_memory()\n",
    "    if ensure_msgs:\n",
    "        for m in ensure_msgs:\n",
    "            print(m)\n",
    "    if bronze_df is None or bronze_df.empty:\n",
    "        consecutive_errors += 1\n",
    "        if consecutive_errors >= 2:\n",
    "            print_section(\"DÚVIDAS OBJETIVAS\")\n",
    "            print(\"- Não foi possível obter bronze_ibov em memória e reingestão falhou. Rede está disponível? Provedores autorizados?\")\n",
    "            print(\"- Deseja fornecer um caminho alternativo para leitura do Bronze antes da escrita?\")\n",
    "        print_section(\"CHECKLIST\")\n",
    "        checklist = {\n",
    "            \"preflight_quality_ok\": \"falha\",\n",
    "            \"parquet_write_summary\": \"falha\",\n",
    "            \"post_write_verification\": \"falha\",\n",
    "            \"manifesto_append_ok\": \"falha\"\n",
    "        }\n",
    "        print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "        for k, v in checklist.items():\n",
    "            if v != \"ok\":\n",
    "                print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "        return\n",
    "\n",
    "    # 2) Pré-voo de qualidade\n",
    "    ok_quality, details, q_errs = preflight_checks(bronze_df)\n",
    "    print_section(\"PREFLIGHT QUALITY\")\n",
    "    print(json.dumps({\"ok\": ok_quality, \"details\": details, \"errors\": q_errs}, ensure_ascii=False, indent=2))\n",
    "    if not ok_quality:\n",
    "        for e in q_errs:\n",
    "            print(e)\n",
    "        print(\"VALIDATION_ERROR: Pré-condições não atendidas; escrita abortada.\")\n",
    "        print_section(\"CHECKLIST\")\n",
    "        checklist = {\n",
    "            \"preflight_quality_ok\": \"falha\",\n",
    "            \"parquet_write_summary\": \"falha\",\n",
    "            \"post_write_verification\": \"falha\",\n",
    "            \"manifesto_append_ok\": \"falha\"\n",
    "        }\n",
    "        print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "        for k, v in checklist.items():\n",
    "            if v != \"ok\":\n",
    "                print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "        return\n",
    "\n",
    "    # 3) Escrita Parquet particionado (overwrite-by-partition)\n",
    "    print_section(\"PARQUET WRITE\")\n",
    "    write_ok, write_summary, write_errs = write_parquet_partitioned(bronze_df, PARQUET_TARGET, partition_col=\"year\")\n",
    "    if write_errs:\n",
    "        for e in write_errs:\n",
    "            print(e)\n",
    "    print(json.dumps({\"ok\": write_ok, \"years_written\": write_summary.get(\"years_written\", []), \"files_per_partition\": write_summary.get(\"files_per_partition\", {})}, ensure_ascii=False, indent=2))\n",
    "    if not write_ok:\n",
    "        consecutive_errors += 1\n",
    "    else:\n",
    "        consecutive_errors = 0\n",
    "\n",
    "    if consecutive_errors >= 2:\n",
    "        print(\"VALIDATION_ERROR: Falhas repetidas na escrita do Parquet.\")\n",
    "        print_section(\"DÚVIDAS OBJETIVAS\")\n",
    "        print(\"- Podemos instalar/atualizar pyarrow para habilitar escrita particionada com snappy?\")\n",
    "        print(\"- Há permissões de escrita no diretório alvo?\")\n",
    "        print_section(\"CHECKLIST\")\n",
    "        checklist = {\n",
    "            \"preflight_quality_ok\": \"ok\",\n",
    "            \"parquet_write_summary\": \"falha\",\n",
    "            \"post_write_verification\": \"falha\",\n",
    "            \"manifesto_append_ok\": \"falha\"\n",
    "        }\n",
    "        print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "        for k, v in checklist.items():\n",
    "            if v != \"ok\":\n",
    "                print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "        return\n",
    "\n",
    "    # 4) Pós-escrita: reabrir e verificar\n",
    "    print_section(\"POST-WRITE VERIFICATION\")\n",
    "    df_reopen, reopen_info, reopen_errs = reopen_dataset_summary(PARQUET_TARGET)\n",
    "    if reopen_errs:\n",
    "        for e in reopen_errs:\n",
    "            print(e)\n",
    "    print(json.dumps(reopen_info, ensure_ascii=False, indent=2))\n",
    "    if df_reopen is None or df_reopen.empty:\n",
    "        consecutive_errors += 1\n",
    "    else:\n",
    "        consecutive_errors = 0\n",
    "\n",
    "    if consecutive_errors >= 2:\n",
    "        print(\"VALIDATION_ERROR: Reabertura pós-escrita falhou repetidamente.\")\n",
    "        print_section(\"DÚVIDAS OBJETIVAS\")\n",
    "        print(\"- Podemos confirmar a instalação do engine Parquet (pyarrow) para leitura?\")\n",
    "        print(\"- O dataset contém arquivos corrompidos?\")\n",
    "        print_section(\"CHECKLIST\")\n",
    "        checklist = {\n",
    "            \"preflight_quality_ok\": \"ok\",\n",
    "            \"parquet_write_summary\": \"ok\",\n",
    "            \"post_write_verification\": \"falha\",\n",
    "            \"manifesto_append_ok\": \"falha\"\n",
    "        }\n",
    "        print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "        for k, v in checklist.items():\n",
    "            if v != \"ok\":\n",
    "                print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "        return\n",
    "\n",
    "    # 5) Manifesto: criar se faltar e adicionar linha\n",
    "    print_section(\"MANIFESTO APPEND\")\n",
    "    man_ok, man_line, man_errs = append_manifesto_row(MANIFESTO_PATH, TICKER, bronze_df, PARQUET_TARGET)\n",
    "    if man_errs:\n",
    "        for e in man_errs:\n",
    "            print(e)\n",
    "    if man_ok and man_line:\n",
    "        print(man_line)\n",
    "    else:\n",
    "        print(\"VALIDATION_ERROR: manifesto não atualizado.\")\n",
    "\n",
    "    # 6) Checklist final\n",
    "    print_section(\"CHECKLIST\")\n",
    "    checklist = {\n",
    "        \"preflight_quality_ok\": \"ok\" if ok_quality else \"falha\",\n",
    "        \"parquet_write_summary\": \"ok\" if write_ok else \"falha\",\n",
    "        \"post_write_verification\": \"ok\" if (df_reopen is not None and reopen_info.get(\"rows_total\", 0) > 0) else \"falha\",\n",
    "        \"manifesto_append_ok\": \"ok\" if man_ok else \"falha\"\n",
    "    }\n",
    "    print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "    for k, v in checklist.items():\n",
    "        if v != \"ok\":\n",
    "            print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Contrato:\n",
    "    # - Obtém bronze_ibov (memória ou reingesta silenciosa), valida pré-voo,\n",
    "    # - Escreve Parquet particionado (snappy, overwrite-by-partition),\n",
    "    # - Reabre para verificar, e registra manifesto (append/gera).\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38006c3",
   "metadata": {},
   "source": [
    "# Instrução 1B–MANIFESTO–FIX (recriar/append da linha ^BVSP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e16ff2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_found: {\"path\": \"/home/wrm/BOLSA_2026/bronze/IBOV.parquet\", \"exists\": true, \"is_dir\": true, \"has_year_subdirs\": true, \"rows_total\": 3400, \"date_min\": \"2012-01-03\", \"date_max\": \"2025-09-19\"}\n",
      "manifesto_status: {\"path\": \"/home/wrm/BOLSA_2026/manifestos/bronze_ibov_manifesto.csv\", \"exists_before\": true, \"had_row_for_^BVSP_before\": false}\n",
      "write_action: appended_row\n",
      "manifesto_tail: 2025-09-19T13:25:01.095765+00:00,^BVSP,3400,2012-01-03,2025-09-19,\"[\"\"date\"\", \"\"open\"\", \"\"high\"\", \"\"low\"\", \"\"close\"\", \"\"volume\"\", \"\"ticker\"\"]\",\"[\"\"year=2012\"\", \"\"year=2013\"\", \"\"year=2014\"\", \"\"year=2015\"\", \"\"year=2016\"\", \"\"year=2017\"\", \"\"year=2018\"\", \"\"year=2019\"\", \"\"year=2020\"\", \"\"year=2021\"\", \"\"year=2022\"\", \"\"year=2023\"\", \"\"year=2024\"\", \"\"year=2025\"\"]\",/home/wrm/BOLSA_2026/bronze/IBOV.parquet\n"
     ]
    }
   ],
   "source": [
    "# Instrução 1B–MANIFESTO–FIX (recriar/append da linha ^BVSP)\n",
    "# Objetivo: Garantir que /home/wrm/BOLSA_2026/manifestos/bronze_ibov_manifesto.csv contenha a linha mais recente do ^BVSP,\n",
    "# consistente com o dataset em /home/wrm/BOLSA_2026/bronze/IBOV.parquet.\n",
    "# Disciplina: Um único bloco de código auto-contido. dry_run=False (vai escrever/append no CSV).\n",
    "# Relatórios: imprimir dataset_found, manifesto_status, write_action, manifesto_tail. Mensagens normativas em caso de falha.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import io\n",
    "import json\n",
    "import csv\n",
    "import traceback\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Dependências opcionais\n",
    "try:\n",
    "    import pyarrow.dataset as ds\n",
    "    import pyarrow as pa  # noqa: F401\n",
    "except Exception:\n",
    "    ds = None\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    pd = None\n",
    "\n",
    "# Parâmetros fixos\n",
    "DATASET_PATH = \"/home/wrm/BOLSA_2026/bronze/IBOV.parquet\"\n",
    "MANIFESTO_PATH = \"/home/wrm/BOLSA_2026/manifestos/bronze_ibov_manifesto.csv\"\n",
    "TICKER = \"^BVSP\"\n",
    "COLUMNS_JSON = json.dumps([\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"ticker\"], ensure_ascii=False)\n",
    "TARGET_PATH = DATASET_PATH\n",
    "\n",
    "def print_normative_error(msg: str):\n",
    "    print(msg)\n",
    "    sys.exit(1)\n",
    "\n",
    "def safe_iso_date(ts) -> str:\n",
    "    if ts is None:\n",
    "        return \"\"\n",
    "    if isinstance(ts, str):\n",
    "        return ts\n",
    "    try:\n",
    "        # pandas Timestamp or datetime\n",
    "        if hasattr(ts, \"to_pydatetime\"):\n",
    "            ts = ts.to_pydatetime()\n",
    "        if isinstance(ts, datetime):\n",
    "            return ts.date().isoformat()\n",
    "        # Fallback: str\n",
    "        return str(ts)\n",
    "    except Exception:\n",
    "        return str(ts)\n",
    "\n",
    "def read_dataset_summary(dataset_path: str):\n",
    "    exists = os.path.exists(dataset_path)\n",
    "    is_dir = os.path.isdir(dataset_path)\n",
    "    partitions = []\n",
    "    has_year_subdirs = False\n",
    "    rows_total = None\n",
    "    date_min = None\n",
    "    date_max = None\n",
    "\n",
    "    if is_dir:\n",
    "        try:\n",
    "            for name in os.listdir(dataset_path):\n",
    "                full = os.path.join(dataset_path, name)\n",
    "                if os.path.isdir(full) and re.fullmatch(r\"year=\\d{4}\", name):\n",
    "                    partitions.append(name)\n",
    "            partitions = sorted(partitions)\n",
    "            has_year_subdirs = len(partitions) > 0\n",
    "        except Exception:\n",
    "            # keep defaults; will be validated later\n",
    "            pass\n",
    "\n",
    "    # Tentar reabrir o dataset e computar contagens e extremos de data\n",
    "    if exists and is_dir and has_year_subdirs:\n",
    "        # Preferir pyarrow.dataset\n",
    "        if ds is not None:\n",
    "            try:\n",
    "                dset = ds.dataset(dataset_path, format=\"parquet\", partitioning=\"hive\")\n",
    "                # rows_total\n",
    "                try:\n",
    "                    rows_total = dset.count_rows()\n",
    "                except Exception:\n",
    "                    # Fallback: contar linhas via to_table apenas da coluna date\n",
    "                    tbl = dset.to_table(columns=[\"date\"])\n",
    "                    rows_total = tbl.num_rows\n",
    "                # Extremos de data\n",
    "                tbl_date = dset.to_table(columns=[\"date\"])\n",
    "                if pd is None:\n",
    "                    # Converter via pyarrow para Python nativo e calcular min/max\n",
    "                    col = tbl_date.column(\"date\")\n",
    "                    # to_pylist pode ser grande; dataset diário é pequeno, ok\n",
    "                    pylist = col.to_pylist()\n",
    "                    # Filtrar None\n",
    "                    vals = [v for v in pylist if v is not None]\n",
    "                    if len(vals) == 0:\n",
    "                        raise ValueError(\"Coluna 'date' vazia após filtragem.\")\n",
    "                    # Valores podem ser datetime ou int (epoch); normalizar\n",
    "                    # pyarrow geralmente já entrega datetime\n",
    "                    dmin = min(vals)\n",
    "                    dmax = max(vals)\n",
    "                    date_min = dmin\n",
    "                    date_max = dmax\n",
    "                else:\n",
    "                    s = tbl_date.to_pandas(types_mapper=None)  # pandas Series se single column\n",
    "                    if isinstance(s, pd.DataFrame):\n",
    "                        # Garantir Series\n",
    "                        if \"date\" in s.columns:\n",
    "                            s = s[\"date\"]\n",
    "                        else:\n",
    "                            # pegar primeira coluna\n",
    "                            s = s.iloc[:, 0]\n",
    "                    s = pd.to_datetime(s, utc=True, errors=\"coerce\")\n",
    "                    s = s.dropna()\n",
    "                    if s.empty:\n",
    "                        raise ValueError(\"Coluna 'date' sem valores válidos.\")\n",
    "                    date_min = s.min()\n",
    "                    date_max = s.max()\n",
    "            except Exception as e:\n",
    "                # Fallback: pandas.read_parquet no diretório (requer pandas + engine disponível)\n",
    "                if pd is None:\n",
    "                    print_normative_error(f\"VALIDATION_ERROR: falha ao abrir dataset com pyarrow.dataset e pandas ausente. Detalhe: {e}\")\n",
    "                try:\n",
    "                    df = pd.read_parquet(dataset_path, columns=[\"date\"])\n",
    "                    if df.empty:\n",
    "                        raise ValueError(\"Dataset lido via pandas está vazio.\")\n",
    "                    rows_total = len(df)\n",
    "                    s = pd.to_datetime(df[\"date\"], utc=True, errors=\"coerce\").dropna()\n",
    "                    if s.empty:\n",
    "                        raise ValueError(\"Coluna 'date' sem valores válidos (pandas).\")\n",
    "                    date_min = s.min()\n",
    "                    date_max = s.max()\n",
    "                except Exception as e2:\n",
    "                    print_normative_error(f\"VALIDATION_ERROR: falha ao reabrir dataset com pandas.read_parquet. Detalhe: {e2}\")\n",
    "        else:\n",
    "            # Sem pyarrow.dataset: usar pandas diretamente\n",
    "            if pd is None:\n",
    "                print_normative_error(\"VALIDATION_ERROR: nem pyarrow.dataset nem pandas disponíveis para reabrir dataset.\")\n",
    "            try:\n",
    "                df = pd.read_parquet(dataset_path, columns=[\"date\"])\n",
    "                if df.empty:\n",
    "                    raise ValueError(\"Dataset lido via pandas está vazio.\")\n",
    "                rows_total = len(df)\n",
    "                s = pd.to_datetime(df[\"date\"], utc=True, errors=\"coerce\").dropna()\n",
    "                if s.empty:\n",
    "                    raise ValueError(\"Coluna 'date' sem valores válidos (pandas).\")\n",
    "                date_min = s.min()\n",
    "                date_max = s.max()\n",
    "            except Exception as e:\n",
    "                print_normative_error(f\"VALIDATION_ERROR: falha ao reabrir dataset com pandas.read_parquet. Detalhe: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"path\": dataset_path,\n",
    "        \"exists\": exists,\n",
    "        \"is_dir\": is_dir,\n",
    "        \"has_year_subdirs\": has_year_subdirs,\n",
    "        \"partitions\": partitions,\n",
    "        \"rows_total\": int(rows_total) if rows_total is not None else None,\n",
    "        \"date_min\": safe_iso_date(date_min),\n",
    "        \"date_max\": safe_iso_date(date_max),\n",
    "    }\n",
    "\n",
    "def ensure_manifesto_dir(manifesto_path: str):\n",
    "    d = os.path.dirname(manifesto_path)\n",
    "    if d:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def read_manifesto_status(manifesto_path: str):\n",
    "    exists_before = os.path.exists(manifesto_path)\n",
    "    had_row_for_vbsp_before = False\n",
    "    last_line = \"\"\n",
    "    last_row = None\n",
    "    if exists_before:\n",
    "        try:\n",
    "            # Ler de forma robusta com pandas se disponível\n",
    "            if pd is not None:\n",
    "                mdf = pd.read_csv(manifesto_path, dtype=str)\n",
    "                mdf = mdf.fillna(\"\")\n",
    "                if not mdf.empty:\n",
    "                    had_row_for_vbsp_before = any(mdf[\"ticker\"] == TICKER) if \"ticker\" in mdf.columns else False\n",
    "                    # última linha como CSV string\n",
    "                    last_row = mdf.iloc[-1].to_dict()\n",
    "                    output = io.StringIO()\n",
    "                    writer = csv.DictWriter(output, fieldnames=mdf.columns.tolist())\n",
    "                    writer.writeheader()\n",
    "                    writer.writerow(last_row)\n",
    "                    last_line = output.getvalue().strip().splitlines()[-1]\n",
    "                else:\n",
    "                    had_row_for_vbsp_before = False\n",
    "                    last_line = \"\"\n",
    "            else:\n",
    "                # Sem pandas: ler manualmente\n",
    "                with open(manifesto_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    lines = [ln.rstrip(\"\\n\") for ln in f.readlines()]\n",
    "                if len(lines) >= 2:\n",
    "                    header = lines[0]\n",
    "                    last_line = lines[-1]\n",
    "                    try:\n",
    "                        # testar presença de ticker em alguma linha\n",
    "                        had_row_for_vbsp_before = any(TICKER in ln.split(\",\")[1:2] for ln in lines[1:])\n",
    "                    except Exception:\n",
    "                        had_row_for_vbsp_before = (TICKER in \"\\n\".join(lines[1:]))\n",
    "                else:\n",
    "                    had_row_for_vbsp_before = False\n",
    "                    last_line = \"\"\n",
    "        except Exception:\n",
    "            # Se falhar leitura, considerar inexistente para fins de fluxo seguro\n",
    "            exists_before = os.path.exists(manifesto_path)\n",
    "            had_row_for_vbsp_before = False\n",
    "            last_line = \"\"\n",
    "    return {\n",
    "        \"path\": manifesto_path,\n",
    "        \"exists_before\": exists_before,\n",
    "        \"had_row_for_^BVSP_before\": had_row_for_vbsp_before,\n",
    "        \"last_line\": last_line,\n",
    "        \"last_row_dict\": last_row\n",
    "    }\n",
    "\n",
    "def append_or_create_manifesto(manifesto_path: str, row_dict: dict, manifesto_status: dict):\n",
    "    ensure_manifesto_dir(manifesto_path)\n",
    "    exists_before = manifesto_status[\"exists_before\"]\n",
    "    last_row_dict = manifesto_status.get(\"last_row_dict\")\n",
    "    last_line_prior = manifesto_status.get(\"last_line\", \"\")\n",
    "    # Decisão:\n",
    "    # - Se não existe, criar arquivo com header + linha => created_file\n",
    "    # - Se existe:\n",
    "    #     - Se última linha já é do ^BVSP e contém mesmos valores-chave (rows_total, date_min, date_max, target_path), então skip\n",
    "    #     - Caso contrário, append => appended_row\n",
    "    action = None\n",
    "    reason = None\n",
    "\n",
    "    if not exists_before:\n",
    "        # Criar novo\n",
    "        fieldnames = [\"timestamp\",\"ticker\",\"rows_total\",\"date_min\",\"date_max\",\"columns_json\",\"partitions_json\",\"target_path\"]\n",
    "        try:\n",
    "            with open(manifesto_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                writer.writerow(row_dict)\n",
    "            action = \"created_file\"\n",
    "        except Exception as e:\n",
    "            print_normative_error(f\"VALIDATION_ERROR: falha ao criar manifesto. Detalhe: {e}\")\n",
    "    else:\n",
    "        # Existe: decidir se precisa append\n",
    "        need_append = True\n",
    "        if last_row_dict is not None:\n",
    "            last_ticker = last_row_dict.get(\"ticker\", \"\")\n",
    "            same_rows = str(last_row_dict.get(\"rows_total\", \"\")) == str(row_dict.get(\"rows_total\", \"\"))\n",
    "            same_dmin = str(last_row_dict.get(\"date_min\", \"\")) == str(row_dict.get(\"date_min\", \"\"))\n",
    "            same_dmax = str(last_row_dict.get(\"date_max\", \"\")) == str(row_dict.get(\"date_max\", \"\"))\n",
    "            same_path = str(last_row_dict.get(\"target_path\", \"\")) == str(row_dict.get(\"target_path\", \"\"))\n",
    "            if last_ticker == TICKER and same_rows and same_dmin and same_dmax and same_path:\n",
    "                need_append = False\n",
    "        # Se última linha não é do ^BVSP, garantir que a última linha após operação seja do ^BVSP => forçar append\n",
    "        if last_row_dict is not None and last_row_dict.get(\"ticker\", \"\") != TICKER:\n",
    "            need_append = True\n",
    "\n",
    "        if need_append:\n",
    "            try:\n",
    "                with open(manifesto_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "                    writer = csv.DictWriter(f, fieldnames=[\"timestamp\",\"ticker\",\"rows_total\",\"date_min\",\"date_max\",\"columns_json\",\"partitions_json\",\"target_path\"])\n",
    "                    writer.writerow(row_dict)\n",
    "                action = \"appended_row\"\n",
    "            except Exception as e:\n",
    "                print_normative_error(f\"VALIDATION_ERROR: falha ao fazer append no manifesto. Detalhe: {e}\")\n",
    "        else:\n",
    "            action = \"skipped\"\n",
    "            reason = \"última linha ^BVSP já reflete o estado atual\"\n",
    "\n",
    "    return action, reason\n",
    "\n",
    "def read_manifesto_tail(manifesto_path: str):\n",
    "    try:\n",
    "        with open(manifesto_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [ln.rstrip(\"\\n\") for ln in f.readlines()]\n",
    "        if len(lines) == 0:\n",
    "            return \"\"\n",
    "        return lines[-1]\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def main():\n",
    "    # 1) Reabrir dataset físico e coletar resumo\n",
    "    dataset_info = read_dataset_summary(DATASET_PATH)\n",
    "\n",
    "    # 2) Validar existência e estrutura mínima\n",
    "    if not dataset_info[\"exists\"] or not dataset_info[\"is_dir\"]:\n",
    "        print(f\"dataset_found: {json.dumps(dataset_info, ensure_ascii=False)}\")\n",
    "        print_normative_error(\"VALIDATION_ERROR: dataset não encontrado ou não é diretório.\")\n",
    "    if not dataset_info[\"has_year_subdirs\"]:\n",
    "        print(f\"dataset_found: {json.dumps(dataset_info, ensure_ascii=False)}\")\n",
    "        print_normative_error(\"VALIDATION_ERROR: partições de ano não detectadas (year=YYYY).\")\n",
    "    if dataset_info[\"rows_total\"] is None or dataset_info[\"rows_total\"] <= 0:\n",
    "        print(f\"dataset_found: {json.dumps(dataset_info, ensure_ascii=False)}\")\n",
    "        print_normative_error(\"VALIDATION_ERROR: falha ao computar rows_total do dataset.\")\n",
    "    if not dataset_info[\"date_min\"] or not dataset_info[\"date_max\"]:\n",
    "        print(f\"dataset_found: {json.dumps(dataset_info, ensure_ascii=False)}\")\n",
    "        print_normative_error(\"VALIDATION_ERROR: falha ao computar extremos de data (date_min/date_max).\")\n",
    "\n",
    "    # 3) Construir linha para o manifesto\n",
    "    now_iso = datetime.now(timezone.utc).isoformat()\n",
    "    partitions_json = json.dumps(dataset_info[\"partitions\"], ensure_ascii=False)\n",
    "    row = {\n",
    "        \"timestamp\": now_iso,\n",
    "        \"ticker\": TICKER,\n",
    "        \"rows_total\": str(dataset_info[\"rows_total\"]),\n",
    "        \"date_min\": dataset_info[\"date_min\"],\n",
    "        \"date_max\": dataset_info[\"date_max\"],\n",
    "        \"columns_json\": COLUMNS_JSON,\n",
    "        \"partitions_json\": partitions_json,\n",
    "        \"target_path\": TARGET_PATH,\n",
    "    }\n",
    "\n",
    "    # 4) Status atual do manifesto\n",
    "    manifesto_status = read_manifesto_status(MANIFESTO_PATH)\n",
    "\n",
    "    # 5) Escrever (criar/append/skip)\n",
    "    action, reason = append_or_create_manifesto(MANIFESTO_PATH, row, manifesto_status)\n",
    "\n",
    "    # 6) Checklist: dataset_found, manifesto_status, write_action, manifesto_tail\n",
    "    print(f\"dataset_found: {json.dumps({k: (v if k!='partitions' else None) for k,v in dataset_info.items() if k!='partitions'}, ensure_ascii=False)}\")\n",
    "    # Mostrar partitions em manifesto_status? Requisito pede apenas no row; aqui imprimimos status do manifesto\n",
    "    ms_print = {\n",
    "        \"path\": manifesto_status[\"path\"],\n",
    "        \"exists_before\": manifesto_status[\"exists_before\"],\n",
    "        \"had_row_for_^BVSP_before\": manifesto_status[\"had_row_for_^BVSP_before\"]\n",
    "    }\n",
    "    print(f\"manifesto_status: {json.dumps(ms_print, ensure_ascii=False)}\")\n",
    "    if action == \"skipped\" and reason:\n",
    "        print(f\"write_action: {action} (motivo: {reason})\")\n",
    "    else:\n",
    "        print(f\"write_action: {action}\")\n",
    "\n",
    "    tail = read_manifesto_tail(MANIFESTO_PATH)\n",
    "    print(f\"manifesto_tail: {tail}\")\n",
    "\n",
    "    # 7) Verificação final: última linha deve ser do ^BVSP\n",
    "    try:\n",
    "        # Extrair ticker da última linha\n",
    "        # tail contém CSV; assumir segundo campo é 'ticker'\n",
    "        # Se tiver header na última linha (arquivo minimal?), tratar\n",
    "        if tail.strip() == \"\":\n",
    "            print_normative_error(\"CHECKLIST_FAILURE: manifesto vazio após operação.\")\n",
    "        parts = next(csv.reader([tail]))\n",
    "        # Detectar header acidental\n",
    "        if parts and parts[0] == \"timestamp\":\n",
    "            # Pegar penúltima linha se houver\n",
    "            with open(MANIFESTO_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = [ln.rstrip(\"\\n\") for ln in f.readlines()]\n",
    "            if len(lines) >= 2:\n",
    "                tail = lines[-1]\n",
    "                parts = next(csv.reader([tail]))\n",
    "            else:\n",
    "                print_normative_error(\"CHECKLIST_FAILURE: manifesto contém apenas header.\")\n",
    "        # ticker deve estar na coluna 2 (índice 1)\n",
    "        if len(parts) < 2 or parts[1] != TICKER:\n",
    "            print_normative_error(\"CHECKLIST_FAILURE: última linha do manifesto não é do ^BVSP.\")\n",
    "    except SystemExit:\n",
    "        raise\n",
    "    except Exception:\n",
    "        print_normative_error(\"CHECKLIST_FAILURE: falha ao validar a última linha do manifesto.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except SystemExit:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        # Mensagem normativa genérica\n",
    "        msg = f\"VALIDATION_ERROR: exceção não tratada. Detalhe: {e}\\n{traceback.format_exc()}\"\n",
    "        print(msg)\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa534530",
   "metadata": {},
   "source": [
    "# Instrução 1B–MANIFESTO–REPAIR — normalizar header + garantir linha ^BVSP (dry_run=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4344b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manifest_before: {\"exists\": true, \"cols\": [\"timestamp\", \"ticker\", \"rows_total\", \"date_min\", \"date_max\", \"columns_json\", \"partitions_json\", \"target_path\"], \"rows\": 7, \"had_ticker_col\": true, \"had_row_for_^BVSP\": false}\n",
      "dataset_probe: {\"path_exists\": true, \"is_dir\": true, \"has_year_subdirs\": true, \"rows_total\": 3400, \"date_min\": \"2012-01-03\", \"date_max\": \"2025-09-19\"}\n",
      "repair_actions: [\"added_header=no\", \"added_hash_cols=yes\", \"appended_bvsp_row=yes\"]\n",
      "manifest_after_tail: \" \"\"year=2019\"\"\",\" \"\"year=2020\"\"\",\" \"\"year=2021\"\"\",\" \"\"year=2022\"\"\",\" \"\"year=2023\"\"\",\" \"\"year=2024\"\"\",\" \"\"year=2025\"\"]\",/home/wrm/BOLSA_2026/bronze/IBOV.parquet,,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69344/3483137373.py:222: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ts = pd.to_datetime(dfm[\"timestamp\"], errors=\"coerce\")\n",
      "/tmp/ipykernel_69344/3483137373.py:223: FutureWarning: The behavior of Series.argsort in the presence of NA values is deprecated. In a future version, NA values will be ordered last instead of set to -1.\n",
      "  order = ts.argsort(kind=\"mergesort\")  # estável\n"
     ]
    }
   ],
   "source": [
    "# Instrução 1B–MANIFESTO–REPAIR — normalizar header + garantir linha ^BVSP (dry_run=False)\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Dependências opcionais para reabrir dataset\n",
    "try:\n",
    "    import pyarrow.dataset as ds\n",
    "except Exception:\n",
    "    ds = None\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    pd = None\n",
    "\n",
    "MANIFEST_PATH = \"/home/wrm/BOLSA_2026/manifestos/bronze_ibov_manifesto.csv\"\n",
    "DATASET_PATH = \"/home/wrm/BOLSA_2026/bronze/IBOV.parquet\"\n",
    "TICKER = \"^BVSP\"\n",
    "EXPECTED_COLUMNS = [\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"ticker\"]\n",
    "CANONICAL_COLS = [\n",
    "    \"timestamp\",\"ticker\",\"rows_total\",\"date_min\",\"date_max\",\n",
    "    \"columns_json\",\"partitions_json\",\"target_path\",\"hash_head20\",\"hash_tail20\"\n",
    "]\n",
    "\n",
    "\n",
    "def print_normative_error(msg: str):\n",
    "    print(msg)\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "def safe_iso_date(ts) -> str:\n",
    "    if ts is None:\n",
    "        return \"\"\n",
    "    if isinstance(ts, str):\n",
    "        return ts\n",
    "    try:\n",
    "        if hasattr(ts, \"to_pydatetime\"):\n",
    "            ts = ts.to_pydatetime()\n",
    "        if isinstance(ts, datetime):\n",
    "            return ts.date().isoformat()\n",
    "        return str(ts)\n",
    "    except Exception:\n",
    "        return str(ts)\n",
    "\n",
    "\n",
    "def probe_dataset(path: str):\n",
    "    exists = os.path.exists(path)\n",
    "    is_dir = os.path.isdir(path)\n",
    "    partitions = []\n",
    "    has_year_subdirs = False\n",
    "    rows_total = None\n",
    "    dmin = None\n",
    "    dmax = None\n",
    "\n",
    "    if is_dir:\n",
    "        try:\n",
    "            for name in os.listdir(path):\n",
    "                full = os.path.join(path, name)\n",
    "                if os.path.isdir(full) and re.fullmatch(r\"year=\\d{4}\", name):\n",
    "                    partitions.append(name)\n",
    "            partitions.sort()\n",
    "            has_year_subdirs = len(partitions) > 0\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if exists and is_dir and has_year_subdirs:\n",
    "        if ds is not None:\n",
    "            try:\n",
    "                dset = ds.dataset(path, format=\"parquet\", partitioning=\"hive\")\n",
    "                try:\n",
    "                    rows_total = dset.count_rows()\n",
    "                except Exception:\n",
    "                    rows_total = dset.to_table(columns=[\"date\"]).num_rows\n",
    "                tbl_date = dset.to_table(columns=[\"date\"])  # may be large but manageable for daily data\n",
    "                if pd is None:\n",
    "                    col = tbl_date.column(\"date\")\n",
    "                    vals = [v for v in col.to_pylist() if v is not None]\n",
    "                    if not vals:\n",
    "                        raise ValueError(\"Coluna 'date' vazia.\")\n",
    "                    dmin, dmax = min(vals), max(vals)\n",
    "                else:\n",
    "                    s = tbl_date.to_pandas()\n",
    "                    if isinstance(s, pd.DataFrame):\n",
    "                        s = s[\"date\"] if \"date\" in s.columns else s.iloc[:, 0]\n",
    "                    s = pd.to_datetime(s, utc=True, errors=\"coerce\").dropna()\n",
    "                    if s.empty:\n",
    "                        raise ValueError(\"Coluna 'date' sem valores válidos.\")\n",
    "                    dmin, dmax = s.min(), s.max()\n",
    "            except Exception as e:\n",
    "                if pd is None:\n",
    "                    print_normative_error(f\"VALIDATION_ERROR: falha ao reabrir dataset (pyarrow.dataset) e pandas ausente. Detalhe: {e}\")\n",
    "                try:\n",
    "                    df = pd.read_parquet(path, columns=[\"date\"])\n",
    "                    if df.empty:\n",
    "                        raise ValueError(\"Dataset vazio.\")\n",
    "                    rows_total = len(df)\n",
    "                    s = pd.to_datetime(df[\"date\"], utc=True, errors=\"coerce\").dropna()\n",
    "                    if s.empty:\n",
    "                        raise ValueError(\"Coluna 'date' sem valores válidos (pandas).\")\n",
    "                    dmin, dmax = s.min(), s.max()\n",
    "                except Exception as e2:\n",
    "                    print_normative_error(f\"VALIDATION_ERROR: falha ao reabrir dataset (pandas). Detalhe: {e2}\")\n",
    "        else:\n",
    "            if pd is None:\n",
    "                print_normative_error(\"VALIDATION_ERROR: nem pyarrow.dataset nem pandas disponíveis para reabrir dataset.\")\n",
    "            try:\n",
    "                df = pd.read_parquet(path, columns=[\"date\"])\n",
    "                if df.empty:\n",
    "                    raise ValueError(\"Dataset vazio.\")\n",
    "                rows_total = len(df)\n",
    "                s = pd.to_datetime(df[\"date\"], utc=True, errors=\"coerce\").dropna()\n",
    "                if s.empty:\n",
    "                    raise ValueError(\"Coluna 'date' sem valores válidos (pandas).\")\n",
    "                dmin, dmax = s.min(), s.max()\n",
    "            except Exception as e3:\n",
    "                print_normative_error(f\"VALIDATION_ERROR: falha ao reabrir dataset (pandas). Detalhe: {e3}\")\n",
    "\n",
    "    return {\n",
    "        \"path_exists\": exists,\n",
    "        \"is_dir\": is_dir,\n",
    "        \"has_year_subdirs\": has_year_subdirs,\n",
    "        \"rows_total\": (int(rows_total) if rows_total is not None else None),\n",
    "        \"date_min\": safe_iso_date(dmin),\n",
    "        \"date_max\": safe_iso_date(dmax),\n",
    "        \"partitions\": partitions,\n",
    "    }\n",
    "\n",
    "\n",
    "# 1) Ler manifesto\n",
    "exists_before = os.path.exists(MANIFEST_PATH)\n",
    "manifest_before = {\n",
    "    \"exists\": exists_before,\n",
    "    \"cols\": [],\n",
    "    \"rows\": 0,\n",
    "    \"had_ticker_col\": False,\n",
    "    \"had_row_for_^BVSP\": False,\n",
    "}\n",
    "added_header = False\n",
    "added_hash_cols = False\n",
    "appended_bvsp = False\n",
    "\n",
    "if pd is None:\n",
    "    print_normative_error(\"VALIDATION_ERROR: pandas não disponível para normalização do manifesto.\")\n",
    "\n",
    "if exists_before:\n",
    "    try:\n",
    "        dfm = pd.read_csv(MANIFEST_PATH, sep=\",\", header=0, dtype=str)\n",
    "    except Exception as e:\n",
    "        print_normative_error(f\"VALIDATION_ERROR: falha ao ler manifesto com header=0. Detalhe: {e}\")\n",
    "    manifest_before[\"cols\"] = dfm.columns.tolist()\n",
    "    manifest_before[\"rows\"] = int(len(dfm))\n",
    "    manifest_before[\"had_ticker_col\"] = (\"ticker\" in dfm.columns)\n",
    "    if manifest_before[\"had_ticker_col\"]:\n",
    "        manifest_before[\"had_row_for_^BVSP\"] = bool((dfm[\"ticker\"].astype(str) == TICKER).any())\n",
    "    else:\n",
    "        # Reabrir com header=None e forçar schema canônico\n",
    "        try:\n",
    "            dfm = pd.read_csv(MANIFEST_PATH, sep=\",\", header=None, dtype=str, names=CANONICAL_COLS)\n",
    "            added_header = True\n",
    "            manifest_before[\"cols\"] = dfm.columns.tolist()\n",
    "            manifest_before[\"rows\"] = int(len(dfm))\n",
    "            manifest_before[\"had_ticker_col\"] = True\n",
    "            manifest_before[\"had_row_for_^BVSP\"] = bool((dfm[\"ticker\"].astype(str) == TICKER).any())\n",
    "        except Exception as e:\n",
    "            print_normative_error(f\"VALIDATION_ERROR: falha ao reler manifesto com header=None. Detalhe: {e}\")\n",
    "else:\n",
    "    # Criar DataFrame vazio com schema canônico\n",
    "    dfm = pd.DataFrame(columns=CANONICAL_COLS)\n",
    "    added_header = True\n",
    "\n",
    "# 2) Padronizar tipos e colunas canônicas\n",
    "for col in CANONICAL_COLS:\n",
    "    if col not in dfm.columns:\n",
    "        dfm[col] = \"\"\n",
    "        if col in (\"hash_head20\", \"hash_tail20\"):\n",
    "            added_hash_cols = True\n",
    "\n",
    "# Se manifesto tinha colunas extras, manter apenas as canônicas\n",
    "if dfm.columns.tolist() != CANONICAL_COLS:\n",
    "    # Verifique se hash cols estavam ausentes\n",
    "    for hc in (\"hash_head20\", \"hash_tail20\"):\n",
    "        if hc not in dfm.columns:\n",
    "            dfm[hc] = \"\"\n",
    "            added_hash_cols = True\n",
    "    dfm = dfm[CANONICAL_COLS]\n",
    "\n",
    "# Cast básicos\n",
    "dfm[\"ticker\"] = dfm[\"ticker\"].astype(str).fillna(\"\")\n",
    "\n",
    "# 3) Garantir linha ^BVSP (se ausente)\n",
    "if not (dfm[\"ticker\"] == TICKER).any():\n",
    "    probe = probe_dataset(DATASET_PATH)\n",
    "    # Imprimir probe já agora se faltar estrutura mínima\n",
    "    if not (probe[\"path_exists\"] and probe[\"is_dir\"] and probe[\"has_year_subdirs\"]):\n",
    "        print(f\"dataset_probe: {json.dumps({k: (v if k!='partitions' else probe['partitions']) for k,v in probe.items()}, ensure_ascii=False)}\")\n",
    "        print_normative_error(\"VALIDATION_ERROR: dataset indisponível para gerar linha do manifesto.\")\n",
    "    if probe[\"rows_total\"] is None or probe[\"rows_total\"] <= 0 or not probe[\"date_min\"] or not probe[\"date_max\"]:\n",
    "        print(f\"dataset_probe: {json.dumps({k: (v if k!='partitions' else probe['partitions']) for k,v in probe.items()}, ensure_ascii=False)}\")\n",
    "        print_normative_error(\"VALIDATION_ERROR: falha ao obter métricas do dataset (rows_total/date_min/date_max).\")\n",
    "\n",
    "    now_iso = datetime.now(timezone.utc).isoformat()\n",
    "    row = {\n",
    "        \"timestamp\": now_iso,\n",
    "        \"ticker\": TICKER,\n",
    "        \"rows_total\": str(probe[\"rows_total\"]),\n",
    "        \"date_min\": probe[\"date_min\"],\n",
    "        \"date_max\": probe[\"date_max\"],\n",
    "        \"columns_json\": json.dumps(EXPECTED_COLUMNS, ensure_ascii=False),\n",
    "        \"partitions_json\": json.dumps(probe[\"partitions\"], ensure_ascii=False),\n",
    "        \"target_path\": DATASET_PATH,\n",
    "        \"hash_head20\": \"\",\n",
    "        \"hash_tail20\": \"\",\n",
    "    }\n",
    "    dfm = pd.concat([dfm, pd.DataFrame([row])], ignore_index=True)\n",
    "    appended_bvsp = True\n",
    "    # Ordenar por timestamp ascendente\n",
    "    try:\n",
    "        ts = pd.to_datetime(dfm[\"timestamp\"], errors=\"coerce\")\n",
    "        order = ts.argsort(kind=\"mergesort\")  # estável\n",
    "        dfm = dfm.iloc[order].reset_index(drop=True)\n",
    "    except Exception:\n",
    "        # Se falhar parsing, deixa como está\n",
    "        pass\n",
    "\n",
    "# 4) Salvar sobrescrevendo\n",
    "try:\n",
    "    dfm.to_csv(MANIFEST_PATH, index=False)\n",
    "except Exception as e:\n",
    "    print_normative_error(f\"VALIDATION_ERROR: falha ao salvar manifesto normalizado. Detalhe: {e}\")\n",
    "\n",
    "# Checklist\n",
    "manifest_before_print = {\n",
    "    \"exists\": manifest_before[\"exists\"],\n",
    "    \"cols\": manifest_before[\"cols\"],\n",
    "    \"rows\": manifest_before[\"rows\"],\n",
    "    \"had_ticker_col\": manifest_before[\"had_ticker_col\"],\n",
    "    \"had_row_for_^BVSP\": manifest_before[\"had_row_for_^BVSP\"],\n",
    "}\n",
    "print(f\"manifest_before: {json.dumps(manifest_before_print, ensure_ascii=False)}\")\n",
    "\n",
    "# Probe do dataset para checklist final\n",
    "probe_final = probe_dataset(DATASET_PATH)\n",
    "probe_print = {\n",
    "    \"path_exists\": probe_final[\"path_exists\"],\n",
    "    \"is_dir\": probe_final[\"is_dir\"],\n",
    "    \"has_year_subdirs\": probe_final[\"has_year_subdirs\"],\n",
    "    \"rows_total\": probe_final[\"rows_total\"],\n",
    "    \"date_min\": probe_final[\"date_min\"],\n",
    "    \"date_max\": probe_final[\"date_max\"],\n",
    "}\n",
    "print(f\"dataset_probe: {json.dumps(probe_print, ensure_ascii=False)}\")\n",
    "\n",
    "rep_actions = [\n",
    "    f\"added_header={'yes' if added_header else 'no'}\",\n",
    "    f\"added_hash_cols={'yes' if added_hash_cols else 'no'}\",\n",
    "    f\"appended_bvsp_row={'yes' if appended_bvsp else 'no'}\",\n",
    "]\n",
    "print(f\"repair_actions: {json.dumps(rep_actions, ensure_ascii=False)}\")\n",
    "\n",
    "# Tail do manifesto\n",
    "try:\n",
    "    with open(MANIFEST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [ln.rstrip(\"\\n\") for ln in f.readlines()]\n",
    "    tail = lines[-1] if lines else \"\"\n",
    "    if not tail:\n",
    "        print_normative_error(\"CHECKLIST_FAILURE: manifesto vazio após normalização.\")\n",
    "    print(f\"manifest_after_tail: {tail}\")\n",
    "except Exception:\n",
    "    print_normative_error(\"CHECKLIST_FAILURE: falha ao ler tail do manifesto.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671311c9",
   "metadata": {},
   "source": [
    "# Instrução 1C-STRICT — Reabrir Bronze pelo SSOT e Atualizar Manifesto (hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e302a506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== MANIFESTO — LINHA MAIS RECENTE (^BVSP) ========\n",
      "{\n",
      "  \"manifesto_row_loaded\": {\n",
      "    \"target_path_manifest\": \"/home/wrm/BOLSA_2026/bronze/IBOV.parquet\",\n",
      "    \"ticker\": \"^BVSP\",\n",
      "    \"rows_total\": 3400,\n",
      "    \"date_min\": \"2012-01-03\",\n",
      "    \"date_max\": \"2025-09-19\"\n",
      "  }\n",
      "}\n",
      "\n",
      "======== TARGET_PATH — VERIFICAÇÕES ========\n",
      "{\n",
      "  \"target_path_check\": {\n",
      "    \"path\": \"/home/wrm/BOLSA_2026/bronze/IBOV.parquet\",\n",
      "    \"exists\": true,\n",
      "    \"is_dir\": true,\n",
      "    \"has_year_subdirs\": true\n",
      "  }\n",
      "}\n",
      "\n",
      "======== DATASET — ABERTURA ========\n",
      "\n",
      "======== DATASET — SUMÁRIOS ========\n",
      "{\n",
      "  \"dataset_summary\": {\n",
      "    \"min_date\": \"2012-01-03 00:00:00\",\n",
      "    \"max_date\": \"2025-09-19 00:00:00\",\n",
      "    \"rows_total\": 3400\n",
      "  },\n",
      "  \"extreme_partitions_summary\": {\n",
      "    \"min_year\": 2012,\n",
      "    \"min_year_summary\": {\n",
      "      \"min_date\": \"2012-01-03 00:00:00\",\n",
      "      \"max_date\": \"2012-12-28 00:00:00\",\n",
      "      \"rows\": 244\n",
      "    },\n",
      "    \"max_year\": 2025,\n",
      "    \"max_year_summary\": {\n",
      "      \"min_date\": \"2025-01-02 00:00:00\",\n",
      "      \"max_date\": \"2025-09-19 00:00:00\",\n",
      "      \"rows\": 181\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "======== HASHES — HEAD20/TAIL20 ========\n",
      "{\n",
      "  \"hash_head20\": \"a236d590f9ddb0ddc9123c7e4d05909936d9f08a7db2fa93304db9beef2bb337\",\n",
      "  \"hash_tail20\": \"d7c9f771a3fa160cd023cd418f73e36cfe85e845bc70ef13c8eb428ba6055c20\"\n",
      "}\n",
      "\n",
      "======== MANIFESTO — ATUALIZAÇÃO ========\n",
      "timestamp,ticker,rows_total,date_min,date_max,columns_json,partitions_json,target_path,hash_head20,hash_tail20\n",
      "2025-09-19T13:36:33.767957+00:00,^BVSP,3400,2012-01-03,2025-09-19,\"[\"\"date\"\", \"\"open\"\", \"\"high\"\", \"\"low\"\", \"\"close\"\", \"\"volume\"\", \"\"ticker\"\"]\",\"[\"\"year=2012\"\", \"\"year=2013\"\", \"\"year=2014\"\", \"\"year=2015\"\", \"\"year=2016\"\", \"\"year=2017\"\", \"\"year=2018\"\", \"\"year=2019\"\", \"\"year=2020\"\", \"\"year=2021\"\", \"\"year=2022\"\", \"\"year=2023\"\", \"\"year=2024\"\", \"\"year=2025\"\"]\",/home/wrm/BOLSA_2026/bronze/IBOV.parquet,a236d590f9ddb0ddc9123c7e4d05909936d9f08a7db2fa93304db9beef2bb337,d7c9f771a3fa160cd023cd418f73e36cfe85e845bc70ef13c8eb428ba6055c20\n",
      "\n",
      "======== CHECKLIST ========\n",
      "{\n",
      "  \"manifesto_row_loaded\": \"ok\",\n",
      "  \"target_path_check\": \"ok\",\n",
      "  \"dataset_summary\": \"ok\",\n",
      "  \"extreme_partitions_summary\": \"ok\",\n",
      "  \"hashes_computed\": \"ok\",\n",
      "  \"manifesto_update_ok\": \"ok\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69344/767327228.py:186: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'a236d590f9ddb0ddc9123c7e4d05909936d9f08a7db2fa93304db9beef2bb337' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dfm.at[idx, \"hash_head20\"] = hash_head\n",
      "/tmp/ipykernel_69344/767327228.py:187: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'd7c9f771a3fa160cd023cd418f73e36cfe85e845bc70ef13c8eb428ba6055c20' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dfm.at[idx, \"hash_tail20\"] = hash_tail\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Instrução 1C-STRICT — Reabrir Bronze pelo SSOT e Atualizar Manifesto (hashes)\n",
    "# Regras:\n",
    "# - Bloco único, auto-contido.\n",
    "# - dry_run=False (atualiza manifesto).\n",
    "# - Usar APENAS os caminhos do SSOT (manifesto -> target_path).\n",
    "# - Dataset Parquet particionado por year=YYYY, abrir preferindo pyarrow.dataset.\n",
    "# - Mensagens normativas: VALIDATION_ERROR / CHECKLIST_FAILURE.\n",
    "# - Em dois erros consecutivos, parar e emitir dúvidas objetivas.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# Parâmetros\n",
    "# =========================\n",
    "ROOT_DIR = Path(\"/home/wrm/BOLSA_2026\").resolve()\n",
    "MANIFEST_PATH = ROOT_DIR / \"manifestos\" / \"bronze_ibov_manifesto.csv\"\n",
    "TICKER = \"^BVSP\"\n",
    "DRY_RUN = False  # autorizado a atualizar manifesto\n",
    "\n",
    "EXPECTED_COLUMNS = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"ticker\"]\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "def print_section(title: str):\n",
    "    print(\"\\n\" + \"=\" * 8 + f\" {title} \" + \"=\" * 8)\n",
    "\n",
    "def has_year_subdirs(path: Path) -> bool:\n",
    "    try:\n",
    "        if not path.is_dir():\n",
    "            return False\n",
    "        for child in path.iterdir():\n",
    "            if child.is_dir() and re.fullmatch(r\"year=20\\d{2}\", child.name):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def read_manifest_latest_row(manifest_path: Path, ticker: str) -> Tuple[Optional[pd.DataFrame], Optional[int], List[str]]:\n",
    "    errs: List[str] = []\n",
    "    if not manifest_path.exists():\n",
    "        errs.append(\"VALIDATION_ERROR: MANIFEST_NOT_FOUND\")\n",
    "        return None, None, errs\n",
    "    try:\n",
    "        dfm = pd.read_csv(manifest_path)\n",
    "    except Exception as e:\n",
    "        errs.append(f\"VALIDATION_ERROR: MANIFEST_READ_ERROR — {e}\")\n",
    "        return None, None, errs\n",
    "    if \"ticker\" not in dfm.columns:\n",
    "        errs.append(\"VALIDATION_ERROR: MANIFEST_MISSING_TICKER_COLUMN\")\n",
    "        return dfm, None, errs\n",
    "    dfm_tk = dfm[dfm[\"ticker\"] == ticker]\n",
    "    if dfm_tk.empty:\n",
    "        errs.append(f\"VALIDATION_ERROR: MANIFEST_NO_ROW_FOR_TICKER — {ticker}\")\n",
    "        return dfm, None, errs\n",
    "    idx_latest: Optional[int] = None\n",
    "    if \"timestamp\" in dfm.columns:\n",
    "        try:\n",
    "            ts = pd.to_datetime(dfm[\"timestamp\"], errors=\"coerce\")\n",
    "            mask = dfm[\"ticker\"] == ticker\n",
    "            if ts.notna().any() and mask.any():\n",
    "                idx_latest = ts[mask].idxmax()\n",
    "        except Exception:\n",
    "            idx_latest = None\n",
    "    if idx_latest is None:\n",
    "        idxs = dfm.index[dfm[\"ticker\"] == ticker].tolist()\n",
    "        idx_latest = idxs[-1] if idxs else None\n",
    "    if idx_latest is None:\n",
    "        errs.append(\"VALIDATION_ERROR: MANIFEST_CANNOT_LOCATE_LATEST_ROW\")\n",
    "    return dfm, idx_latest, errs\n",
    "\n",
    "def open_dataset_with_pyarrow(path: Path) -> pd.DataFrame:\n",
    "    import pyarrow.dataset as ds  # type: ignore\n",
    "    dataset = ds.dataset(str(path), format=\"parquet\", partitioning=\"hive\")\n",
    "    table = dataset.to_table()\n",
    "    return table.to_pandas()\n",
    "\n",
    "def open_dataset_with_pandas(path: Path) -> pd.DataFrame:\n",
    "    # pandas + pyarrow engine will generally discover hive partitions automatically\n",
    "    try:\n",
    "        return pd.read_parquet(str(path), engine=\"pyarrow\")  # type: ignore\n",
    "    except Exception:\n",
    "        return pd.read_parquet(str(path))  # engine auto\n",
    "\n",
    "def open_dataset_strict(path: Path) -> Tuple[Optional[pd.DataFrame], List[str], str]:\n",
    "    errs: List[str] = []\n",
    "    # 1) pyarrow.dataset\n",
    "    try:\n",
    "        df = open_dataset_with_pyarrow(path)\n",
    "        return df, errs, \"pyarrow.dataset\"\n",
    "    except Exception as e1:\n",
    "        errs.append(f\"OPEN_ERROR_PA_DS: {e1}\")\n",
    "    # 2) pandas.read_parquet\n",
    "    try:\n",
    "        df = open_dataset_with_pandas(path)\n",
    "        return df, errs, \"pandas.read_parquet\"\n",
    "    except Exception as e2:\n",
    "        errs.append(f\"OPEN_ERROR_PD_RP: {e2}\")\n",
    "    return None, errs, \"none\"\n",
    "\n",
    "def normalize_bronze_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in EXPECTED_COLUMNS:\n",
    "        if c not in df.columns:\n",
    "            raise RuntimeError(f\"DATASET_SCHEMA_MISSING_COLUMN: {c}\")\n",
    "    out = df.copy()\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\").dt.tz_localize(None).dt.normalize()\n",
    "    for c in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").astype(\"float64\")\n",
    "    out[\"volume\"] = pd.to_numeric(out[\"volume\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "    out[\"ticker\"] = out[\"ticker\"].astype(\"string\")\n",
    "    out = out[EXPECTED_COLUMNS].sort_values(\"date\").drop_duplicates(subset=[\"date\"], keep=\"last\").reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def dataset_summary(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    if df is None or df.empty:\n",
    "        return {\"min_date\": None, \"max_date\": None, \"rows_total\": 0}\n",
    "    d = df.copy()\n",
    "    d[\"date\"] = pd.to_datetime(d[\"date\"], errors=\"coerce\").dt.tz_localize(None).dt.normalize()\n",
    "    return {\n",
    "        \"min_date\": str(d[\"date\"].min()),\n",
    "        \"max_date\": str(d[\"date\"].max()),\n",
    "        \"rows_total\": int(len(d))\n",
    "    }\n",
    "\n",
    "def extremes_by_year(df: pd.DataFrame) -> Tuple[Optional[int], Optional[int], Dict[str, Any], Dict[str, Any]]:\n",
    "    if df is None or df.empty or \"date\" not in df.columns:\n",
    "        return None, None, {\"min_date\": None, \"max_date\": None, \"rows\": 0}, {\"min_date\": None, \"max_date\": None, \"rows\": 0}\n",
    "    d = df.copy()\n",
    "    d[\"date\"] = pd.to_datetime(d[\"date\"], errors=\"coerce\").dt.tz_localize(None).dt.normalize()\n",
    "    yrs = d[\"date\"].dt.year.dropna().astype(int)\n",
    "    if yrs.empty:\n",
    "        return None, None, {\"min_date\": None, \"max_date\": None, \"rows\": 0}, {\"min_date\": None, \"max_date\": None, \"rows\": 0}\n",
    "    y_min, y_max = int(yrs.min()), int(yrs.max())\n",
    "    g_min = d[yrs == y_min]\n",
    "    g_max = d[yrs == y_max]\n",
    "    s_min = {\"min_date\": str(g_min[\"date\"].min()) if not g_min.empty else None,\n",
    "             \"max_date\": str(g_min[\"date\"].max()) if not g_min.empty else None,\n",
    "             \"rows\": int(len(g_min))}\n",
    "    s_max = {\"min_date\": str(g_max[\"date\"].min()) if not g_max.empty else None,\n",
    "             \"max_date\": str(g_max[\"date\"].max()) if not g_max.empty else None,\n",
    "             \"rows\": int(len(g_max))}\n",
    "    return y_min, y_max, s_min, s_max\n",
    "\n",
    "def sha256_of_csv(df: pd.DataFrame) -> str:\n",
    "    csv_str = df.to_csv(index=False)\n",
    "    return hashlib.sha256(csv_str.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def compute_hashes(df: pd.DataFrame) -> Tuple[str, str]:\n",
    "    for c in EXPECTED_COLUMNS:\n",
    "        if c not in df.columns:\n",
    "            raise RuntimeError(f\"HASH_SCHEMA_MISSING: {c}\")\n",
    "    d = df[EXPECTED_COLUMNS].copy()\n",
    "    d[\"date\"] = pd.to_datetime(d[\"date\"], errors=\"coerce\").dt.tz_localize(None).dt.normalize()\n",
    "    head20 = d.head(20)\n",
    "    tail20 = d.tail(20)\n",
    "    return sha256_of_csv(head20), sha256_of_csv(tail20)\n",
    "\n",
    "def ensure_manifest_hash_columns(dfm: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in [\"hash_head20\", \"hash_tail20\"]:\n",
    "        if c not in dfm.columns:\n",
    "            dfm[c] = np.nan\n",
    "    return dfm\n",
    "\n",
    "def update_manifest_hashes(dfm: pd.DataFrame, idx: int, final_path: Path, hash_head: str, hash_tail: str) -> Tuple[bool, Optional[pd.DataFrame], List[str]]:\n",
    "    errs: List[str] = []\n",
    "    if dfm is None or dfm.empty:\n",
    "        errs.append(\"VALIDATION_ERROR: MANIFEST_EMPTY_OR_NONE\")\n",
    "        return False, None, errs\n",
    "    dfm = ensure_manifest_hash_columns(dfm.copy())\n",
    "    if \"target_path\" not in dfm.columns:\n",
    "        dfm[\"target_path\"] = np.nan\n",
    "    try:\n",
    "        dfm.at[idx, \"hash_head20\"] = hash_head\n",
    "        dfm.at[idx, \"hash_tail20\"] = hash_tail\n",
    "        dfm.at[idx, \"target_path\"] = str(final_path)\n",
    "        if not DRY_RUN:\n",
    "            dfm.to_csv(MANIFEST_PATH, index=False)\n",
    "        return True, dfm, errs\n",
    "    except Exception as e:\n",
    "        errs.append(f\"VALIDATION_ERROR: MANIFEST_WRITE_ERROR — {e}\")\n",
    "        return False, dfm, errs\n",
    "\n",
    "# =========================\n",
    "# Execução Principal\n",
    "# =========================\n",
    "def main():\n",
    "    normative_errors: List[str] = []\n",
    "    consecutive_errors = 0\n",
    "\n",
    "    # 1) Ler manifesto e obter linha mais recente do ^BVSP\n",
    "    print_section(\"MANIFESTO — LINHA MAIS RECENTE (^BVSP)\")\n",
    "    df_manifest, idx_latest, mf_errs = read_manifest_latest_row(MANIFEST_PATH, TICKER)\n",
    "    if mf_errs:\n",
    "        for e in mf_errs:\n",
    "            print(e)\n",
    "        consecutive_errors += 1\n",
    "    else:\n",
    "        consecutive_errors = 0\n",
    "\n",
    "    manifest_row_loaded = {\n",
    "        \"target_path_manifest\": None,\n",
    "        \"ticker\": TICKER,\n",
    "        \"rows_total\": None,\n",
    "        \"date_min\": None,\n",
    "        \"date_max\": None\n",
    "    }\n",
    "\n",
    "    target_path: Optional[Path] = None\n",
    "    if df_manifest is not None and idx_latest is not None and idx_latest in df_manifest.index:\n",
    "        row = df_manifest.loc[idx_latest]\n",
    "        # preencher resumo conforme disponível no manifesto\n",
    "        manifest_row_loaded[\"target_path_manifest\"] = str(row[\"target_path\"]) if \"target_path\" in df_manifest.columns else None\n",
    "        manifest_row_loaded[\"rows_total\"] = int(row[\"rows_total\"]) if \"rows_total\" in df_manifest.columns and pd.notna(row[\"rows_total\"]) else None\n",
    "        manifest_row_loaded[\"date_min\"] = str(row[\"date_min\"]) if \"date_min\" in df_manifest.columns and pd.notna(row[\"date_min\"]) else None\n",
    "        manifest_row_loaded[\"date_max\"] = str(row[\"date_max\"]) if \"date_max\" in df_manifest.columns and pd.notna(row[\"date_max\"]) else None\n",
    "\n",
    "        tp = row[\"target_path\"] if \"target_path\" in df_manifest.columns else None\n",
    "        if isinstance(tp, str) and tp.strip():\n",
    "            target_path = Path(tp).resolve()\n",
    "        else:\n",
    "            print(\"VALIDATION_ERROR: MANIFEST_TARGET_PATH_MISSING_OR_EMPTY\")\n",
    "            consecutive_errors += 1\n",
    "    else:\n",
    "        print(\"VALIDATION_ERROR: MANIFEST_LATEST_ROW_NOT_AVAILABLE\")\n",
    "        consecutive_errors += 1\n",
    "\n",
    "    print(json.dumps({\"manifesto_row_loaded\": manifest_row_loaded}, ensure_ascii=False, indent=2))\n",
    "\n",
    "    if consecutive_errors >= 2:\n",
    "        print_section(\"CHECKLIST\")\n",
    "        checklist = {\n",
    "            \"manifesto_row_loaded\": \"falha\",\n",
    "            \"target_path_check\": \"falha\",\n",
    "            \"dataset_summary\": \"falha\",\n",
    "            \"extreme_partitions_summary\": \"falha\",\n",
    "            \"hashes_computed\": \"falha\",\n",
    "            \"manifesto_update_ok\": \"falha\"\n",
    "        }\n",
    "        print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "        for k, v in checklist.items():\n",
    "            if v != \"ok\":\n",
    "                print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "        print_section(\"DÚVIDAS OBJETIVAS\")\n",
    "        print(\"- O manifesto possui a coluna target_path preenchida para ^BVSP?\")\n",
    "        print(\"- Deseja corrigir/atualizar o manifesto com o caminho correto do dataset Bronze?\")\n",
    "        return\n",
    "\n",
    "    # 2) Validar target_path (existe, é dir, tem subpastas year=YYYY)\n",
    "    print_section(\"TARGET_PATH — VERIFICAÇÕES\")\n",
    "    target_check = {\n",
    "        \"path\": str(target_path) if target_path else None,\n",
    "        \"exists\": False,\n",
    "        \"is_dir\": False,\n",
    "        \"has_year_subdirs\": False\n",
    "    }\n",
    "    if target_path is None:\n",
    "        print(\"VALIDATION_ERROR: TARGET_PATH_NONE\")\n",
    "        consecutive_errors += 1\n",
    "    else:\n",
    "        target_check[\"exists\"] = target_path.exists()\n",
    "        target_check[\"is_dir\"] = target_path.is_dir()\n",
    "        target_check[\"has_year_subdirs\"] = has_year_subdirs(target_path) if target_path.exists() and target_path.is_dir() else False\n",
    "        if not (target_check[\"exists\"] and target_check[\"is_dir\"] and target_check[\"has_year_subdirs\"]):\n",
    "            print(f\"VALIDATION_ERROR: TARGET_PATH_INVALID — {json.dumps(target_check, ensure_ascii=False)}\")\n",
    "            consecutive_errors += 1\n",
    "        else:\n",
    "            consecutive_errors = 0\n",
    "    print(json.dumps({\"target_path_check\": target_check}, ensure_ascii=False, indent=2))\n",
    "\n",
    "    if consecutive_errors >= 2:\n",
    "        print_section(\"CHECKLIST\")\n",
    "        checklist = {\n",
    "            \"manifesto_row_loaded\": \"ok\" if manifest_row_loaded[\"target_path_manifest\"] else \"falha\",\n",
    "            \"target_path_check\": \"falha\",\n",
    "            \"dataset_summary\": \"falha\",\n",
    "            \"extreme_partitions_summary\": \"falha\",\n",
    "            \"hashes_computed\": \"falha\",\n",
    "            \"manifesto_update_ok\": \"falha\"\n",
    "        }\n",
    "        print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "        for k, v in checklist.items():\n",
    "            if v != \"ok\":\n",
    "                print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "        print_section(\"DÚVIDAS OBJETIVAS\")\n",
    "        print(\"- O target_path do manifesto aponta para um diretório particionado com subpastas year=YYYY?\")\n",
    "        print(\"- Deseja corrigir o target_path no manifesto para o caminho real do dataset?\")\n",
    "        return\n",
    "\n",
    "    # 3) Abrir dataset (pyarrow.dataset preferido; fallback pandas+pyarrow)\n",
    "    print_section(\"DATASET — ABERTURA\")\n",
    "    df_opened: Optional[pd.DataFrame] = None\n",
    "    engine_used = None\n",
    "    open_errs: List[str] = []\n",
    "    if target_path is not None:\n",
    "        df_opened, open_errs, engine_used = open_dataset_strict(target_path)\n",
    "        if df_opened is None or df_opened.empty:\n",
    "            print(json.dumps({\"open_attempts_errors\": open_errs, \"engine_used\": engine_used}, ensure_ascii=False, indent=2))\n",
    "            print(\"VALIDATION_ERROR: DATASET_OPEN_FAILED\")\n",
    "            consecutive_errors += 1\n",
    "        else:\n",
    "            try:\n",
    "                df_opened = normalize_bronze_schema(df_opened)\n",
    "                consecutive_errors = 0\n",
    "            except Exception as e:\n",
    "                print(f\"VALIDATION_ERROR: DATASET_SCHEMA_NORMALIZE_ERROR — {e}\")\n",
    "                consecutive_errors += 1\n",
    "\n",
    "    if consecutive_errors >= 2 or df_opened is None or df_opened.empty:\n",
    "        print_section(\"CHECKLIST\")\n",
    "        checklist = {\n",
    "            \"manifesto_row_loaded\": \"ok\" if manifest_row_loaded[\"target_path_manifest\"] else \"falha\",\n",
    "            \"target_path_check\": \"ok\" if target_check[\"exists\"] and target_check[\"is_dir\"] else \"falha\",\n",
    "            \"dataset_summary\": \"falha\",\n",
    "            \"extreme_partitions_summary\": \"falha\",\n",
    "            \"hashes_computed\": \"falha\",\n",
    "            \"manifesto_update_ok\": \"falha\"\n",
    "        }\n",
    "        print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "        for k, v in checklist.items():\n",
    "            if v != \"ok\":\n",
    "                print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "        print_section(\"DÚVIDAS OBJETIVAS\")\n",
    "        print(\"- Podemos instalar/usar pyarrow para leitura do dataset particionado?\")\n",
    "        print(\"- Confirme se o caminho possui arquivos Parquet válidos sob as partições year=YYYY.\")\n",
    "        return\n",
    "\n",
    "    # 4) Summaries do dataset completo e partições extremas\n",
    "    print_section(\"DATASET — SUMÁRIOS\")\n",
    "    ds_summary = dataset_summary(df_opened)\n",
    "    y_min, y_max, min_year_summary, max_year_summary = extremes_by_year(df_opened)\n",
    "    extremes = {\n",
    "        \"min_year\": y_min,\n",
    "        \"min_year_summary\": min_year_summary,\n",
    "        \"max_year\": y_max,\n",
    "        \"max_year_summary\": max_year_summary\n",
    "    }\n",
    "    print(json.dumps({\"dataset_summary\": ds_summary, \"extreme_partitions_summary\": extremes}, ensure_ascii=False, indent=2))\n",
    "\n",
    "    # 5) Hashes head20/tail20\n",
    "    print_section(\"HASHES — HEAD20/TAIL20\")\n",
    "    hashes_ok = False\n",
    "    hash_head20 = None\n",
    "    hash_tail20 = None\n",
    "    try:\n",
    "        hash_head20, hash_tail20 = compute_hashes(df_opened)\n",
    "        hashes_ok = True\n",
    "    except Exception as e:\n",
    "        print(f\"VALIDATION_ERROR: HASH_COMPUTE_ERROR — {e}\")\n",
    "    print(json.dumps({\"hash_head20\": hash_head20, \"hash_tail20\": hash_tail20}, ensure_ascii=False, indent=2))\n",
    "\n",
    "    # 6) Atualizar manifesto (mesma linha mais recente do ^BVSP)\n",
    "    print_section(\"MANIFESTO — ATUALIZAÇÃO\")\n",
    "    manifesto_ok = False\n",
    "    final_manifest_line = None\n",
    "    if df_manifest is None or idx_latest is None or idx_latest not in df_manifest.index:\n",
    "        print(\"VALIDATION_ERROR: MANIFEST_ROW_NOT_UPDATABLE\")\n",
    "    elif not hashes_ok or hash_head20 is None or hash_tail20 is None:\n",
    "        print(\"VALIDATION_ERROR: SKIP_MANIFEST_UPDATE — hashes indisponíveis.\")\n",
    "    else:\n",
    "        ok, dfm_updated, errs = update_manifest_hashes(df_manifest, idx_latest, target_path, hash_head20, hash_tail20)  # type: ignore\n",
    "        for e in errs:\n",
    "            print(e)\n",
    "        manifesto_ok = ok and (dfm_updated is not None)\n",
    "        if dfm_updated is not None:\n",
    "            # Exibir a linha final (mesma posição idx_latest)\n",
    "            try:\n",
    "                final_manifest_line = dfm_updated.loc[[idx_latest]]\n",
    "            except Exception as e:\n",
    "                print(f\"VALIDATION_ERROR: MANIFEST_PREVIEW_ERROR — {e}\")\n",
    "\n",
    "    if final_manifest_line is not None:\n",
    "        try:\n",
    "            print(final_manifest_line.to_csv(index=False).strip())\n",
    "        except Exception:\n",
    "            print(json.dumps(final_manifest_line.to_dict(orient=\"records\"), ensure_ascii=False, indent=2))\n",
    "    else:\n",
    "        print(\"MANIFESTO_PREVIEW: indisponível.\")\n",
    "\n",
    "    # 7) Checklist Obrigatório\n",
    "    print_section(\"CHECKLIST\")\n",
    "    checklist = {\n",
    "        \"manifesto_row_loaded\": \"ok\" if manifest_row_loaded[\"target_path_manifest\"] else \"falha\",\n",
    "        \"target_path_check\": \"ok\" if (target_check[\"exists\"] and target_check[\"is_dir\"] and target_check[\"has_year_subdirs\"]) else \"falha\",\n",
    "        \"dataset_summary\": \"ok\" if (ds_summary[\"rows_total\"] > 0 and ds_summary[\"min_date\"] is not None and ds_summary[\"max_date\"] is not None) else \"falha\",\n",
    "        \"extreme_partitions_summary\": \"ok\" if (y_min is not None and y_max is not None and min_year_summary[\"rows\"] > 0 and max_year_summary[\"rows\"] > 0) else \"falha\",\n",
    "        \"hashes_computed\": \"ok\" if hashes_ok else \"falha\",\n",
    "        \"manifesto_update_ok\": \"ok\" if manifesto_ok else \"falha\"\n",
    "    }\n",
    "    print(json.dumps(checklist, ensure_ascii=False, indent=2))\n",
    "    for k, v in checklist.items():\n",
    "        if v != \"ok\":\n",
    "            print(f\"CHECKLIST_FAILURE: {k} não atendido.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Contrato:\n",
    "    # - Lê SSOT (manifesto), reabre Bronze no target_path informado,\n",
    "    # - Calcula hashes head/tail 20 e atualiza a linha mais recente do ^BVSP no manifesto,\n",
    "    # - Imprime checklist e mensagens normativas.\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42ba4b",
   "metadata": {},
   "source": [
    "# ESTÁ CORRETO?\n",
    "\n",
    "Sim — está correto e completo.\n",
    "\n",
    "Por que está ok\n",
    "\n",
    "Manifesto: última linha de ^BVSP presente, com target_path=/home/wrm/BOLSA_2026/bronze/IBOV.parquet.\n",
    "\n",
    "Dataset: caminho existe, é diretório com partições year=YYYY.\n",
    "\n",
    "Sumários: min=2012-01-03, max=2025-09-19, rows_total=3400 — coerente com a reabertura.\n",
    "\n",
    "Hashes: hash_head20 e hash_tail20 calculados e gravados na mesma linha do manifesto.\n",
    "\n",
    "Checklist: todos os itens “ok”.\n",
    "\n",
    "Única observação (não bloqueia)\n",
    "\n",
    "Esses dois FutureWarning indicam que, no pandas, as colunas hash_head20 e hash_tail20 estavam com dtype numérico (provavelmente float64). Para evitar o aviso nas próximas escritas, basta garantir string antes de setar os valores, por exemplo:\n",
    "\n",
    "dfm[\"hash_head20\"] = dfm[\"hash_head20\"].astype(\"string\")\n",
    "dfm[\"hash_tail20\"] = dfm[\"hash_tail20\"].astype(\"string\")\n",
    "\n",
    "\n",
    "(Se já estiver assim na próxima execução, o aviso some.)\n",
    "\n",
    "Endereços confirmados (SSOT)\n",
    "\n",
    "Bronze (Parquet particionado): /home/wrm/BOLSA_2026/bronze/IBOV.parquet\n",
    "\n",
    "Manifesto Bronze: /home/wrm/BOLSA_2026/manifestos/bronze_ibov_manifesto.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee1bab9",
   "metadata": {},
   "source": [
    "---\n",
    "# **TÉRMINO DO BRONZE**\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project: BOLSA_2026)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
