{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd9b860",
   "metadata": {},
   "source": [
    "# Pipeline Bronze: IBOV download (Jupyter)\n",
    "\n",
    "Este notebook baixa o histórico diário do IBOV (ticker `^BVSP`) cobrindo pelo menos 8 anos, salva um arquivo Parquet em `dados_originais/` e escreve um manifesto `manifesto_dados_originais_bronze_ibov.csv`.\n",
    "\n",
    "Siga a política do projeto: não sobrescrever arquivos existentes — o notebook criará novos arquivos quando necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75298d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared paths and date range: 2013-08-18 -> 2025-09-17\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports e configurações iniciais\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "ROOT = Path('/home/wrm/BOLSA_2026')\n",
    "DADOS = ROOT / 'dados_originais'\n",
    "DADOS.mkdir(parents=True, exist_ok=True)\n",
    "TICKER = '^BVSP'\n",
    "MIN_YEARS = 12\n",
    "END = date.today()\n",
    "START = END - timedelta(days=int(MIN_YEARS * 365.25) + 30)  # buffer\n",
    "print('Prepared paths and date range:', START, '->', END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e812d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze target parquet: /home/wrm/BOLSA_2026/dados_originais/IBOV_2013-08-18_2025-09-17.parquet\n",
      "Manifest path: /home/wrm/BOLSA_2026/dados_originais/manifesto_dados_originais_bronze_ibov.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Check for existing bronze files and build output path (do not overwrite)\n",
    "out_parquet = DADOS / f'IBOV_{START.isoformat()}_{END.isoformat()}.parquet'\n",
    "manifest_csv = DADOS / 'manifesto_dados_originais_bronze_ibov.csv'\n",
    "print('Bronze target parquet:', out_parquet)\n",
    "print('Manifest path:', manifest_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48b21e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver path: /home/wrm/BOLSA_2026/intermediarios/silver/run_20250917_144039/IBOV_silver.parquet\n",
      "Silver rows: 2996 | Date range: 2013-08-19 -> 2025-09-17\n",
      "Duplicates dropped: 0\n",
      "NaNs after cleaning: {\"date\": 0, \"open\": 0, \"high\": 0, \"low\": 0, \"close\": 0, \"adj_close\": 0, \"volume\": 0}\n",
      "Columns: ['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume']\n",
      "AdjClose rule: filled_from_close_all_rows\n",
      "Manifest updated: /home/wrm/BOLSA_2026/intermediarios/silver/manifesto_silver_ibov.csv\n",
      "Silver sha256: 350216dfaa30971bb83a75e9f3364b25397b33aeebfa2eb7e80d805a2279b7ca\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Create Silver from Bronze (Instrução 02)\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import hashlib, json\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path('/home/wrm/BOLSA_2026')\n",
    "BRONZE_PATH = ROOT / 'dados_originais' / 'IBOV_2013-08-18_2025-09-17.parquet'\n",
    "BRONZE_SHA256_EXPECTED = 'c18ab2ee0d4ffbc4e969bcfc79fa2c31445c739096ea75a1e3a4968b038eaafa'\n",
    "SILVER_ROOT = ROOT / 'intermediarios' / 'silver'\n",
    "MANIFESTO_SILVER = SILVER_ROOT / 'manifesto_silver_ibov.csv'\n",
    "REQUIRED_COLS = ['date','open','high','low','close','adj_close','volume']\n",
    "\n",
    "def compute_sha256(path: Path, chunk_size: int = 8192) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open('rb') as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def flatten_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = []\n",
    "    for c in df.columns:\n",
    "        if isinstance(c, tuple):\n",
    "            cols.append(c[0])\n",
    "        else:\n",
    "            cols.append(c)\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "def snake_standardize(col: str) -> str:\n",
    "    return col.strip().lower()\n",
    "\n",
    "def ensure_date_and_order(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if 'date' not in df.columns:\n",
    "        raise KeyError(\"Input data must contain a 'date' column\")\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date', ascending=True)\n",
    "    return df\n",
    "\n",
    "def create_silver(dry_run: bool = False):\n",
    "    run_ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    run_dir = SILVER_ROOT / f'run_{run_ts}'\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    silver_path = run_dir / 'IBOV_silver.parquet'\n",
    "\n",
    "    if not BRONZE_PATH.exists():\n",
    "        raise FileNotFoundError(f'Bronze file not found: {BRONZE_PATH}')\n",
    "\n",
    "    df = pd.read_parquet(BRONZE_PATH)\n",
    "    df = flatten_columns(df)\n",
    "    col_map = {c: snake_standardize(c) for c in df.columns}\n",
    "    df = df.rename(columns=col_map)\n",
    "\n",
    "    # Ensure required cols exist\n",
    "    for c in REQUIRED_COLS:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "\n",
    "    df = df[REQUIRED_COLS]\n",
    "    df = ensure_date_and_order(df)\n",
    "\n",
    "    # Cleaning rules\n",
    "    price_cols = ['open','high','low','close','adj_close']\n",
    "    mask_all_prices_nan = df[price_cols].isna().all(axis=1)\n",
    "    df_clean = df.loc[~mask_all_prices_nan].copy()\n",
    "    df_clean = df_clean.loc[~df_clean['volume'].isna()].copy()\n",
    "\n",
    "    n_before_dups = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates(subset=['date'], keep='last')\n",
    "    n_after_dups = len(df_clean)\n",
    "    duplicates_dropped = n_before_dups - n_after_dups\n",
    "\n",
    "    df_clean = df_clean.sort_values('date', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    total_rows = len(df_clean)\n",
    "    if total_rows == 0:\n",
    "        adj_close_rule = 'no_rows_after_cleaning'\n",
    "    else:\n",
    "        adj_nan_count = int(df_clean['adj_close'].isna().sum())\n",
    "        if adj_nan_count == total_rows:\n",
    "            df_clean['adj_close'] = df_clean['close']\n",
    "            adj_close_rule = 'filled_from_close_all_rows'\n",
    "        else:\n",
    "            adj_close_rule = 'as_is'\n",
    "\n",
    "    nan_counts = df_clean.isna().sum().to_dict()\n",
    "    nan_counts_json = json.dumps({k: int(v) for k, v in nan_counts.items()})\n",
    "\n",
    "    final_cols = list(df_clean.columns)\n",
    "    if final_cols != REQUIRED_COLS:\n",
    "        df_clean = df_clean.reindex(columns=REQUIRED_COLS)\n",
    "        final_cols = list(df_clean.columns)\n",
    "        if final_cols != REQUIRED_COLS:\n",
    "            raise AssertionError(f'Final schema mismatch. Expected {REQUIRED_COLS}, got {final_cols}')\n",
    "\n",
    "    if len(df_clean) > 0:\n",
    "        date_min = df_clean['date'].min().strftime('%Y-%m-%d')\n",
    "        date_max = df_clean['date'].max().strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        date_min = None\n",
    "        date_max = None\n",
    "\n",
    "    if not dry_run:\n",
    "        df_clean.to_parquet(silver_path, index=False, compression='snappy')\n",
    "        silver_sha256 = compute_sha256(silver_path)\n",
    "\n",
    "        manifest_row = {\n",
    "            'run_ts': run_ts,\n",
    "            'source_parquet_path': str(BRONZE_PATH),\n",
    "            'source_sha256': BRONZE_SHA256_EXPECTED,\n",
    "            'rows': int(len(df_clean)),\n",
    "            'date_min': date_min,\n",
    "            'date_max': date_max,\n",
    "            'duplicates_dropped': int(duplicates_dropped),\n",
    "            'nan_counts_json': nan_counts_json,\n",
    "            'adj_close_rule': adj_close_rule,\n",
    "            'columns_json': json.dumps(REQUIRED_COLS),\n",
    "            'silver_parquet_path': str(silver_path),\n",
    "            'silver_sha256': silver_sha256\n",
    "        }\n",
    "\n",
    "        SILVER_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "        write_header = not MANIFESTO_SILVER.exists()\n",
    "        with MANIFESTO_SILVER.open('a', encoding='utf-8') as f:\n",
    "            if write_header:\n",
    "                f.write(','.join(list(manifest_row.keys())) + '\\n')\n",
    "            row_values = [str(manifest_row[h]) for h in list(manifest_row.keys())]\n",
    "            f.write(','.join(row_values) + '\\n')\n",
    "\n",
    "        # Prints required by spec (all properly-terminated strings)\n",
    "        print(f'Silver path: {silver_path}')\n",
    "        print(f'Silver rows: {len(df_clean)} | Date range: {date_min} -> {date_max}')\n",
    "        print(f'Duplicates dropped: {duplicates_dropped}')\n",
    "        print(f'NaNs after cleaning: {nan_counts_json}')\n",
    "        print(f'Columns: {REQUIRED_COLS}')\n",
    "        print(f'AdjClose rule: {adj_close_rule}')\n",
    "        print(f'Manifest updated: {MANIFESTO_SILVER}')\n",
    "        print(f'Silver sha256: {silver_sha256}')\n",
    "        return manifest_row\n",
    "    else:\n",
    "        print('Dry run mode - no files written.')\n",
    "        print(f'Would write silver to: {silver_path}')\n",
    "        print(f'Rows after cleaning: {len(df_clean)} | Date range: {date_min} -> {date_max}')\n",
    "        print(f'Duplicates dropped: {duplicates_dropped}')\n",
    "        print(f'NaNs after cleaning: {nan_counts_json}')\n",
    "        print(f'Columns: {REQUIRED_COLS}')\n",
    "        print(f'AdjClose rule: {adj_close_rule}')\n",
    "        return { 'run_ts': run_ts, 'rows': int(len(df_clean)), 'date_min': date_min, 'date_max': date_max, 'duplicates_dropped': int(duplicates_dropped), 'nan_counts_json': nan_counts_json, 'adj_close_rule': adj_close_rule, 'silver_path': str(silver_path) }\n",
    "\n",
    "# Execute creation when running cell\n",
    "_result = create_silver(dry_run=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24968969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calib grid head (amostra):\n",
      "horizon    k  neutral_prop\n",
      "     d1 0.60      0.519869\n",
      "     d1 0.55      0.480131\n",
      "     d1 0.50      0.448414\n",
      "     d1 0.65      0.554138\n",
      "     d1 0.70      0.584761\n",
      "     d1 0.45      0.404666\n",
      "     d1 0.75      0.610645\n",
      "     d1 0.40      0.362012\n",
      "     d1 0.80      0.639081\n",
      "     d1 0.85      0.664601\n",
      "---\n",
      "horizon    k  neutral_prop\n",
      "     d3 0.80      0.405327\n",
      "     d3 0.85      0.429040\n",
      "     d3 0.75      0.383072\n",
      "     d3 0.90      0.447282\n",
      "     d3 0.70      0.359358\n",
      "     d3 0.95      0.473185\n",
      "     d3 1.00      0.495804\n",
      "     d3 0.65      0.332360\n",
      "     d3 1.05      0.516600\n",
      "     d3 0.60      0.311200\n",
      "---\n",
      "horizon    k  neutral_prop\n",
      "     d5 0.85      0.339540\n",
      "     d5 0.90      0.353414\n",
      "     d5 0.80      0.323476\n",
      "     d5 0.75      0.308507\n",
      "     d5 0.95      0.376050\n",
      "     d5 0.70      0.292808\n",
      "     d5 1.00      0.393939\n",
      "     d5 0.65      0.275283\n",
      "     d5 1.05      0.410004\n",
      "     d5 1.10      0.425703\n",
      "---\n",
      "Chosen ks: {\"d1\": 0.55, \"d3\": 0.75, \"d5\": 0.75}\n",
      "Chosen neutral proportions: {\"d1\": 48.01, \"d3\": 38.31, \"d5\": 30.85}\n",
      "Coverage after windows (preview): rows=2744, range=2014-08-26 -> 2025-09-17\n",
      "End of cell - dry run only (no files written)\n"
     ]
    }
   ],
   "source": [
    "# Instrução 03B — Calibração de k por horizonte (dry-run)\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import hashlib, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# CONFIG: silver input (validated) — do not modify paths here unless you know what you're doing\n",
    "ROOT = Path(\"/home/wrm/BOLSA_2026\")\n",
    "SILVER_PATH = ROOT / \"intermediarios\" / \"silver\" / \"run_20250917_114743\" / \"IBOV_silver.parquet\"\n",
    "SILVER_SHA256_EXPECTED = \"350216dfaa30971bb83a75e9f3364b25397b33aeebfa2eb7e80d805a2279b7ca\"\n",
    "SIGMA_WINDOW = 252\n",
    "HORIZONS = {\"d1\": 1, \"d3\": 3, \"d5\": 5}\n",
    "\n",
    "def compute_sha256(path: Path, chunk_size: int = 8192) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def load_silver(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Silver not found: {path}\")\n",
    "    df = pd.read_parquet(path)\n",
    "    # flatten multiindex cols if present and standardize names\n",
    "    cols = [c[0] if isinstance(c, tuple) else c for c in df.columns]\n",
    "    df.columns = [str(c).strip().lower() for c in cols]\n",
    "    # ensure expected cols\n",
    "    expected = [\"date\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"]\n",
    "    for c in expected:\n",
    "        if c not in df.columns:\n",
    "            raise KeyError(f\"Silver missing required column: {c}\")\n",
    "    df = df[expected].copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def compute_logret_and_sigma(df: pd.DataFrame, sigma_window: int) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"logret\"] = np.log(df[\"adj_close\"] / df[\"adj_close\"].shift(1))\n",
    "    df[\"sigma_rolling\"] = df[\"logret\"].rolling(window=sigma_window, min_periods=sigma_window).std()\n",
    "    return df\n",
    "\n",
    "def horizon_logret(df: pd.DataFrame, h: int) -> pd.Series:\n",
    "    return np.log(df[\"adj_close\"].shift(-h) / df[\"adj_close\"])\n",
    "\n",
    "def neutral_prop_for_k(df: pd.DataFrame, h: int, k: float) -> float:\n",
    "    ret = horizon_logret(df, h)\n",
    "    thresh = k * df[\"sigma_rolling\"]\n",
    "    mask = (~ret.isna()) & (~thresh.isna())\n",
    "    if mask.sum() == 0:\n",
    "        return float('nan')\n",
    "    selected_ret = ret[mask]\n",
    "    selected_th = thresh[mask]\n",
    "    neutral = ((selected_ret >= -selected_th) & (selected_ret <= selected_th)).sum()\n",
    "    return float(neutral) / float(mask.sum())\n",
    "\n",
    "# Grid of k values: 0.40 .. 1.20 step 0.05\n",
    "ks = [round(x, 2) for x in np.arange(0.40, 1.201, 0.05)]\n",
    "\n",
    "def calibrate_k_dryrun(silver_path: Path, ks: list, sigma_window: int, horizons: dict):\n",
    "    df = load_silver(silver_path)\n",
    "    df = compute_logret_and_sigma(df, sigma_window)\n",
    "\n",
    "    # coverage after windows\n",
    "    mask_cov = ~df[\"sigma_rolling\"].isna()\n",
    "    cov_df = df.loc[mask_cov].copy().reset_index(drop=True)\n",
    "    rows = len(cov_df)\n",
    "    date_min = cov_df[\"date\"].min().strftime(\"%Y-%m-%d\") if rows>0 else None\n",
    "    date_max = cov_df[\"date\"].max().strftime(\"%Y-%m-%d\") if rows>0 else None\n",
    "\n",
    "    results = []\n",
    "    for h_name, h in horizons.items():\n",
    "        for k in ks:\n",
    "            prop = neutral_prop_for_k(df, h, k)\n",
    "            results.append({\"horizon\": h_name, \"h\": h, \"k\": k, \"neutral_prop\": prop})\n",
    "\n",
    "    res_df = pd.DataFrame(results)\n",
    "    # drop nan neutral_prop rows\n",
    "    res_df = res_df.dropna(subset=[\"neutral_prop\"]).reset_index(drop=True)\n",
    "\n",
    "    # For reporting: find ks that first enter the acceptable band per horizon\n",
    "    target = {\"d1\": (0.45, 0.55), \"d3\": (0.38, 0.45), \"d5\": (0.30, 0.38)}\n",
    "    chosen = {}\n",
    "    chosen_props = {}\n",
    "    for h_name in horizons.keys():\n",
    "        band = target[h_name]\n",
    "        cand = res_df[res_df[\"horizon\"] == h_name].sort_values(\"k\").reset_index(drop=True)\n",
    "        sel = cand[(cand[\"neutral_prop\"] >= band[0]) & (cand[\"neutral_prop\"] <= band[1])]\n",
    "        if not sel.empty:\n",
    "            # pick smallest k that satisfies\n",
    "            pick = float(sel.iloc[0][\"k\"])\n",
    "            chosen[h_name] = pick\n",
    "            chosen_props[h_name] = float(sel.iloc[0][\"neutral_prop\"])\n",
    "        else:\n",
    "            chosen[h_name] = None\n",
    "            chosen_props[h_name] = None\n",
    "\n",
    "    # Print mandated outputs\n",
    "    print(\"Calib grid head (amostra):\")\n",
    "    # show for each horizon the 10 rows closest to the center of band\n",
    "    samples = []\n",
    "    for h_name in horizons.keys():\n",
    "        band = target[h_name]\n",
    "        cand = res_df[res_df[\"horizon\"] == h_name].copy()\n",
    "        cand['dist'] = cand['neutral_prop'].apply(lambda x: abs(x - ((band[0]+band[1])/2)))\n",
    "        cand = cand.sort_values('dist').head(10)[[\"horizon\", \"k\", \"neutral_prop\"]]\n",
    "        samples.append(cand)\n",
    "        print(cand.to_string(index=False))\n",
    "        print(\"---\")\n",
    "\n",
    "    print(\"Chosen ks: \" + json.dumps(chosen))\n",
    "    # format percents\n",
    "    chosen_perc = {k: (v if v is None else round(v*100, 2)) for k, v in chosen_props.items()}\n",
    "    print(\"Chosen neutral proportions: \" + json.dumps(chosen_perc))\n",
    "    print(f\"Coverage after windows (preview): rows={rows}, range={date_min} -> {date_max}\")\n",
    "\n",
    "    return res_df, chosen, chosen_props, {\"rows\": rows, \"date_min\": date_min, \"date_max\": date_max}\n",
    "\n",
    "# Run dry-run calibration and print outputs\n",
    "res_df, chosen, chosen_props, coverage = calibrate_k_dryrun(SILVER_PATH, ks, SIGMA_WINDOW, HORIZONS)\n",
    "\n",
    "# Final question to user as required by protocol\n",
    "print(\"End of cell - dry run only (no files written)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project: BOLSA_2026)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
