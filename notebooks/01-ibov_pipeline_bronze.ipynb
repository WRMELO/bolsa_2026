{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd9b860",
   "metadata": {},
   "source": [
    "# Pipeline Bronze: IBOV download (Jupyter)\n",
    "\n",
    "Este notebook baixa o histórico diário do IBOV (ticker `^BVSP`) cobrindo pelo menos 8 anos, salva um arquivo Parquet em `dados_originais/` e escreve um manifesto `manifesto_dados_originais_bronze_ibov.csv`.\n",
    "\n",
    "Siga a política do projeto: não sobrescrever arquivos existentes — o notebook criará novos arquivos quando necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f75298d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared paths and date range: 2013-08-18 -> 2025-09-17\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports e configurações iniciais\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "ROOT = Path('/home/wrm/BOLSA_2026')\n",
    "DADOS = ROOT / 'dados_originais'\n",
    "DADOS.mkdir(parents=True, exist_ok=True)\n",
    "TICKER = '^BVSP'\n",
    "MIN_YEARS = 12\n",
    "END = date.today()\n",
    "START = END - timedelta(days=int(MIN_YEARS * 365.25) + 30)  # buffer\n",
    "print('Prepared paths and date range:', START, '->', END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e812d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze target parquet: /home/wrm/BOLSA_2026/dados_originais/IBOV_2013-08-18_2025-09-17.parquet\n",
      "Manifest path: /home/wrm/BOLSA_2026/dados_originais/manifesto_dados_originais_bronze_ibov.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Check for existing bronze files and build output path (do not overwrite)\n",
    "out_parquet = DADOS / f'IBOV_{START.isoformat()}_{END.isoformat()}.parquet'\n",
    "manifest_csv = DADOS / 'manifesto_dados_originais_bronze_ibov.csv'\n",
    "print('Bronze target parquet:', out_parquet)\n",
    "print('Manifest path:', manifest_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b21e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver path: /home/wrm/BOLSA_2026/intermediarios/silver/run_20250917_140001/IBOV_silver.parquet\n",
      "Silver rows: 2996 | Date range: 2013-08-19 -> 2025-09-17\n",
      "Duplicates dropped: 0\n",
      "NaNs after cleaning: {\"date\": 0, \"open\": 0, \"high\": 0, \"low\": 0, \"close\": 0, \"adj_close\": 0, \"volume\": 0}\n",
      "Columns: ['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume']\n",
      "AdjClose rule: filled_from_close_all_rows\n",
      "Manifest updated: /home/wrm/BOLSA_2026/intermediarios/silver/manifesto_silver_ibov.csv\n",
      "Silver sha256: 350216dfaa30971bb83a75e9f3364b25397b33aeebfa2eb7e80d805a2279b7ca\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Create Silver from Bronze (Instrução 02)\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import hashlib, json\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path('/home/wrm/BOLSA_2026')\n",
    "BRONZE_PATH = ROOT / 'dados_originais' / 'IBOV_2013-08-18_2025-09-17.parquet'\n",
    "BRONZE_SHA256_EXPECTED = 'c18ab2ee0d4ffbc4e969bcfc79fa2c31445c739096ea75a1e3a4968b038eaafa'\n",
    "SILVER_ROOT = ROOT / 'intermediarios' / 'silver'\n",
    "MANIFESTO_SILVER = SILVER_ROOT / 'manifesto_silver_ibov.csv'\n",
    "REQUIRED_COLS = ['date','open','high','low','close','adj_close','volume']\n",
    "\n",
    "def compute_sha256(path: Path, chunk_size: int = 8192) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open('rb') as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def flatten_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = []\n",
    "    for c in df.columns:\n",
    "        if isinstance(c, tuple):\n",
    "            cols.append(c[0])\n",
    "        else:\n",
    "            cols.append(c)\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "def snake_standardize(col: str) -> str:\n",
    "    return col.strip().lower()\n",
    "\n",
    "def ensure_date_and_order(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if 'date' not in df.columns:\n",
    "        raise KeyError(\"Input data must contain a 'date' column\")\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date', ascending=True)\n",
    "    return df\n",
    "\n",
    "def create_silver(dry_run: bool = False):\n",
    "    run_ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    run_dir = SILVER_ROOT / f'run_{run_ts}'\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    silver_path = run_dir / 'IBOV_silver.parquet'\n",
    "\n",
    "    if not BRONZE_PATH.exists():\n",
    "        raise FileNotFoundError(f'Bronze file not found: {BRONZE_PATH}')\n",
    "\n",
    "    df = pd.read_parquet(BRONZE_PATH)\n",
    "    df = flatten_columns(df)\n",
    "    col_map = {c: snake_standardize(c) for c in df.columns}\n",
    "    df = df.rename(columns=col_map)\n",
    "\n",
    "    # Ensure required cols exist\n",
    "    for c in REQUIRED_COLS:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "\n",
    "    df = df[REQUIRED_COLS]\n",
    "    df = ensure_date_and_order(df)\n",
    "\n",
    "    # Cleaning rules\n",
    "    price_cols = ['open','high','low','close','adj_close']\n",
    "    mask_all_prices_nan = df[price_cols].isna().all(axis=1)\n",
    "    df_clean = df.loc[~mask_all_prices_nan].copy()\n",
    "    df_clean = df_clean.loc[~df_clean['volume'].isna()].copy()\n",
    "\n",
    "    n_before_dups = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates(subset=['date'], keep='last')\n",
    "    n_after_dups = len(df_clean)\n",
    "    duplicates_dropped = n_before_dups - n_after_dups\n",
    "\n",
    "    df_clean = df_clean.sort_values('date', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    total_rows = len(df_clean)\n",
    "    if total_rows == 0:\n",
    "        adj_close_rule = 'no_rows_after_cleaning'\n",
    "    else:\n",
    "        adj_nan_count = int(df_clean['adj_close'].isna().sum())\n",
    "        if adj_nan_count == total_rows:\n",
    "            df_clean['adj_close'] = df_clean['close']\n",
    "            adj_close_rule = 'filled_from_close_all_rows'\n",
    "        else:\n",
    "            adj_close_rule = 'as_is'\n",
    "\n",
    "    nan_counts = df_clean.isna().sum().to_dict()\n",
    "    nan_counts_json = json.dumps({k: int(v) for k, v in nan_counts.items()})\n",
    "\n",
    "    final_cols = list(df_clean.columns)\n",
    "    if final_cols != REQUIRED_COLS:\n",
    "        df_clean = df_clean.reindex(columns=REQUIRED_COLS)\n",
    "        final_cols = list(df_clean.columns)\n",
    "        if final_cols != REQUIRED_COLS:\n",
    "            raise AssertionError(f'Final schema mismatch. Expected {REQUIRED_COLS}, got {final_cols}')\n",
    "\n",
    "    if len(df_clean) > 0:\n",
    "        date_min = df_clean['date'].min().strftime('%Y-%m-%d')\n",
    "        date_max = df_clean['date'].max().strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        date_min = None\n",
    "        date_max = None\n",
    "\n",
    "    if not dry_run:\n",
    "        df_clean.to_parquet(silver_path, index=False, compression='snappy')\n",
    "        silver_sha256 = compute_sha256(silver_path)\n",
    "\n",
    "        manifest_row = {\n",
    "            'run_ts': run_ts,\n",
    "            'source_parquet_path': str(BRONZE_PATH),\n",
    "            'source_sha256': BRONZE_SHA256_EXPECTED,\n",
    "            'rows': int(len(df_clean)),\n",
    "            'date_min': date_min,\n",
    "            'date_max': date_max,\n",
    "            'duplicates_dropped': int(duplicates_dropped),\n",
    "            'nan_counts_json': nan_counts_json,\n",
    "            'adj_close_rule': adj_close_rule,\n",
    "            'columns_json': json.dumps(REQUIRED_COLS),\n",
    "            'silver_parquet_path': str(silver_path),\n",
    "            'silver_sha256': silver_sha256\n",
    "        }\n",
    "\n",
    "        SILVER_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "        write_header = not MANIFESTO_SILVER.exists()\n",
    "        with MANIFESTO_SILVER.open('a', encoding='utf-8') as f:\n",
    "            if write_header:\n",
    "                f.write(','.join(list(manifest_row.keys())) + '\\n')\n",
    "            row_values = [str(manifest_row[h]) for h in list(manifest_row.keys())]\n",
    "            f.write(','.join(row_values) + '\\n')\n",
    "\n",
    "        # Prints required by spec (all properly-terminated strings)\n",
    "        print(f'Silver path: {silver_path}')\n",
    "        print(f'Silver rows: {len(df_clean)} | Date range: {date_min} -> {date_max}')\n",
    "        print(f'Duplicates dropped: {duplicates_dropped}')\n",
    "        print(f'NaNs after cleaning: {nan_counts_json}')\n",
    "        print(f'Columns: {REQUIRED_COLS}')\n",
    "        print(f'AdjClose rule: {adj_close_rule}')\n",
    "        print(f'Manifest updated: {MANIFESTO_SILVER}')\n",
    "        print(f'Silver sha256: {silver_sha256}')\n",
    "        return manifest_row\n",
    "    else:\n",
    "        print('Dry run mode - no files written.')\n",
    "        print(f'Would write silver to: {silver_path}')\n",
    "        print(f'Rows after cleaning: {len(df_clean)} | Date range: {date_min} -> {date_max}')\n",
    "        print(f'Duplicates dropped: {duplicates_dropped}')\n",
    "        print(f'NaNs after cleaning: {nan_counts_json}')\n",
    "        print(f'Columns: {REQUIRED_COLS}')\n",
    "        print(f'AdjClose rule: {adj_close_rule}')\n",
    "        return { 'run_ts': run_ts, 'rows': int(len(df_clean)), 'date_min': date_min, 'date_max': date_max, 'duplicates_dropped': int(duplicates_dropped), 'nan_counts_json': nan_counts_json, 'adj_close_rule': adj_close_rule, 'silver_path': str(silver_path) }\n",
    "\n",
    "# Execute creation when running cell\n",
    "_result = create_silver(dry_run=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project: BOLSA_2026)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
