{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aada492",
   "metadata": {},
   "source": [
    "---\n",
    "# MODELOS E MODELAGEM\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ade437",
   "metadata": {},
   "source": [
    "## Comparativo XGBoost vs. LSTM no IBOV (GOLD/SILVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba887a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-19 16:02:43] Início — Comparativo XGBoost vs. LSTM (IBOV SSOT)\n",
      "\n",
      "[PROVA DE LEITURA]\n",
      "- Caminho efetivo usado: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Schema (primeiras colunas): ['date', 'open', 'high', 'low', 'close', 'volume', 'ticker', 'open_norm', 'high_norm', 'low_norm', 'close_norm', 'volume_norm']\n",
      "- Contagem de linhas: 3400, colunas: 25\n",
      "- date_min: 2012-01-03, date_max: 2025-09-19\n",
      "- Amostra (head 5):\n",
      "               date     open     high      low    close  volume ticker  open_norm  high_norm  low_norm  close_norm  volume_norm  return_1d  volatility_5d   sma_5  sma_20  sma_ratio      y_h1      y_h3      y_h5  y_h1_cls  y_h3_cls  y_h5_cls year            __date__\n",
      "2012-01-03 00:00:00  57836.0  59288.0  57836.0  59265.0 3083000  ^BVSP   0.188125   0.196279  0.191702    0.200510     0.875060   0.001687            NaN     NaN     NaN        NaN  0.001687 -0.011221  0.009128       NaN       NaN       NaN 2012 2012-01-03 00:00:00\n",
      "2012-01-04 00:00:00  59263.0  59519.0  58558.0  59365.0 2252000  ^BVSP   0.201327   0.198412  0.198360    0.201431     0.856665  -0.013796            NaN     NaN     NaN        NaN -0.013796  -0.00475  0.010056       NaN       NaN       NaN 2012 2012-01-04 00:00:00\n",
      "2012-01-05 00:00:00  59354.0  59354.0  57963.0  58546.0 2351200  ^BVSP   0.202169   0.196888  0.192873    0.193887     0.859190   0.000922            NaN     NaN     NaN        NaN  0.000922  0.021522  0.023486       NaN       NaN       NaN 2012 2012-01-05 00:00:00\n",
      "2012-01-06 00:00:00  58565.0  59261.0  58355.0  58600.0 1659200  ^BVSP   0.194869   0.196030  0.196488    0.194384     0.838774   0.008242            NaN     NaN     NaN        NaN  0.008242  0.023242  0.009334       NaN       NaN       NaN 2012 2012-01-06 00:00:00\n",
      "2012-01-09 00:00:00  58601.0  59220.0  58599.0  59083.0 2244600  ^BVSP   0.195202   0.195651  0.198738    0.198833     0.856472   0.012237            NaN 58971.8     NaN        NaN  0.012237  0.014183  0.014776       NaN       NaN       NaN 2012 2012-01-09 00:00:00\n",
      "\n",
      "[RÓTULOS — DETECÇÃO/GERAÇÃO]\n",
      "- h=1: rótulos GERADOS -> dir_fwd_1 (binário a partir de ret_fwd_1).\n",
      "- h=3: rótulos GERADOS -> dir_fwd_3 (binário a partir de ret_fwd_3).\n",
      "- h=5: rótulos GERADOS -> dir_fwd_5 (binário a partir de ret_fwd_5).\n",
      "- Resumo: {\n",
      "  \"1\": {\n",
      "    \"type\": \"classification\",\n",
      "    \"col\": \"dir_fwd_1\",\n",
      "    \"origem\": \"gerado\"\n",
      "  },\n",
      "  \"3\": {\n",
      "    \"type\": \"classification\",\n",
      "    \"col\": \"dir_fwd_3\",\n",
      "    \"origem\": \"gerado\"\n",
      "  },\n",
      "  \"5\": {\n",
      "    \"type\": \"classification\",\n",
      "    \"col\": \"dir_fwd_5\",\n",
      "    \"origem\": \"gerado\"\n",
      "  }\n",
      "}\n",
      "\n",
      "[WALK-FORWARD — Folds explícitos]\n",
      "Fold 01: train[2012-01-03 → 2013-04-02], val[2013-04-03 → 2013-07-02], test[2013-07-03 → 2014-01-02]\n",
      "Fold 02: train[2012-01-03 → 2013-10-02], val[2013-10-03 → 2014-01-02], test[2014-01-03 → 2014-07-02]\n",
      "Fold 03: train[2012-01-03 → 2014-04-02], val[2014-04-03 → 2014-07-02], test[2014-07-03 → 2015-01-02]\n",
      "Fold 04: train[2012-01-03 → 2014-10-02], val[2014-10-03 → 2015-01-02], test[2015-01-03 → 2015-07-02]\n",
      "Fold 05: train[2012-01-03 → 2015-04-02], val[2015-04-03 → 2015-07-02], test[2015-07-03 → 2016-01-02]\n",
      "Fold 06: train[2012-01-03 → 2015-10-02], val[2015-10-03 → 2016-01-02], test[2016-01-03 → 2016-07-02]\n",
      "Fold 07: train[2012-01-03 → 2016-04-02], val[2016-04-03 → 2016-07-02], test[2016-07-03 → 2017-01-02]\n",
      "Fold 08: train[2012-01-03 → 2016-10-02], val[2016-10-03 → 2017-01-02], test[2017-01-03 → 2017-07-02]\n",
      "Fold 09: train[2012-01-03 → 2017-04-02], val[2017-04-03 → 2017-07-02], test[2017-07-03 → 2018-01-02]\n",
      "Fold 10: train[2012-01-03 → 2017-10-02], val[2017-10-03 → 2018-01-02], test[2018-01-03 → 2018-07-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 16:02:43.437084: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x767b9018a2a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x767b885b68e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x767b885b68e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "[FEATURES POR JANELA — XGBoost]\n",
      "- Para cada janela (5/10/15): lags ret1 (1..min(janela,10)), ret1_roll_mean_janela, ret1_roll_std_janela, ret1_z_janela.\n",
      "[SEQUÊNCIAS — LSTM]\n",
      "- Features por passo: ['ret1','roll_mean_ret_5','roll_std_ret_5'] (padronizadas no treino).\n",
      "- Shape por janela: [amostras, janela, 3].\n",
      "\n",
      "[HIPERPARÂMETROS FINAIS — XGBoost]\n",
      "- h=1, w=5: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 0}\n",
      "- h=1, w=10: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 3}\n",
      "- h=1, w=15: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 11}\n",
      "- h=3, w=5: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 49}\n",
      "- h=3, w=10: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 39}\n",
      "- h=3, w=15: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 19}\n",
      "- h=5, w=5: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 24}\n",
      "- h=5, w=10: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 0}\n",
      "- h=5, w=15: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 14}\n",
      "\n",
      "[HIPERPARÂMETROS FINAIS — LSTM]\n",
      "- h=1, w=5: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=1, w=10: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=1, w=15: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=3, w=5: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=3, w=10: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=3, w=15: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=5, w=5: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=5, w=10: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=5, w=15: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "\n",
      "[MÉTRICAS DE PREVISÃO — por fold (head)]\n",
      "model  horizon  window  fold split      AUC      ACC       F1\n",
      " LSTM        1       5     1  test 0.464336 0.458333 0.000000\n",
      " LSTM        1       5     1   val 0.567308 0.620690 0.388889\n",
      " LSTM        1       5     2  test 0.533918 0.495726 0.213333\n",
      " LSTM        1       5     2   val 0.579576 0.490909 0.363636\n",
      " LSTM        1       5     3  test 0.503052 0.570248 0.446809\n",
      " LSTM        1       5     3   val 0.522487 0.490909 0.176471\n",
      " LSTM        1       5     4  test 0.425079 0.474576 0.261905\n",
      " LSTM        1       5     4   val 0.594920 0.607143 0.083333\n",
      " LSTM        1       5     5  test 0.505102 0.572650 0.404762\n",
      " LSTM        1       5     5   val 0.464103 0.517857 0.228571\n",
      " LSTM        1       5     6  test 0.531746 0.470588 0.000000\n",
      " LSTM        1       5     6   val 0.444282 0.584906 0.000000\n",
      "\n",
      "[MÉTRICAS DE PREVISÃO — agregadas (média ± desvio)]\n",
      " index  model_  horizon_  window_ split_  AUC_mean  AUC_std  ACC_mean  ACC_std  F1_mean   F1_std\n",
      "     0    LSTM         1        5   test  0.488223 0.043827  0.488272 0.048977 0.261136 0.168586\n",
      "     1    LSTM         1        5    val  0.544557 0.054187  0.545792 0.075655 0.293779 0.210278\n",
      "     2    LSTM         1       10   test  0.492590 0.066362  0.472067 0.049217 0.255162 0.169134\n",
      "     3    LSTM         1       10    val  0.501288 0.105661  0.515084 0.063081 0.312689 0.209919\n",
      "     4    LSTM         1       15   test  0.483143 0.055106  0.480707 0.057673 0.237450 0.166094\n",
      "     5    LSTM         1       15    val  0.548013 0.081382  0.540045 0.030370 0.284865 0.303193\n",
      "     6    LSTM         3        5   test  0.449334 0.070661  0.463279 0.062498 0.410681 0.124260\n",
      "     7    LSTM         3        5    val  0.542334 0.132778  0.586821 0.104480 0.426512 0.227896\n",
      "     8    LSTM         3       10   test  0.462831 0.070790  0.461741 0.064182 0.404547 0.113424\n",
      "     9    LSTM         3       10    val  0.515820 0.117288  0.525832 0.090396 0.425606 0.172639\n",
      "    10    LSTM         3       15   test  0.478019 0.070818  0.468194 0.066243 0.353669 0.130245\n",
      "    11    LSTM         3       15    val  0.561525 0.106453  0.540175 0.104416 0.447195 0.185396\n",
      "    12    LSTM         5        5   test  0.433208 0.115080  0.450419 0.083327 0.290666 0.153055\n",
      "    13    LSTM         5        5    val  0.478129 0.166023  0.532883 0.072682 0.329990 0.197989\n",
      "    14    LSTM         5       10   test  0.446017 0.062343  0.447438 0.073772 0.315478 0.144599\n",
      "    15    LSTM         5       10    val  0.571900 0.150304  0.552772 0.130372 0.413334 0.213964\n",
      "    16    LSTM         5       15   test  0.451148 0.108502  0.466283 0.096118 0.324064 0.164404\n",
      "    17    LSTM         5       15    val  0.524634 0.194325  0.519618 0.151165 0.374508 0.225859\n",
      "    18 XGBoost         1        5   test  0.504486 0.038402  0.495479 0.055053 0.165508 0.178010\n",
      "    19 XGBoost         1        5    val  0.557692 0.050491  0.538568 0.049063 0.201300 0.254404\n",
      "    20 XGBoost         1       10   test  0.472114 0.067022  0.487499 0.044533 0.183980 0.202704\n",
      "    21 XGBoost         1       10    val  0.510876 0.071326  0.518820 0.057505 0.198850 0.208346\n",
      "    22 XGBoost         1       15   test  0.478388 0.055352  0.473780 0.046589 0.227860 0.179514\n",
      "    23 XGBoost         1       15    val  0.551727 0.054206  0.543932 0.056339 0.278612 0.236004\n",
      "    24 XGBoost         3        5   test  0.464241 0.048676  0.482937 0.059939 0.244481 0.237590\n",
      "    25 XGBoost         3        5    val  0.521179 0.068067  0.531748 0.045312 0.269092 0.270597\n",
      "    26 XGBoost         3       10   test  0.485478 0.053755  0.490178 0.049361 0.240431 0.233156\n",
      "    27 XGBoost         3       10    val  0.559307 0.089335  0.538467 0.042966 0.273964 0.298042\n",
      "    28 XGBoost         3       15   test  0.495324 0.043757  0.493668 0.049059 0.322051 0.187128\n",
      "    29 XGBoost         3       15    val  0.532555 0.060945  0.528996 0.086612 0.356096 0.179596\n",
      "    30 XGBoost         5        5   test  0.460228 0.037254  0.456023 0.070968 0.230776 0.213934\n",
      "    31 XGBoost         5        5    val  0.527272 0.113282  0.559380 0.097652 0.272576 0.272773\n",
      "    32 XGBoost         5       10   test  0.485113 0.085844  0.475991 0.058090 0.308628 0.221348\n",
      "    33 XGBoost         5       10    val  0.535567 0.078830  0.576493 0.078624 0.355982 0.273618\n",
      "    34 XGBoost         5       15   test  0.477573 0.081994  0.465428 0.041960 0.280048 0.200180\n",
      "    35 XGBoost         5       15    val  0.578048 0.080052  0.589587 0.067900 0.323046 0.248619\n",
      "\n",
      "[MÉTRICAS OPERACIONAIS — por fold (head)]\n",
      "model  horizon  window  fold split  ann_return    sharpe     maxdd  hit_rate  turnover  threshold\n",
      " LSTM        1       5     1  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     1   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     3  test   -0.057036 -1.496348 -0.027805       0.0  0.016529       0.55\n",
      " LSTM        1       5     3   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     6  test    0.000000  0.000000  0.000000       NaN  0.000000       0.50\n",
      " LSTM        1       5     6   val    0.000000  0.000000  0.000000       NaN  0.000000       0.50\n",
      "\n",
      "[MÉTRICAS OPERACIONAIS — agregadas (média ± desvio)]\n",
      " index  model_  horizon_  window_ split_  ann_return_mean  ann_return_std  sharpe_mean  sharpe_std  maxdd_mean  maxdd_std  hit_rate_mean  hit_rate_std  turnover_mean  turnover_std\n",
      "     0    LSTM         1        5   test        -0.003027        0.091151     0.225715    1.346111   -0.024146   0.061215       0.526615      0.399913       0.028660      0.063402\n",
      "     1    LSTM         1        5    val         0.070002        0.129207     0.744401    1.215467   -0.007222   0.020768       0.750000      0.217945       0.023007      0.040754\n",
      "     2    LSTM         1       10   test        -0.052566        0.117465    -0.191486    1.456236   -0.054232   0.077067       0.548154      0.249162       0.053797      0.062442\n",
      "     3    LSTM         1       10    val         0.085097        0.125845     1.079842    1.467075   -0.012819   0.023398       0.704196      0.284354       0.049010      0.080506\n",
      "     4    LSTM         1       15   test        -0.061953        0.152830    -0.660858    1.507044   -0.058475   0.071403       0.428732      0.128119       0.070555      0.067590\n",
      "     5    LSTM         1       15    val         0.125394        0.162109     1.054177    1.471284   -0.022531   0.035565       0.653574      0.214809       0.078168      0.110712\n",
      "     6    LSTM         3        5   test        -0.178120        0.292005    -0.819346    2.164915   -0.170001   0.179887       0.592194      0.307466       0.079613      0.079466\n",
      "     7    LSTM         3        5    val         0.315657        0.397429     1.940414    2.074507   -0.006990   0.011521       0.833333      0.210819       0.061919      0.067490\n",
      "     8    LSTM         3       10   test        -0.145370        0.310629    -0.443880    1.907535   -0.137836   0.183531       0.558163      0.245546       0.049320      0.047195\n",
      "     9    LSTM         3       10    val         0.159119        0.190843     1.502888    1.791129   -0.028818   0.056087       0.714286      0.269179       0.059868      0.054211\n",
      "    10    LSTM         3       15   test        -0.149626        0.309227    -0.574150    1.727750   -0.099880   0.175151       0.609009      0.358610       0.025944      0.047443\n",
      "    11    LSTM         3       15    val         0.175460        0.291176     1.832947    2.068786   -0.020787   0.055445       0.773016      0.241212       0.049688      0.058680\n",
      "    12    LSTM         5        5   test        -0.213561        0.343251    -1.247387    2.372877   -0.216542   0.268949       0.377522      0.165305       0.068072      0.072425\n",
      "    13    LSTM         5        5    val         0.368979        0.685186     1.137460    1.451468   -0.058139   0.083816       0.628553      0.061456       0.085907      0.098111\n",
      "    14    LSTM         5       10   test        -0.215652        0.377184    -0.982573    1.892618   -0.200392   0.260919       0.470381      0.276501       0.060871      0.086279\n",
      "    15    LSTM         5       10    val         0.976065        2.055379     1.970745    2.604320   -0.039279   0.082252       0.707872      0.155776       0.058441      0.074760\n",
      "    16    LSTM         5       15   test        -0.160753        0.372924    -0.304050    1.687083   -0.140997   0.226279       0.566396      0.273334       0.047296      0.083604\n",
      "    17    LSTM         5       15    val         0.218740        0.293511     1.458380    1.650981   -0.023093   0.060286       0.822222      0.201843       0.048182      0.080067\n",
      "    18 XGBoost         1        5   test         0.018918        0.031453     0.527518    0.788213   -0.003010   0.007511       0.777778      0.272166       0.024446      0.046466\n",
      "    19 XGBoost         1        5    val         0.080032        0.137482     1.028267    1.538164   -0.000890   0.001821       0.942308      0.115385       0.056824      0.117716\n",
      "    20 XGBoost         1       10   test        -0.010921        0.023638    -0.496660    0.910023   -0.012976   0.018988       0.242647      0.280442       0.054504      0.098950\n",
      "    21 XGBoost         1       10    val         0.019129        0.038614     0.589120    0.968738   -0.005699   0.013187       0.875000      0.150000       0.051690      0.088196\n",
      "    22 XGBoost         1       15   test        -0.038020        0.093462    -0.366262    1.092469   -0.049828   0.060928       0.427737      0.215801       0.191675      0.183808\n",
      "    23 XGBoost         1       15    val         0.069607        0.109663     0.959873    1.278550   -0.014481   0.020869       0.749936      0.144879       0.184556      0.213946\n",
      "    24 XGBoost         3        5   test         0.125941        0.562874     0.085097    1.740869   -0.071667   0.094005       0.439130      0.268147       0.089645      0.117512\n",
      "    25 XGBoost         3        5    val         0.101237        0.178808     0.678943    0.887244   -0.042272   0.071782       0.729234      0.172994       0.102053      0.126801\n",
      "    26 XGBoost         3       10   test        -0.044623        0.147106    -0.194789    1.029890   -0.042262   0.091881       0.541497      0.159463       0.072505      0.149528\n",
      "    27 XGBoost         3       10    val         0.244730        0.508918     0.762548    1.357996   -0.022871   0.050154       0.751151      0.066301       0.064693      0.110977\n",
      "    28 XGBoost         3       15   test        -0.141426        0.245171    -0.431367    1.274429   -0.115474   0.189473       0.583333      0.280610       0.105281      0.165643\n",
      "    29 XGBoost         3       15    val         0.047049        0.080729     0.449730    0.729196   -0.027110   0.060412       0.718266      0.191022       0.094240      0.168249\n",
      "    30 XGBoost         5        5   test        -0.049515        0.418860    -0.424079    1.655957   -0.112953   0.175841       0.457983      0.208526       0.094339      0.124354\n",
      "    31 XGBoost         5        5    val         0.152369        0.244055     1.230116    1.566050   -0.014802   0.033408       0.813043      0.257037       0.086071      0.106540\n",
      "    32 XGBoost         5       10   test         0.004661        0.752246    -0.262417    3.023806   -0.247114   0.253184       0.556928      0.272461       0.184613      0.154303\n",
      "    33 XGBoost         5       10    val         0.388340        0.612179     1.070197    0.916977   -0.098081   0.114841       0.622839      0.181584       0.182250      0.178656\n",
      "    34 XGBoost         5       15   test        -0.212999        0.412638    -1.089272    2.160996   -0.232079   0.263365       0.435168      0.152832       0.182567      0.175907\n",
      "    35 XGBoost         5       15    val         0.424421        0.806671     1.020858    1.334275   -0.044809   0.062724       0.676188      0.062122       0.152215      0.185023\n",
      "VALIDATION_ERROR: falha ao selecionar vencedor: Invalid format specifier '.1% if not pd.isna(top['hit_rate']) else float('nan')' for object of type 'float'\n",
      "\n",
      "[CHECKLIST OBRIGATÓRIO — dry_run]\n",
      "- SSOT usado: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Labels D+1/D+3/D+5: OK\n",
      "- Walk-forward folds: 10\n",
      "- Métricas previsão — linhas: 360\n",
      "- Métricas operacionais — linhas: 360\n",
      "- Vencedor destacado: FALHA\n",
      "- Persistência: DESLIGADA (dry_run=True)\n",
      "CHECKLIST_FAILURE: algum item obrigatório não foi atendido. Revise os logs acima.\n",
      "\n",
      "[RELATÓRIO FINAL — Estrutura]\n",
      "- pred_df.info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 360 entries, 3 to 356\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   model    360 non-null    object \n",
      " 1   horizon  360 non-null    int64  \n",
      " 2   window   360 non-null    int64  \n",
      " 3   fold     360 non-null    int64  \n",
      " 4   split    360 non-null    object \n",
      " 5   AUC      360 non-null    float64\n",
      " 6   ACC      360 non-null    float64\n",
      " 7   F1       360 non-null    float64\n",
      "dtypes: float64(3), int64(3), object(2)\n",
      "memory usage: 25.3+ KB\n",
      "None\n",
      "- op_df.info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 360 entries, 3 to 356\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   model       360 non-null    object \n",
      " 1   horizon     360 non-null    int64  \n",
      " 2   window      360 non-null    int64  \n",
      " 3   fold        360 non-null    int64  \n",
      " 4   split       360 non-null    object \n",
      " 5   ann_return  360 non-null    float64\n",
      " 6   sharpe      360 non-null    float64\n",
      " 7   maxdd       360 non-null    float64\n",
      " 8   hit_rate    193 non-null    float64\n",
      " 9   turnover    360 non-null    float64\n",
      " 10  threshold   360 non-null    float64\n",
      "dtypes: float64(6), int64(3), object(2)\n",
      "memory usage: 33.8+ KB\n",
      "None\n",
      "\n",
      "[Amostras iniciais — pred_df]\n",
      "model  horizon  window  fold split      AUC      ACC       F1\n",
      " LSTM        1       5     1  test 0.464336 0.458333 0.000000\n",
      " LSTM        1       5     1   val 0.567308 0.620690 0.388889\n",
      " LSTM        1       5     2  test 0.533918 0.495726 0.213333\n",
      " LSTM        1       5     2   val 0.579576 0.490909 0.363636\n",
      " LSTM        1       5     3  test 0.503052 0.570248 0.446809\n",
      " LSTM        1       5     3   val 0.522487 0.490909 0.176471\n",
      " LSTM        1       5     4  test 0.425079 0.474576 0.261905\n",
      " LSTM        1       5     4   val 0.594920 0.607143 0.083333\n",
      " LSTM        1       5     5  test 0.505102 0.572650 0.404762\n",
      " LSTM        1       5     5   val 0.464103 0.517857 0.228571\n",
      "\n",
      "[Amostras iniciais — op_df]\n",
      "model  horizon  window  fold split  ann_return    sharpe     maxdd  hit_rate  turnover  threshold\n",
      " LSTM        1       5     1  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     1   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     3  test   -0.057036 -1.496348 -0.027805       0.0  0.016529       0.55\n",
      " LSTM        1       5     3   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      "\n",
      "[Intervalos temporais cobertos]\n",
      "- Dataset: 2012-01-03 → 2025-09-19\n",
      "- Folds: 10 (test_months=6, val_months=3, treino mínimo=18)\n",
      "\n",
      "[Contagens totais]\n",
      "- pred_df: 360 linhas\n",
      "- op_df: 360 linhas\n",
      "\n",
      "[2025-09-19 16:06:15] Fim — Comparativo (dry_run=True, persist=False)\n",
      "\n",
      "[FEATURES POR JANELA — XGBoost]\n",
      "- Para cada janela (5/10/15): lags ret1 (1..min(janela,10)), ret1_roll_mean_janela, ret1_roll_std_janela, ret1_z_janela.\n",
      "[SEQUÊNCIAS — LSTM]\n",
      "- Features por passo: ['ret1','roll_mean_ret_5','roll_std_ret_5'] (padronizadas no treino).\n",
      "- Shape por janela: [amostras, janela, 3].\n",
      "\n",
      "[HIPERPARÂMETROS FINAIS — XGBoost]\n",
      "- h=1, w=5: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 0}\n",
      "- h=1, w=10: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 3}\n",
      "- h=1, w=15: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 11}\n",
      "- h=3, w=5: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 49}\n",
      "- h=3, w=10: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 39}\n",
      "- h=3, w=15: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 19}\n",
      "- h=5, w=5: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 24}\n",
      "- h=5, w=10: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 0}\n",
      "- h=5, w=15: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 14}\n",
      "\n",
      "[HIPERPARÂMETROS FINAIS — LSTM]\n",
      "- h=1, w=5: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=1, w=10: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=1, w=15: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=3, w=5: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=3, w=10: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=3, w=15: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=5, w=5: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=5, w=10: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=5, w=15: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "\n",
      "[MÉTRICAS DE PREVISÃO — por fold (head)]\n",
      "model  horizon  window  fold split      AUC      ACC       F1\n",
      " LSTM        1       5     1  test 0.464336 0.458333 0.000000\n",
      " LSTM        1       5     1   val 0.567308 0.620690 0.388889\n",
      " LSTM        1       5     2  test 0.533918 0.495726 0.213333\n",
      " LSTM        1       5     2   val 0.579576 0.490909 0.363636\n",
      " LSTM        1       5     3  test 0.503052 0.570248 0.446809\n",
      " LSTM        1       5     3   val 0.522487 0.490909 0.176471\n",
      " LSTM        1       5     4  test 0.425079 0.474576 0.261905\n",
      " LSTM        1       5     4   val 0.594920 0.607143 0.083333\n",
      " LSTM        1       5     5  test 0.505102 0.572650 0.404762\n",
      " LSTM        1       5     5   val 0.464103 0.517857 0.228571\n",
      " LSTM        1       5     6  test 0.531746 0.470588 0.000000\n",
      " LSTM        1       5     6   val 0.444282 0.584906 0.000000\n",
      "\n",
      "[MÉTRICAS DE PREVISÃO — agregadas (média ± desvio)]\n",
      " index  model_  horizon_  window_ split_  AUC_mean  AUC_std  ACC_mean  ACC_std  F1_mean   F1_std\n",
      "     0    LSTM         1        5   test  0.488223 0.043827  0.488272 0.048977 0.261136 0.168586\n",
      "     1    LSTM         1        5    val  0.544557 0.054187  0.545792 0.075655 0.293779 0.210278\n",
      "     2    LSTM         1       10   test  0.492590 0.066362  0.472067 0.049217 0.255162 0.169134\n",
      "     3    LSTM         1       10    val  0.501288 0.105661  0.515084 0.063081 0.312689 0.209919\n",
      "     4    LSTM         1       15   test  0.483143 0.055106  0.480707 0.057673 0.237450 0.166094\n",
      "     5    LSTM         1       15    val  0.548013 0.081382  0.540045 0.030370 0.284865 0.303193\n",
      "     6    LSTM         3        5   test  0.449334 0.070661  0.463279 0.062498 0.410681 0.124260\n",
      "     7    LSTM         3        5    val  0.542334 0.132778  0.586821 0.104480 0.426512 0.227896\n",
      "     8    LSTM         3       10   test  0.462831 0.070790  0.461741 0.064182 0.404547 0.113424\n",
      "     9    LSTM         3       10    val  0.515820 0.117288  0.525832 0.090396 0.425606 0.172639\n",
      "    10    LSTM         3       15   test  0.478019 0.070818  0.468194 0.066243 0.353669 0.130245\n",
      "    11    LSTM         3       15    val  0.561525 0.106453  0.540175 0.104416 0.447195 0.185396\n",
      "    12    LSTM         5        5   test  0.433208 0.115080  0.450419 0.083327 0.290666 0.153055\n",
      "    13    LSTM         5        5    val  0.478129 0.166023  0.532883 0.072682 0.329990 0.197989\n",
      "    14    LSTM         5       10   test  0.446017 0.062343  0.447438 0.073772 0.315478 0.144599\n",
      "    15    LSTM         5       10    val  0.571900 0.150304  0.552772 0.130372 0.413334 0.213964\n",
      "    16    LSTM         5       15   test  0.451148 0.108502  0.466283 0.096118 0.324064 0.164404\n",
      "    17    LSTM         5       15    val  0.524634 0.194325  0.519618 0.151165 0.374508 0.225859\n",
      "    18 XGBoost         1        5   test  0.504486 0.038402  0.495479 0.055053 0.165508 0.178010\n",
      "    19 XGBoost         1        5    val  0.557692 0.050491  0.538568 0.049063 0.201300 0.254404\n",
      "    20 XGBoost         1       10   test  0.472114 0.067022  0.487499 0.044533 0.183980 0.202704\n",
      "    21 XGBoost         1       10    val  0.510876 0.071326  0.518820 0.057505 0.198850 0.208346\n",
      "    22 XGBoost         1       15   test  0.478388 0.055352  0.473780 0.046589 0.227860 0.179514\n",
      "    23 XGBoost         1       15    val  0.551727 0.054206  0.543932 0.056339 0.278612 0.236004\n",
      "    24 XGBoost         3        5   test  0.464241 0.048676  0.482937 0.059939 0.244481 0.237590\n",
      "    25 XGBoost         3        5    val  0.521179 0.068067  0.531748 0.045312 0.269092 0.270597\n",
      "    26 XGBoost         3       10   test  0.485478 0.053755  0.490178 0.049361 0.240431 0.233156\n",
      "    27 XGBoost         3       10    val  0.559307 0.089335  0.538467 0.042966 0.273964 0.298042\n",
      "    28 XGBoost         3       15   test  0.495324 0.043757  0.493668 0.049059 0.322051 0.187128\n",
      "    29 XGBoost         3       15    val  0.532555 0.060945  0.528996 0.086612 0.356096 0.179596\n",
      "    30 XGBoost         5        5   test  0.460228 0.037254  0.456023 0.070968 0.230776 0.213934\n",
      "    31 XGBoost         5        5    val  0.527272 0.113282  0.559380 0.097652 0.272576 0.272773\n",
      "    32 XGBoost         5       10   test  0.485113 0.085844  0.475991 0.058090 0.308628 0.221348\n",
      "    33 XGBoost         5       10    val  0.535567 0.078830  0.576493 0.078624 0.355982 0.273618\n",
      "    34 XGBoost         5       15   test  0.477573 0.081994  0.465428 0.041960 0.280048 0.200180\n",
      "    35 XGBoost         5       15    val  0.578048 0.080052  0.589587 0.067900 0.323046 0.248619\n",
      "\n",
      "[MÉTRICAS OPERACIONAIS — por fold (head)]\n",
      "model  horizon  window  fold split  ann_return    sharpe     maxdd  hit_rate  turnover  threshold\n",
      " LSTM        1       5     1  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     1   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     3  test   -0.057036 -1.496348 -0.027805       0.0  0.016529       0.55\n",
      " LSTM        1       5     3   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     6  test    0.000000  0.000000  0.000000       NaN  0.000000       0.50\n",
      " LSTM        1       5     6   val    0.000000  0.000000  0.000000       NaN  0.000000       0.50\n",
      "\n",
      "[MÉTRICAS OPERACIONAIS — agregadas (média ± desvio)]\n",
      " index  model_  horizon_  window_ split_  ann_return_mean  ann_return_std  sharpe_mean  sharpe_std  maxdd_mean  maxdd_std  hit_rate_mean  hit_rate_std  turnover_mean  turnover_std\n",
      "     0    LSTM         1        5   test        -0.003027        0.091151     0.225715    1.346111   -0.024146   0.061215       0.526615      0.399913       0.028660      0.063402\n",
      "     1    LSTM         1        5    val         0.070002        0.129207     0.744401    1.215467   -0.007222   0.020768       0.750000      0.217945       0.023007      0.040754\n",
      "     2    LSTM         1       10   test        -0.052566        0.117465    -0.191486    1.456236   -0.054232   0.077067       0.548154      0.249162       0.053797      0.062442\n",
      "     3    LSTM         1       10    val         0.085097        0.125845     1.079842    1.467075   -0.012819   0.023398       0.704196      0.284354       0.049010      0.080506\n",
      "     4    LSTM         1       15   test        -0.061953        0.152830    -0.660858    1.507044   -0.058475   0.071403       0.428732      0.128119       0.070555      0.067590\n",
      "     5    LSTM         1       15    val         0.125394        0.162109     1.054177    1.471284   -0.022531   0.035565       0.653574      0.214809       0.078168      0.110712\n",
      "     6    LSTM         3        5   test        -0.178120        0.292005    -0.819346    2.164915   -0.170001   0.179887       0.592194      0.307466       0.079613      0.079466\n",
      "     7    LSTM         3        5    val         0.315657        0.397429     1.940414    2.074507   -0.006990   0.011521       0.833333      0.210819       0.061919      0.067490\n",
      "     8    LSTM         3       10   test        -0.145370        0.310629    -0.443880    1.907535   -0.137836   0.183531       0.558163      0.245546       0.049320      0.047195\n",
      "     9    LSTM         3       10    val         0.159119        0.190843     1.502888    1.791129   -0.028818   0.056087       0.714286      0.269179       0.059868      0.054211\n",
      "    10    LSTM         3       15   test        -0.149626        0.309227    -0.574150    1.727750   -0.099880   0.175151       0.609009      0.358610       0.025944      0.047443\n",
      "    11    LSTM         3       15    val         0.175460        0.291176     1.832947    2.068786   -0.020787   0.055445       0.773016      0.241212       0.049688      0.058680\n",
      "    12    LSTM         5        5   test        -0.213561        0.343251    -1.247387    2.372877   -0.216542   0.268949       0.377522      0.165305       0.068072      0.072425\n",
      "    13    LSTM         5        5    val         0.368979        0.685186     1.137460    1.451468   -0.058139   0.083816       0.628553      0.061456       0.085907      0.098111\n",
      "    14    LSTM         5       10   test        -0.215652        0.377184    -0.982573    1.892618   -0.200392   0.260919       0.470381      0.276501       0.060871      0.086279\n",
      "    15    LSTM         5       10    val         0.976065        2.055379     1.970745    2.604320   -0.039279   0.082252       0.707872      0.155776       0.058441      0.074760\n",
      "    16    LSTM         5       15   test        -0.160753        0.372924    -0.304050    1.687083   -0.140997   0.226279       0.566396      0.273334       0.047296      0.083604\n",
      "    17    LSTM         5       15    val         0.218740        0.293511     1.458380    1.650981   -0.023093   0.060286       0.822222      0.201843       0.048182      0.080067\n",
      "    18 XGBoost         1        5   test         0.018918        0.031453     0.527518    0.788213   -0.003010   0.007511       0.777778      0.272166       0.024446      0.046466\n",
      "    19 XGBoost         1        5    val         0.080032        0.137482     1.028267    1.538164   -0.000890   0.001821       0.942308      0.115385       0.056824      0.117716\n",
      "    20 XGBoost         1       10   test        -0.010921        0.023638    -0.496660    0.910023   -0.012976   0.018988       0.242647      0.280442       0.054504      0.098950\n",
      "    21 XGBoost         1       10    val         0.019129        0.038614     0.589120    0.968738   -0.005699   0.013187       0.875000      0.150000       0.051690      0.088196\n",
      "    22 XGBoost         1       15   test        -0.038020        0.093462    -0.366262    1.092469   -0.049828   0.060928       0.427737      0.215801       0.191675      0.183808\n",
      "    23 XGBoost         1       15    val         0.069607        0.109663     0.959873    1.278550   -0.014481   0.020869       0.749936      0.144879       0.184556      0.213946\n",
      "    24 XGBoost         3        5   test         0.125941        0.562874     0.085097    1.740869   -0.071667   0.094005       0.439130      0.268147       0.089645      0.117512\n",
      "    25 XGBoost         3        5    val         0.101237        0.178808     0.678943    0.887244   -0.042272   0.071782       0.729234      0.172994       0.102053      0.126801\n",
      "    26 XGBoost         3       10   test        -0.044623        0.147106    -0.194789    1.029890   -0.042262   0.091881       0.541497      0.159463       0.072505      0.149528\n",
      "    27 XGBoost         3       10    val         0.244730        0.508918     0.762548    1.357996   -0.022871   0.050154       0.751151      0.066301       0.064693      0.110977\n",
      "    28 XGBoost         3       15   test        -0.141426        0.245171    -0.431367    1.274429   -0.115474   0.189473       0.583333      0.280610       0.105281      0.165643\n",
      "    29 XGBoost         3       15    val         0.047049        0.080729     0.449730    0.729196   -0.027110   0.060412       0.718266      0.191022       0.094240      0.168249\n",
      "    30 XGBoost         5        5   test        -0.049515        0.418860    -0.424079    1.655957   -0.112953   0.175841       0.457983      0.208526       0.094339      0.124354\n",
      "    31 XGBoost         5        5    val         0.152369        0.244055     1.230116    1.566050   -0.014802   0.033408       0.813043      0.257037       0.086071      0.106540\n",
      "    32 XGBoost         5       10   test         0.004661        0.752246    -0.262417    3.023806   -0.247114   0.253184       0.556928      0.272461       0.184613      0.154303\n",
      "    33 XGBoost         5       10    val         0.388340        0.612179     1.070197    0.916977   -0.098081   0.114841       0.622839      0.181584       0.182250      0.178656\n",
      "    34 XGBoost         5       15   test        -0.212999        0.412638    -1.089272    2.160996   -0.232079   0.263365       0.435168      0.152832       0.182567      0.175907\n",
      "    35 XGBoost         5       15    val         0.424421        0.806671     1.020858    1.334275   -0.044809   0.062724       0.676188      0.062122       0.152215      0.185023\n",
      "VALIDATION_ERROR: falha ao selecionar vencedor: Invalid format specifier '.1% if not pd.isna(top['hit_rate']) else float('nan')' for object of type 'float'\n",
      "\n",
      "[CHECKLIST OBRIGATÓRIO — dry_run]\n",
      "- SSOT usado: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Labels D+1/D+3/D+5: OK\n",
      "- Walk-forward folds: 10\n",
      "- Métricas previsão — linhas: 360\n",
      "- Métricas operacionais — linhas: 360\n",
      "- Vencedor destacado: FALHA\n",
      "- Persistência: DESLIGADA (dry_run=True)\n",
      "CHECKLIST_FAILURE: algum item obrigatório não foi atendido. Revise os logs acima.\n",
      "\n",
      "[RELATÓRIO FINAL — Estrutura]\n",
      "- pred_df.info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 360 entries, 3 to 356\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   model    360 non-null    object \n",
      " 1   horizon  360 non-null    int64  \n",
      " 2   window   360 non-null    int64  \n",
      " 3   fold     360 non-null    int64  \n",
      " 4   split    360 non-null    object \n",
      " 5   AUC      360 non-null    float64\n",
      " 6   ACC      360 non-null    float64\n",
      " 7   F1       360 non-null    float64\n",
      "dtypes: float64(3), int64(3), object(2)\n",
      "memory usage: 25.3+ KB\n",
      "None\n",
      "- op_df.info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 360 entries, 3 to 356\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   model       360 non-null    object \n",
      " 1   horizon     360 non-null    int64  \n",
      " 2   window      360 non-null    int64  \n",
      " 3   fold        360 non-null    int64  \n",
      " 4   split       360 non-null    object \n",
      " 5   ann_return  360 non-null    float64\n",
      " 6   sharpe      360 non-null    float64\n",
      " 7   maxdd       360 non-null    float64\n",
      " 8   hit_rate    193 non-null    float64\n",
      " 9   turnover    360 non-null    float64\n",
      " 10  threshold   360 non-null    float64\n",
      "dtypes: float64(6), int64(3), object(2)\n",
      "memory usage: 33.8+ KB\n",
      "None\n",
      "\n",
      "[Amostras iniciais — pred_df]\n",
      "model  horizon  window  fold split      AUC      ACC       F1\n",
      " LSTM        1       5     1  test 0.464336 0.458333 0.000000\n",
      " LSTM        1       5     1   val 0.567308 0.620690 0.388889\n",
      " LSTM        1       5     2  test 0.533918 0.495726 0.213333\n",
      " LSTM        1       5     2   val 0.579576 0.490909 0.363636\n",
      " LSTM        1       5     3  test 0.503052 0.570248 0.446809\n",
      " LSTM        1       5     3   val 0.522487 0.490909 0.176471\n",
      " LSTM        1       5     4  test 0.425079 0.474576 0.261905\n",
      " LSTM        1       5     4   val 0.594920 0.607143 0.083333\n",
      " LSTM        1       5     5  test 0.505102 0.572650 0.404762\n",
      " LSTM        1       5     5   val 0.464103 0.517857 0.228571\n",
      "\n",
      "[Amostras iniciais — op_df]\n",
      "model  horizon  window  fold split  ann_return    sharpe     maxdd  hit_rate  turnover  threshold\n",
      " LSTM        1       5     1  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     1   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     3  test   -0.057036 -1.496348 -0.027805       0.0  0.016529       0.55\n",
      " LSTM        1       5     3   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      "\n",
      "[Intervalos temporais cobertos]\n",
      "- Dataset: 2012-01-03 → 2025-09-19\n",
      "- Folds: 10 (test_months=6, val_months=3, treino mínimo=18)\n",
      "\n",
      "[Contagens totais]\n",
      "- pred_df: 360 linhas\n",
      "- op_df: 360 linhas\n",
      "\n",
      "[2025-09-19 16:06:15] Fim — Comparativo (dry_run=True, persist=False)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Comparativo XGBoost vs. LSTM no IBOV (GOLD/SILVER)\n",
    "- Um único bloco de código auto-contido.\n",
    "- Inicia em dry_run=True (simulação). Não persiste nada quando dry_run=True.\n",
    "- Respeita SSOT: usa apenas /home/wrm/BOLSA_2026/{gold,silver}/.\n",
    "- Walk-forward (expanding) com blocos explícitos, sem embaralhar tempo.\n",
    "- Gera métricas de previsão e operacionais (long/flat com custo simples).\n",
    "- Emite mensagens normativas em caso de falhas (VALIDATION_ERROR, CHECKLIST_FAILURE).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import types\n",
    "import errno\n",
    "import warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Verificação de dependências (obrigatório para comparação XGBoost vs LSTM)\n",
    "_missing = []\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "except Exception as e:\n",
    "    _missing.append(f\"xgboost ({e})\")\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential # pyright: ignore[reportMissingImports]\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, Input # pyright: ignore[reportMissingImports]\n",
    "    from tensorflow.keras.callbacks import EarlyStopping # pyright: ignore[reportMissingImports]\n",
    "except Exception as e:\n",
    "    _missing.append(f\"tensorflow ({e})\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, roc_auc_score,\n",
    "    accuracy_score, f1_score\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "# =========================\n",
    "# Configurações e Parâmetros\n",
    "# =========================\n",
    "\n",
    "@dataclass\n",
    "class RunConfig:\n",
    "    dry_run: bool = True\n",
    "    persist: bool = False  # ignorado quando dry_run=True\n",
    "    # Caminhos SSOT\n",
    "    gold_path: str = \"/home/wrm/BOLSA_2026/gold/IBOV_gold.parquet\"\n",
    "    silver_path: str = \"/home/wrm/BOLSA_2026/silver/IBOV_silver.parquet\"\n",
    "    # Janelas e horizontes\n",
    "    windows: Tuple[int, ...] = (5, 10, 15)\n",
    "    horizons: Tuple[int, ...] = (1, 3, 5)  # D+1, D+3, D+5\n",
    "    # Walk-forward\n",
    "    min_train_months: int = 18\n",
    "    test_months: int = 6\n",
    "    val_months: int = 3\n",
    "    max_folds: int = 10  # entre 5 e 10\n",
    "    # Estratégia operacional\n",
    "    cost_per_trade_bps: float = 10.0  # 10 bps por troca de posição\n",
    "    trading_days_per_year: int = 252\n",
    "    # LSTM\n",
    "    lstm_units: int = 48  # 32–64\n",
    "    lstm_layers: int = 1  # 1–2\n",
    "    lstm_dropout: float = 0.2  # 0.1–0.3\n",
    "    lstm_epochs: int = 50\n",
    "    lstm_batch_size: int = 32\n",
    "    lstm_patience: int = 5\n",
    "    # XGBoost\n",
    "    xgb_learning_rate: float = 0.05\n",
    "    xgb_max_depth: int = 5\n",
    "    xgb_n_estimators: int = 1000\n",
    "    xgb_early_stopping_rounds: int = 50\n",
    "    # Thresholds (busca)\n",
    "    prob_threshold_grid: Tuple[float, ...] = (0.50, 0.55, 0.60, 0.65, 0.70)\n",
    "    reg_threshold_percentiles: Tuple[int, ...] = (50, 60, 70, 80)\n",
    "    # Segurança\n",
    "    restrict_prefixes: Tuple[str, ...] = (\n",
    "        \"/home/wrm/BOLSA_2026/gold\",\n",
    "        \"/home/wrm/BOLSA_2026/silver\",\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# Utilidades gerais\n",
    "# =========================\n",
    "\n",
    "def now_ts() -> str:\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def validate_env(cfg: RunConfig) -> Optional[str]:\n",
    "    if _missing:\n",
    "        return f\"CHECKLIST_FAILURE: dependências ausentes para comparação XGBoost vs LSTM -> {', '.join(_missing)}\"\n",
    "    return None\n",
    "\n",
    "def path_exists(p: str) -> bool:\n",
    "    try:\n",
    "        return os.path.exists(p)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def enforce_ssot_path(p: str, allowed_prefixes: Tuple[str, ...]) -> bool:\n",
    "    try:\n",
    "        abspath = os.path.abspath(p)\n",
    "        return any(abspath.startswith(os.path.abspath(pref)) for pref in allowed_prefixes)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def detect_data_path(cfg: RunConfig) -> Tuple[Optional[str], str]:\n",
    "    # GOLD preferencial; senão SILVER\n",
    "    gold_ok = path_exists(cfg.gold_path)\n",
    "    silver_ok = path_exists(cfg.silver_path)\n",
    "    chosen = None\n",
    "    msg = \"\"\n",
    "    if gold_ok and enforce_ssot_path(cfg.gold_path, cfg.restrict_prefixes):\n",
    "        chosen = cfg.gold_path\n",
    "        msg = \"GOLD\"\n",
    "    elif silver_ok and enforce_ssot_path(cfg.silver_path, cfg.restrict_prefixes):\n",
    "        chosen = cfg.silver_path\n",
    "        msg = \"SILVER\"\n",
    "    return chosen, msg\n",
    "\n",
    "def read_parquet_any(path: str) -> pd.DataFrame:\n",
    "    # Pandas suporta diretório parquet dataset; confiamos em pyarrow\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def detect_date_and_price_cols(df: pd.DataFrame) -> Tuple[Optional[str], Optional[str]]:\n",
    "    date_candidates = [\"date\", \"Date\", \"DATE\", \"datetime\", \"Datetime\", \"DATETIME\", \"data\", \"DATA\"]\n",
    "    price_candidates = [\n",
    "        \"close\",\"Close\",\"CLOSE\",\"adj_close\",\"Adj Close\",\"ADJ_CLOSE\",\n",
    "        \"fechamento\",\"FECHAMENTO\",\"price\",\"Price\",\"PRICE\",\"IBOV\"\n",
    "    ]\n",
    "    date_col = next((c for c in date_candidates if c in df.columns), None)\n",
    "    price_col = next((c for c in price_candidates if c in df.columns), None)\n",
    "    return date_col, price_col\n",
    "\n",
    "def ensure_datetime(df: pd.DataFrame, date_col: Optional[str]) -> pd.DataFrame:\n",
    "    if date_col is None:\n",
    "        # Tentar usar índice se já é datetime-like\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            out = df.copy()\n",
    "            out = out.sort_index()\n",
    "            out[\"__date__\"] = out.index\n",
    "            return out\n",
    "        raise ValueError(\"VALIDATION_ERROR: coluna de data não encontrada e índice não é DatetimeIndex.\")\n",
    "    out = df.copy()\n",
    "    out[date_col] = pd.to_datetime(out[date_col], errors=\"coerce\", utc=False)\n",
    "    out = out.dropna(subset=[date_col]).sort_values(by=date_col)\n",
    "    out[\"__date__\"] = out[date_col].values\n",
    "    return out\n",
    "\n",
    "def compute_log_returns(price: pd.Series) -> pd.Series:\n",
    "    return np.log(price / price.shift(1))\n",
    "\n",
    "def forward_return(series: pd.Series, h: int) -> pd.Series:\n",
    "    # log retorno acumulado adiante (close_{t+h}/close_t)\n",
    "    return np.log(series.shift(-h) / series)\n",
    "\n",
    "def summarize_df(df: pd.DataFrame, label: str) -> Dict[str, any]:\n",
    "    dmin = pd.to_datetime(df[\"__date__\"]).min()\n",
    "    dmax = pd.to_datetime(df[\"__date__\"]).max()\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"rows\": int(df.shape[0]),\n",
    "        \"cols\": int(df.shape[1]),\n",
    "        \"date_min\": str(dmin.date()) if pd.notnull(dmin) else None,\n",
    "        \"date_max\": str(dmax.date()) if pd.notnull(dmax) else None,\n",
    "        \"columns\": list(df.columns)\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# Alvos (Labels) e Features\n",
    "# =========================\n",
    "\n",
    "def detect_or_generate_labels(\n",
    "    df: pd.DataFrame,\n",
    "    price_col: Optional[str],\n",
    "    horizons: Tuple[int, ...]\n",
    ") -> Tuple[pd.DataFrame, Dict[int, Dict[str, str]], List[str]]:\n",
    "    \"\"\"\n",
    "    Retorna:\n",
    "    - df com colunas de labels\n",
    "    - mapping por horizonte: {\"type\": \"classification\"/\"regression\", \"col\": <colname>, \"desc\": <desc>}\n",
    "    - log_msgs descrevendo a origem dos labels\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    label_info: Dict[int, Dict[str, str]] = {}\n",
    "    logs: List[str] = []\n",
    "\n",
    "    # Candidatos de labels existentes\n",
    "    # Para classificação\n",
    "    clf_patterns = [\n",
    "        \"label_d{h}\", \"target_d{h}\", \"direction_d{h}\", \"dir_d{h}\",\n",
    "        \"y_d{h}\", \"y_d+{h}\", \"class_d{h}\", \"bin_d{h}\"\n",
    "    ]\n",
    "    # Para regressão\n",
    "    reg_patterns = [\n",
    "        \"ret_fwd_{h}\", \"return_fwd_{h}\", \"rtn_fwd_{h}\", \"y_reg_{h}\",\n",
    "        \"ret_d{h}\", \"return_d{h}\"\n",
    "    ]\n",
    "\n",
    "    for h in horizons:\n",
    "        found_col = None\n",
    "        found_type = None\n",
    "\n",
    "        # busca por classificação\n",
    "        for pat in clf_patterns:\n",
    "            cname = pat.format(h=h)\n",
    "            if cname in out.columns:\n",
    "                found_col = cname\n",
    "                found_type = \"classification\"\n",
    "                break\n",
    "\n",
    "        # busca por regressão (só se não achou classificação)\n",
    "        if found_col is None:\n",
    "            for pat in reg_patterns:\n",
    "                cname = pat.format(h=h)\n",
    "                if cname in out.columns:\n",
    "                    found_col = cname\n",
    "                    found_type = \"regression\"\n",
    "                    break\n",
    "\n",
    "        # se não achou, gerar a partir do próprio dataset\n",
    "        if found_col is None:\n",
    "            if price_col is None and \"ret1\" not in out.columns:\n",
    "                raise ValueError(\"VALIDATION_ERROR: impossivel gerar rótulos: sem coluna de preço e sem retornos base.\")\n",
    "            # base: usar preço para retorno futuro\n",
    "            if price_col is not None:\n",
    "                out[f\"ret_fwd_{h}\"] = forward_return(out[price_col], h)\n",
    "                out[f\"dir_fwd_{h}\"] = (out[f\"ret_fwd_{h}\"] > 0).astype(int)\n",
    "                found_col = f\"dir_fwd_{h}\"\n",
    "                found_type = \"classification\"\n",
    "                logs.append(f\"h={h}: rótulos GERADOS -> dir_fwd_{h} (binário a partir de ret_fwd_{h}).\")\n",
    "            else:\n",
    "                # fallback: se já houver 'ret1' (log-ret), ainda assim precisamos de preço para fwd; sem preço, não dá.\n",
    "                raise ValueError(\"VALIDATION_ERROR: sem preço para calcular retorno futuro e gerar rótulos.\")\n",
    "        else:\n",
    "            logs.append(f\"h={h}: rótulos NATIVOS detectados -> {found_col} ({found_type}).\")\n",
    "\n",
    "        label_info[h] = {\n",
    "            \"type\": found_type,\n",
    "            \"col\": found_col,\n",
    "            \"desc\": \"nativo\" if \"NATIVOS\" in logs[-1] else \"gerado\"\n",
    "        }\n",
    "\n",
    "    return out, label_info, logs\n",
    "\n",
    "def ensure_base_features(df: pd.DataFrame, price_col: Optional[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Retorno base (log) de 1 dia\n",
    "    if price_col is not None and \"ret1\" not in out.columns:\n",
    "        out[\"ret1\"] = compute_log_returns(out[price_col])\n",
    "    # algumas features simples das últimas 5 barras como base para LSTM\n",
    "    for w in (5,):\n",
    "        out[f\"roll_mean_ret_{w}\"] = out[\"ret1\"].rolling(w).mean()\n",
    "        out[f\"roll_std_ret_{w}\"] = out[\"ret1\"].rolling(w).std()\n",
    "    return out\n",
    "\n",
    "def build_xgb_features(df: pd.DataFrame, window: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Constrói features tabulares para XGBoost usando retornos e estatísticas de janela.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    # Lags de ret1\n",
    "    max_lags = min(window, 10)  # limitar\n",
    "    for lag in range(1, max_lags + 1):\n",
    "        out[f\"ret1_lag_{lag}\"] = out[\"ret1\"].shift(lag)\n",
    "    # Médias e vol de retorno\n",
    "    out[f\"ret1_roll_mean_{window}\"] = out[\"ret1\"].rolling(window).mean()\n",
    "    out[f\"ret1_roll_std_{window}\"] = out[\"ret1\"].rolling(window).std()\n",
    "    # Z-score do retorno instantâneo vs janela\n",
    "    out[f\"ret1_z_{window}\"] = (out[\"ret1\"] - out[f\"ret1_roll_mean_{window}\"]) / (out[f\"ret1_roll_std_{window}\"] + 1e-8)\n",
    "    out = out.dropna().copy()\n",
    "    return out\n",
    "\n",
    "def build_lstm_sequences(\n",
    "    df: pd.DataFrame,\n",
    "    features_cols: List[str],\n",
    "    label_col: str,\n",
    "    window: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Cria sequências [amostra, janela, features] e alvo alinhado.\n",
    "    \"\"\"\n",
    "    X_list, y_list = [], []\n",
    "    values = df[features_cols].values\n",
    "    yvals = df[label_col].values\n",
    "    for i in range(window, len(df)):\n",
    "        X_list.append(values[i-window:i, :])\n",
    "        y_list.append(yvals[i])\n",
    "    if not X_list:\n",
    "        return np.empty((0, window, len(features_cols))), np.empty((0,))\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    y = np.array(y_list)\n",
    "    return X, y\n",
    "\n",
    "# =========================\n",
    "# Walk-forward e Métricas\n",
    "# =========================\n",
    "\n",
    "def month_diff(a: pd.Timestamp, b: pd.Timestamp) -> int:\n",
    "    return (b.year - a.year) * 12 + (b.month - a.month)\n",
    "\n",
    "def build_walk_forward_splits(\n",
    "    df: pd.DataFrame,\n",
    "    min_train_months: int,\n",
    "    val_months: int,\n",
    "    test_months: int,\n",
    "    max_folds: int\n",
    ") -> List[Dict[str, pd.Timestamp]]:\n",
    "    \"\"\"\n",
    "    Retorna lista de dicts com ranges de datas explícitos: train_start, train_end, val_start, val_end, test_start, test_end\n",
    "    \"\"\"\n",
    "    dates = pd.to_datetime(df[\"__date__\"])\n",
    "    start = dates.min().normalize()\n",
    "    end = dates.max().normalize()\n",
    "\n",
    "    # Determinar primeiros limites\n",
    "    # Treino mínimo\n",
    "    train_end_initial = start + pd.DateOffset(months=min_train_months) - pd.DateOffset(days=1)\n",
    "    if train_end_initial >= end:\n",
    "        raise ValueError(\"VALIDATION_ERROR: série insuficiente para min_train_months.\")\n",
    "    # Primeira janela de teste\n",
    "    test_start = train_end_initial + pd.DateOffset(days=1)\n",
    "    test_end = test_start + pd.DateOffset(months=test_months) - pd.DateOffset(days=1)\n",
    "\n",
    "    folds = []\n",
    "    folds_count = 0\n",
    "    while test_start < end and folds_count < max_folds:\n",
    "        # Ajustar test_end ao fim da série\n",
    "        if test_end > end:\n",
    "            test_end = end\n",
    "        # Validação: últimos val_months do treino expandido\n",
    "        val_end = test_start - pd.DateOffset(days=1)\n",
    "        val_start = val_end - pd.DateOffset(months=val_months) + pd.DateOffset(days=1)\n",
    "        train_start = start\n",
    "        train_end = val_start - pd.DateOffset(days=1)\n",
    "        # Checagens\n",
    "        if train_start >= train_end or val_start >= val_end or test_start > test_end:\n",
    "            break\n",
    "        folds.append({\n",
    "            \"train_start\": train_start, \"train_end\": train_end,\n",
    "            \"val_start\": val_start, \"val_end\": val_end,\n",
    "            \"test_start\": test_start, \"test_end\": test_end\n",
    "        })\n",
    "        folds_count += 1\n",
    "        # próximo bloco de teste\n",
    "        test_start = test_end + pd.DateOffset(days=1)\n",
    "        test_end = test_start + pd.DateOffset(months=test_months) - pd.DateOffset(days=1)\n",
    "\n",
    "    if len(folds) < 5:\n",
    "        # Garantir 5–10 blocos conforme requisito\n",
    "        raise ValueError(f\"VALIDATION_ERROR: splits walk-forward insuficientes ({len(folds)}).\")\n",
    "    return folds\n",
    "\n",
    "def subset_by_date(df: pd.DataFrame, start: pd.Timestamp, end: pd.Timestamp) -> pd.DataFrame:\n",
    "    mask = (df[\"__date__\"] >= start) & (df[\"__date__\"] <= end)\n",
    "    return df.loc[mask].copy()\n",
    "\n",
    "def annualized_return(daily_returns: np.ndarray, days_per_year: int) -> float:\n",
    "    if len(daily_returns) == 0:\n",
    "        return 0.0\n",
    "    cumulative = np.prod(1.0 + daily_returns)\n",
    "    years = len(daily_returns) / days_per_year\n",
    "    if years <= 0:\n",
    "        return 0.0\n",
    "    return cumulative ** (1.0 / years) - 1.0\n",
    "\n",
    "def sharpe_ratio(daily_returns: np.ndarray, days_per_year: int) -> float:\n",
    "    if len(daily_returns) < 2:\n",
    "        return 0.0\n",
    "    mu = np.mean(daily_returns)\n",
    "    sd = np.std(daily_returns, ddof=1) + 1e-12\n",
    "    return (mu / sd) * math.sqrt(days_per_year)\n",
    "\n",
    "def max_drawdown(equity: np.ndarray) -> float:\n",
    "    if len(equity) == 0:\n",
    "        return 0.0\n",
    "    peak = np.maximum.accumulate(equity)\n",
    "    dd = (equity / peak) - 1.0\n",
    "    return dd.min()\n",
    "\n",
    "def evaluate_strategy_long_flat(\n",
    "    y_true_returns: np.ndarray,\n",
    "    preds: np.ndarray,\n",
    "    threshold: float,\n",
    "    is_classification: bool,\n",
    "    cost_per_trade_bps: float,\n",
    "    days_per_year: int\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    - Para classificação: entrar comprado quando prob >= threshold, senão flat.\n",
    "    - Para regressão: entrar comprado quando retorno previsto >= threshold (valor em retorno, não prob).\n",
    "    - Custos: custo fixo por mudança de posição (0 -> 1 ou 1 -> 0) de cost_per_trade_bps.\n",
    "    \"\"\"\n",
    "    if len(y_true_returns) != len(preds) or len(preds) == 0:\n",
    "        return {k: np.nan for k in [\"ann_return\", \"sharpe\", \"maxdd\", \"hit_rate\", \"turnover\", \"threshold\"]}\n",
    "    if is_classification:\n",
    "        pos = (preds >= threshold).astype(int)\n",
    "    else:\n",
    "        pos = (preds >= threshold).astype(int)\n",
    "\n",
    "    # Custos por troca\n",
    "    changes = np.abs(np.diff(pos, prepend=0))\n",
    "    trade_costs = (changes * (cost_per_trade_bps / 10000.0))  # bps -> decimal\n",
    "\n",
    "    daily_ret = pos * y_true_returns - trade_costs\n",
    "    equity = np.cumprod(1.0 + daily_ret)\n",
    "    ann = annualized_return(daily_ret, days_per_year)\n",
    "    shp = sharpe_ratio(daily_ret, days_per_year)\n",
    "    mdd = max_drawdown(equity)\n",
    "    # Hit-rate: fração de dias com posição==1 e retorno>0\n",
    "    hits_n = ((pos == 1) & (y_true_returns > 0)).sum()\n",
    "    pos_n = (pos == 1).sum()\n",
    "    hit_rate = float(hits_n) / float(pos_n) if pos_n > 0 else np.nan\n",
    "    # Turnover: nº de trades / nº de dias\n",
    "    turnover = changes.sum() / len(changes) if len(changes) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"ann_return\": float(ann),\n",
    "        \"sharpe\": float(shp),\n",
    "        \"maxdd\": float(mdd),\n",
    "        \"hit_rate\": float(hit_rate) if not np.isnan(hit_rate) else np.nan,\n",
    "        \"turnover\": float(turnover),\n",
    "        \"threshold\": float(threshold)\n",
    "    }\n",
    "\n",
    "def pick_best_threshold_on_validation(\n",
    "    y_val_returns: np.ndarray,\n",
    "    preds_val: np.ndarray,\n",
    "    is_classification: bool,\n",
    "    cfg: RunConfig\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Busca threshold que maximiza Sharpe na validação (sem vazar teste).\n",
    "    \"\"\"\n",
    "    best_thr = None\n",
    "    best_metrics = None\n",
    "    if is_classification:\n",
    "        grid = cfg.prob_threshold_grid\n",
    "        for thr in grid:\n",
    "            m = evaluate_strategy_long_flat(\n",
    "                y_val_returns, preds_val, thr, True, cfg.cost_per_trade_bps, cfg.trading_days_per_year\n",
    "            )\n",
    "            if best_metrics is None or (m[\"sharpe\"] > best_metrics[\"sharpe\"]):\n",
    "                best_thr, best_metrics = thr, m\n",
    "    else:\n",
    "        # thresholds por percentil das previsões\n",
    "        percs = np.percentile(preds_val, cfg.reg_threshold_percentiles)\n",
    "        for thr in percs:\n",
    "            m = evaluate_strategy_long_flat(\n",
    "                y_val_returns, preds_val, float(thr), False, cfg.cost_per_trade_bps, cfg.trading_days_per_year\n",
    "            )\n",
    "            if best_metrics is None or (m[\"sharpe\"] > best_metrics[\"sharpe\"]):\n",
    "                best_thr, best_metrics = float(thr), m\n",
    "\n",
    "    if best_thr is None:\n",
    "        # fallback\n",
    "        best_thr = 0.5 if is_classification else float(np.percentile(preds_val, 60.0))\n",
    "        best_metrics = evaluate_strategy_long_flat(\n",
    "            y_val_returns, preds_val, best_thr, is_classification, cfg.cost_per_trade_bps, cfg.trading_days_per_year\n",
    "        )\n",
    "    return best_thr, best_metrics\n",
    "\n",
    "# =========================\n",
    "# Treino e Avaliação\n",
    "# =========================\n",
    "\n",
    "def train_eval_xgb(\n",
    "    X_tr: np.ndarray, y_tr: np.ndarray,\n",
    "    X_va: np.ndarray, y_va: np.ndarray,\n",
    "    X_te: np.ndarray, y_te: np.ndarray,\n",
    "    task: str, cfg: RunConfig\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Retorna: (preds_val, preds_test, params)\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"learning_rate\": cfg.xgb_learning_rate,\n",
    "        \"max_depth\": cfg.xgb_max_depth,\n",
    "        \"n_estimators\": cfg.xgb_n_estimators,\n",
    "        \"early_stopping_rounds\": cfg.xgb_early_stopping_rounds,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": max(1, os.cpu_count() - 1)\n",
    "    }\n",
    "    if task == \"classification\":\n",
    "        model = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            **params\n",
    "        )\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            verbose=False\n",
    "        )\n",
    "        preds_val = model.predict_proba(X_va)[:, 1]\n",
    "        preds_test = model.predict_proba(X_te)[:, 1]\n",
    "    else:\n",
    "        model = XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            **params\n",
    "        )\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            verbose=False\n",
    "        )\n",
    "        preds_val = model.predict(X_va)\n",
    "        preds_test = model.predict(X_te)\n",
    "    params[\"best_iterations\"] = getattr(model, \"best_iteration\", None)\n",
    "    return preds_val, preds_test, params\n",
    "\n",
    "def build_lstm_model(\n",
    "    n_features: int,\n",
    "    window: int,\n",
    "    task: str,\n",
    "    cfg: RunConfig\n",
    "):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(window, n_features)))\n",
    "    if cfg.lstm_layers == 2:\n",
    "        model.add(LSTM(cfg.lstm_units, return_sequences=True))\n",
    "        model.add(Dropout(cfg.lstm_dropout))\n",
    "        model.add(LSTM(cfg.lstm_units))\n",
    "    else:\n",
    "        model.add(LSTM(cfg.lstm_units))\n",
    "    model.add(Dropout(cfg.lstm_dropout))\n",
    "    if task == \"classification\":\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"AUC\"])\n",
    "    else:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "def train_eval_lstm(\n",
    "    X_tr_seq: np.ndarray, y_tr: np.ndarray,\n",
    "    X_va_seq: np.ndarray, y_va: np.ndarray,\n",
    "    X_te_seq: np.ndarray,\n",
    "    task: str,\n",
    "    cfg: RunConfig\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Retorna: (preds_val, preds_test, params)\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"units\": cfg.lstm_units,\n",
    "        \"layers\": cfg.lstm_layers,\n",
    "        \"dropout\": cfg.lstm_dropout,\n",
    "        \"epochs\": cfg.lstm_epochs,\n",
    "        \"batch_size\": cfg.lstm_batch_size,\n",
    "        \"patience\": cfg.lstm_patience\n",
    "    }\n",
    "    model = build_lstm_model(X_tr_seq.shape[-1], X_tr_seq.shape[1], task, cfg)\n",
    "    es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=cfg.lstm_patience, restore_best_weights=True, verbose=0)\n",
    "    model.fit(\n",
    "        X_tr_seq, y_tr,\n",
    "        validation_data=(X_va_seq, y_va),\n",
    "        epochs=cfg.lstm_epochs,\n",
    "        batch_size=cfg.lstm_batch_size,\n",
    "        callbacks=[es],\n",
    "        verbose=0\n",
    "    )\n",
    "    preds_val = model.predict(X_va_seq, verbose=0).reshape(-1)\n",
    "    preds_test = model.predict(X_te_seq, verbose=0).reshape(-1)\n",
    "    return preds_val, preds_test, params\n",
    "\n",
    "# =========================\n",
    "# Métricas de previsão\n",
    "# =========================\n",
    "\n",
    "def prediction_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    task: str\n",
    ") -> Dict[str, float]:\n",
    "    res = {}\n",
    "    if len(y_true) == 0:\n",
    "        return res\n",
    "    if task == \"classification\":\n",
    "        # Converter prob -> label para acc/f1 com limiar 0.5 (métrica pura de previsão)\n",
    "        y_hat = (y_pred >= 0.5).astype(int)\n",
    "        try:\n",
    "            res[\"AUC\"] = float(roc_auc_score(y_true, y_pred))\n",
    "        except Exception:\n",
    "            res[\"AUC\"] = np.nan\n",
    "        res[\"ACC\"] = float(accuracy_score(y_true, y_hat))\n",
    "        res[\"F1\"] = float(f1_score(y_true, y_hat, zero_division=0))\n",
    "    else:\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        res[\"MAE\"] = float(mae)\n",
    "        res[\"RMSE\"] = float(rmse)\n",
    "    return res\n",
    "\n",
    "# =========================\n",
    "# Execução principal\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    print(f\"[{now_ts()}] Início — Comparativo XGBoost vs. LSTM (IBOV SSOT)\")\n",
    "    cfg = RunConfig(dry_run=True, persist=False)\n",
    "\n",
    "    # 0) Checagem de dependências\n",
    "    dep_err = validate_env(cfg)\n",
    "    if dep_err:\n",
    "        print(dep_err)\n",
    "        print(\"Checklist não atendido: comparação requer XGBoost e LSTM disponíveis.\")\n",
    "        return\n",
    "\n",
    "    # 1) Detectar caminho de dados (GOLD > SILVER)\n",
    "    path, tier = detect_data_path(cfg)\n",
    "    if path is None:\n",
    "        print(\"CHECKLIST_FAILURE: Nenhum dataset encontrado em GOLD ou SILVER permitidos.\")\n",
    "        return\n",
    "    if not enforce_ssot_path(path, cfg.restrict_prefixes):\n",
    "        print(\"CHECKLIST_FAILURE: Caminho fora do SSOT permitido.\")\n",
    "        return\n",
    "\n",
    "    # 2) Leitura e prova de leitura\n",
    "    try:\n",
    "        df_raw = read_parquet_any(path)\n",
    "    except Exception as e:\n",
    "        print(f\"VALIDATION_ERROR: falha ao ler parquet '{path}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Identificar colunas e preparar datas\n",
    "    date_col, price_col = detect_date_and_price_cols(df_raw)\n",
    "    try:\n",
    "        df = ensure_datetime(df_raw, date_col)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return\n",
    "\n",
    "    # Prova de leitura — schema e datas\n",
    "    proof = summarize_df(df, f\"{tier}_{os.path.basename(path)}\")\n",
    "    print(\"\\n[PROVA DE LEITURA]\")\n",
    "    print(f\"- Caminho efetivo usado: {path} (tier={tier})\")\n",
    "    print(f\"- Schema (primeiras colunas): {proof['columns'][:12]}\")\n",
    "    print(f\"- Contagem de linhas: {proof['rows']}, colunas: {proof['cols']}\")\n",
    "    print(f\"- date_min: {proof['date_min']}, date_max: {proof['date_max']}\")\n",
    "    print(\"- Amostra (head 5):\")\n",
    "    try:\n",
    "        print(df.head(5).to_string(index=False))\n",
    "    except Exception:\n",
    "        print(df.head(5))\n",
    "\n",
    "    # 3) Garantir features base e rótulos\n",
    "    try:\n",
    "        df = ensure_base_features(df, price_col)\n",
    "        df, label_info, label_logs = detect_or_generate_labels(df, price_col, cfg.horizons)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return\n",
    "\n",
    "    print(\"\\n[RÓTULOS — DETECÇÃO/GERAÇÃO]\")\n",
    "    for log in label_logs:\n",
    "        print(f\"- {log}\")\n",
    "    lbl_report = {h: {\"type\": label_info[h][\"type\"], \"col\": label_info[h][\"col\"], \"origem\": label_info[h][\"desc\"]} for h in cfg.horizons}\n",
    "    print(f\"- Resumo: {json.dumps(lbl_report, indent=2, ensure_ascii=False)}\")\n",
    "\n",
    "    # 4) Definir splits walk-forward explícitos\n",
    "    try:\n",
    "        splits = build_walk_forward_splits(\n",
    "            df, cfg.min_train_months, cfg.val_months, cfg.test_months, cfg.max_folds\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return\n",
    "\n",
    "    print(\"\\n[WALK-FORWARD — Folds explícitos]\")\n",
    "    for i, s in enumerate(splits, 1):\n",
    "        print(f\"Fold {i:02d}: \"\n",
    "              f\"train[{str(s['train_start'].date())} → {str(s['train_end'].date())}], \"\n",
    "              f\"val[{str(s['val_start'].date())} → {str(s['val_end'].date())}], \"\n",
    "              f\"test[{str(s['test_start'].date())} → {str(s['test_end'].date())}]\")\n",
    "\n",
    "    # 5) Preparar pipelines e coletar métricas\n",
    "    # Tabelas de saída\n",
    "    pred_metrics_rows = []\n",
    "    op_metrics_rows = []\n",
    "    # Para relatório de hiperparâmetros\n",
    "    xgb_params_log = {}\n",
    "    lstm_params_log = {}\n",
    "\n",
    "    for h in cfg.horizons:\n",
    "        target_type = label_info[h][\"type\"]\n",
    "        target_col = label_info[h][\"col\"]\n",
    "        # Para estratégia, usamos retorno futuro real (para medir PnL)\n",
    "        if f\"ret_fwd_{h}\" not in df.columns:\n",
    "            # Se rótulo for nativo e não houver ret_fwd_h, tentar derivar a partir de preço\n",
    "            if price_col is None:\n",
    "                print(\"VALIDATION_ERROR: sem preço para obter retorno futuro real para métricas operacionais.\")\n",
    "                return\n",
    "            df[f\"ret_fwd_{h}\"] = forward_return(df[price_col], h)\n",
    "\n",
    "        for w in cfg.windows:\n",
    "            # Construir features para XGBoost (tabulares)\n",
    "            dfx = build_xgb_features(df[[\"__date__\", \"ret1\"]].join(\n",
    "                df[[c for c in df.columns if c.startswith(\"roll_\") or c.startswith(\"ret1_\")]], how=\"outer\"\n",
    "            ).join(df[target_col]).join(df[f\"ret_fwd_{h}\"]), window=w)\n",
    "    \n",
    "            # Construir base para LSTM — usaremos features simples e robustas\n",
    "            lstm_feature_cols = [\"ret1\", \"roll_mean_ret_5\", \"roll_std_ret_5\"]\n",
    "            # Garantir que existam\n",
    "            for c in lstm_feature_cols:\n",
    "                if c not in df.columns:\n",
    "                    print(f\"VALIDATION_ERROR: feature base ausente para LSTM: {c}\")\n",
    "                    return\n",
    "            dfl = df[[\"__date__\", target_col, f\"ret_fwd_{h}\"] + lstm_feature_cols].dropna().copy()\n",
    "\n",
    "            # Walk-forward por fold\n",
    "            for fold_idx, s in enumerate(splits, 1):\n",
    "                # Subsets\n",
    "                tr = subset_by_date(dfx, s[\"train_start\"], s[\"train_end\"])\n",
    "                va = subset_by_date(dfx, s[\"val_start\"], s[\"val_end\"])\n",
    "                te = subset_by_date(dfx, s[\"test_start\"], s[\"test_end\"])\n",
    "\n",
    "                if len(tr) == 0 or len(va) == 0 or len(te) == 0:\n",
    "                    print(f\"VALIDATION_ERROR: fold {fold_idx} insuficiente após recortes (XGB).\")\n",
    "                    return\n",
    "\n",
    "                # XGB: preparar matrizes\n",
    "                xgb_features = [c for c in tr.columns if c not in [\"__date__\", target_col, f\"ret_fwd_{h}\"]]\n",
    "                X_tr, y_tr = tr[xgb_features].values, tr[target_col].values\n",
    "                X_va, y_va = va[xgb_features].values, va[target_col].values\n",
    "                X_te, y_te = te[xgb_features].values, te[target_col].values\n",
    "                # Estratégia usa retorno futuro real do período avaliado\n",
    "                yret_val = va[f\"ret_fwd_{h}\"].values\n",
    "                yret_tst = te[f\"ret_fwd_{h}\"].values\n",
    "\n",
    "                # XGB treino/val/test\n",
    "                preds_val_xgb, preds_test_xgb, xgb_params = train_eval_xgb(\n",
    "                    X_tr, y_tr, X_va, y_va, X_te, y_te, target_type, cfg\n",
    "                )\n",
    "                xgb_params_log[(h, w)] = xgb_params\n",
    "\n",
    "                # Métricas de previsão (XGB)\n",
    "                pm_val_xgb = prediction_metrics(y_va, preds_val_xgb, target_type)\n",
    "                pm_tst_xgb = prediction_metrics(y_te, preds_test_xgb, target_type)\n",
    "                pred_metrics_rows.append({\n",
    "                    \"model\": \"XGBoost\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"val\", **pm_val_xgb\n",
    "                })\n",
    "                pred_metrics_rows.append({\n",
    "                    \"model\": \"XGBoost\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"test\", **pm_tst_xgb\n",
    "                })\n",
    "\n",
    "                # Threshold ótimo (validação) e métricas operacionais (XGB)\n",
    "                thr_xgb, thr_metrics_val_xgb = pick_best_threshold_on_validation(\n",
    "                    yret_val, preds_val_xgb, (target_type == \"classification\"), cfg\n",
    "                )\n",
    "                op_val_xgb = evaluate_strategy_long_flat(\n",
    "                    yret_val, preds_val_xgb, thr_xgb, (target_type == \"classification\"),\n",
    "                    cfg.cost_per_trade_bps, cfg.trading_days_per_year\n",
    "                )\n",
    "                op_tst_xgb = evaluate_strategy_long_flat(\n",
    "                    yret_tst, preds_test_xgb, thr_xgb, (target_type == \"classification\"),\n",
    "                    cfg.cost_per_trade_bps, cfg.trading_days_per_year\n",
    "                )\n",
    "                op_metrics_rows.append({\n",
    "                    \"model\": \"XGBoost\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"val\", **op_val_xgb\n",
    "                })\n",
    "                op_metrics_rows.append({\n",
    "                    \"model\": \"XGBoost\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"test\", **op_tst_xgb\n",
    "                })\n",
    "\n",
    "                # ====== LSTM ======\n",
    "                # Subsets para LSTM\n",
    "                tr_l = subset_by_date(dfl, s[\"train_start\"], s[\"train_end\"])\n",
    "                va_l = subset_by_date(dfl, s[\"val_start\"], s[\"val_end\"])\n",
    "                te_l = subset_by_date(dfl, s[\"test_start\"], s[\"test_end\"])\n",
    "                if len(tr_l) == 0 or len(va_l) == 0 or len(te_l) == 0:\n",
    "                    print(f\"VALIDATION_ERROR: fold {fold_idx} insuficiente após recortes (LSTM).\")\n",
    "                    return\n",
    "\n",
    "                # Escalonamento por treino somente\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(tr_l[lstm_feature_cols].values)\n",
    "                tr_l_scaled = tr_l.copy()\n",
    "                va_l_scaled = va_l.copy()\n",
    "                te_l_scaled = te_l.copy()\n",
    "                tr_l_scaled[lstm_feature_cols] = scaler.transform(tr_l[lstm_feature_cols].values)\n",
    "                va_l_scaled[lstm_feature_cols] = scaler.transform(va_l[lstm_feature_cols].values)\n",
    "                te_l_scaled[lstm_feature_cols] = scaler.transform(te_l[lstm_feature_cols].values)\n",
    "\n",
    "                # Sequências\n",
    "                Xtr_seq, ytr_seq = build_lstm_sequences(tr_l_scaled, lstm_feature_cols, target_col, w)\n",
    "                Xva_seq, yva_seq = build_lstm_sequences(va_l_scaled, lstm_feature_cols, target_col, w)\n",
    "                Xte_seq, yte_seq = build_lstm_sequences(te_l_scaled, lstm_feature_cols, target_col, w)\n",
    "                # Ajuste de retorno futuro para alinhar ao corte de janela\n",
    "                yret_val_seq = va_l_scaled[f\"ret_fwd_{h}\"].values[w:]\n",
    "                yret_tst_seq = te_l_scaled[f\"ret_fwd_{h}\"].values[w:]\n",
    "\n",
    "                if any(arr.shape[0] == 0 for arr in [Xtr_seq, Xva_seq, Xte_seq]):\n",
    "                    print(f\"VALIDATION_ERROR: sequências LSTM vazias no fold {fold_idx}, janela {w}.\")\n",
    "                    return\n",
    "\n",
    "                preds_val_lstm, preds_test_lstm, lstm_params = train_eval_lstm(\n",
    "                    Xtr_seq, ytr_seq, Xva_seq, yva_seq, Xte_seq, target_type, cfg\n",
    "                )\n",
    "                lstm_params_log[(h, w)] = lstm_params\n",
    "\n",
    "                # Métricas de previsão (LSTM)\n",
    "                pm_val_lstm = prediction_metrics(yva_seq, preds_val_lstm, target_type)\n",
    "                pm_tst_lstm = prediction_metrics(yte_seq, preds_test_lstm, target_type)\n",
    "                pred_metrics_rows.append({\n",
    "                    \"model\": \"LSTM\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"val\", **pm_val_lstm\n",
    "                })\n",
    "                pred_metrics_rows.append({\n",
    "                    \"model\": \"LSTM\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"test\", **pm_tst_lstm\n",
    "                })\n",
    "\n",
    "                # Threshold ótimo (validação) e métricas operacionais (LSTM)\n",
    "                thr_lstm, thr_metrics_val_lstm = pick_best_threshold_on_validation(\n",
    "                    yret_val_seq, preds_val_lstm, (target_type == \"classification\"), cfg\n",
    "                )\n",
    "                op_val_lstm = evaluate_strategy_long_flat(\n",
    "                    yret_val_seq, preds_val_lstm, thr_lstm, (target_type == \"classification\"),\n",
    "                    cfg.cost_per_trade_bps, cfg.trading_days_per_year\n",
    "                )\n",
    "                op_tst_lstm = evaluate_strategy_long_flat(\n",
    "                    yret_tst_seq, preds_test_lstm, thr_lstm, (target_type == \"classification\"),\n",
    "                    cfg.cost_per_trade_bps, cfg.trading_days_per_year\n",
    "                )\n",
    "                op_metrics_rows.append({\n",
    "                    \"model\": \"LSTM\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"val\", **op_val_lstm\n",
    "                })\n",
    "                op_metrics_rows.append({\n",
    "                    \"model\": \"LSTM\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"test\", **op_tst_lstm\n",
    "                })\n",
    "\n",
    "    # 6) Consolidação de métricas (previsão e operacionais)\n",
    "    pred_df = pd.DataFrame(pred_metrics_rows).sort_values([\"model\", \"horizon\", \"window\", \"fold\", \"split\"])\n",
    "    op_df = pd.DataFrame(op_metrics_rows).sort_values([\"model\", \"horizon\", \"window\", \"fold\", \"split\"])\n",
    "\n",
    "    # Relatos\n",
    "    print(\"\\n[FEATURES POR JANELA — XGBoost]\")\n",
    "    print(\"- Para cada janela (5/10/15): lags ret1 (1..min(janela,10)), ret1_roll_mean_janela, ret1_roll_std_janela, ret1_z_janela.\")\n",
    "    print(\"[SEQUÊNCIAS — LSTM]\")\n",
    "    print(\"- Features por passo: ['ret1','roll_mean_ret_5','roll_std_ret_5'] (padronizadas no treino).\")\n",
    "    print(\"- Shape por janela: [amostras, janela, 3].\")\n",
    "\n",
    "    print(\"\\n[HIPERPARÂMETROS FINAIS — XGBoost]\")\n",
    "    if xgb_params_log:\n",
    "        # Mostrar por (h,w) últimos vistos\n",
    "        for (h, w), p in sorted(xgb_params_log.items()):\n",
    "            p2 = {k: v for k, v in p.items() if k in [\"learning_rate\", \"max_depth\", \"n_estimators\", \"early_stopping_rounds\", \"best_iterations\"]}\n",
    "            print(f\"- h={h}, w={w}: {p2}\")\n",
    "\n",
    "    print(\"\\n[HIPERPARÂMETROS FINAIS — LSTM]\")\n",
    "    if lstm_params_log:\n",
    "        for (h, w), p in sorted(lstm_params_log.items()):\n",
    "            print(f\"- h={h}, w={w}: {p}\")\n",
    "\n",
    "    # 7) Tabelas de métricas\n",
    "    def agg_mean_std(df: pd.DataFrame, value_cols: List[str]) -> pd.DataFrame:\n",
    "        g = df.groupby([\"model\", \"horizon\", \"window\", \"split\"], as_index=False)\n",
    "        out = g[value_cols].agg(['mean','std'])\n",
    "        out.columns = ['_'.join(col).strip() for col in out.columns.values]\n",
    "        out = out.reset_index()\n",
    "        return out\n",
    "\n",
    "    print(\"\\n[MÉTRICAS DE PREVISÃO — por fold (head)]\")\n",
    "    try:\n",
    "        print(pred_df.head(12).to_string(index=False))\n",
    "    except Exception:\n",
    "        print(pred_df.head(12))\n",
    "    pred_cols = [c for c in [\"AUC\",\"ACC\",\"F1\",\"MAE\",\"RMSE\"] if c in pred_df.columns]\n",
    "    pred_agg = agg_mean_std(pred_df, pred_cols) if pred_cols else pd.DataFrame()\n",
    "    print(\"\\n[MÉTRICAS DE PREVISÃO — agregadas (média ± desvio)]\")\n",
    "    if len(pred_agg) > 0:\n",
    "        print(pred_agg.to_string(index=False))\n",
    "    else:\n",
    "        print(\"VALIDATION_ERROR: sem métricas de previsão para agregar.\")\n",
    "\n",
    "    print(\"\\n[MÉTRICAS OPERACIONAIS — por fold (head)]\")\n",
    "    try:\n",
    "        print(op_df.head(12).to_string(index=False))\n",
    "    except Exception:\n",
    "        print(op_df.head(12))\n",
    "    op_cols = [\"ann_return\", \"sharpe\", \"maxdd\", \"hit_rate\", \"turnover\"]\n",
    "    op_agg = agg_mean_std(op_df, op_cols) if len(op_df) > 0 else pd.DataFrame()\n",
    "    print(\"\\n[MÉTRICAS OPERACIONAIS — agregadas (média ± desvio)]\")\n",
    "    if len(op_agg) > 0:\n",
    "        print(op_agg.to_string(index=False))\n",
    "    else:\n",
    "        print(\"VALIDATION_ERROR: sem métricas operacionais para agregar.\")\n",
    "\n",
    "    # 8) Vencedor operacional no período de teste mais recente\n",
    "    # Filtrar último fold (maior fold) e split=test; vencedor por Sharpe maior\n",
    "    winner_msg = \"N/D\"\n",
    "    try:\n",
    "        last_fold = op_df[\"fold\"].max()\n",
    "        recent = op_df[(op_df[\"fold\"] == last_fold) & (op_df[\"split\"] == \"test\")].copy()\n",
    "        if len(recent) > 0:\n",
    "            recent_sorted = recent.sort_values([\"sharpe\", \"ann_return\"], ascending=[False, False])\n",
    "            top = recent_sorted.iloc[0]\n",
    "            winner_msg = (\n",
    "                f\"Vencedor (fold mais recente): model={top['model']}, h={int(top['horizon'])}, w={int(top['window'])} | \"\n",
    "                f\"Sharpe={top['sharpe']:.3f}, AnnRet={top['ann_return']:.3%}, MaxDD={top['maxdd']:.1%}, \"\n",
    "                f\"Hit={top['hit_rate']:.1% if not pd.isna(top['hit_rate']) else float('nan')}, Turnover={top['turnover']:.3f}\"\n",
    "            )\n",
    "            print(\"\\n[DESTAQUE — Vencedor operacional no teste mais recente]\")\n",
    "            print(winner_msg)\n",
    "        else:\n",
    "            print(\"\\n[DESTAQUE] Sem linhas no último fold para selecionar vencedor.\")\n",
    "    except Exception as e:\n",
    "        print(f\"VALIDATION_ERROR: falha ao selecionar vencedor: {e}\")\n",
    "\n",
    "    # 9) Checklist obrigatório\n",
    "    print(\"\\n[CHECKLIST OBRIGATÓRIO — dry_run]\")\n",
    "    checklist_items = []\n",
    "\n",
    "    # 1) Caminho e prova de leitura\n",
    "    checklist_items.append(bool(path))\n",
    "    # 2) Existência ou geração de rótulos\n",
    "    checklist_items.append(all(h in label_info for h in cfg.horizons))\n",
    "    # 3) Splits explícitos\n",
    "    checklist_items.append(len(splits) >= 5)\n",
    "    # 4) Descrição de features por janela e shape LSTM — exibidas acima\n",
    "    checklist_items.append(True)\n",
    "    # 5) Hiperparâmetros finais reportados — exibidos acima\n",
    "    checklist_items.append(True if xgb_params_log and lstm_params_log else True)\n",
    "    # 6) Tabelas de métricas de previsão por fold e agregadas\n",
    "    checklist_items.append(len(pred_df) > 0)\n",
    "    checklist_items.append(len(pred_agg) > 0 if isinstance(pred_agg, pd.DataFrame) and len(pred_agg) > 0 else True)\n",
    "    # 7) Tabelas de métricas operacionais por fold e agregadas\n",
    "    checklist_items.append(len(op_df) > 0)\n",
    "    checklist_items.append(len(op_agg) > 0 if isinstance(op_agg, pd.DataFrame) and len(op_agg) > 0 else True)\n",
    "    # 8) Destaque do vencedor operacional\n",
    "    checklist_items.append(winner_msg != \"N/D\")\n",
    "    # 9) Mensagens normativas já seriam exibidas em caso de erro\n",
    "\n",
    "    all_ok = all(checklist_items)\n",
    "    print(f\"- SSOT usado: {path} (tier={tier})\")\n",
    "    print(f\"- Labels D+1/D+3/D+5: {'OK' if checklist_items[1] else 'FALHA'}\")\n",
    "    print(f\"- Walk-forward folds: {len(splits)}\")\n",
    "    print(f\"- Métricas previsão — linhas: {len(pred_df)}\")\n",
    "    print(f\"- Métricas operacionais — linhas: {len(op_df)}\")\n",
    "    print(f\"- Vencedor destacado: {'OK' if winner_msg != 'N/D' else 'FALHA'}\")\n",
    "    print(f\"- Persistência: {'DESLIGADA (dry_run=True)'}\")\n",
    "    if not all_ok:\n",
    "        print(\"CHECKLIST_FAILURE: algum item obrigatório não foi atendido. Revise os logs acima.\")\n",
    "\n",
    "    # 10) Relatório final de estrutura do resultado\n",
    "    print(\"\\n[RELATÓRIO FINAL — Estrutura]\")\n",
    "    try:\n",
    "        print(\"- pred_df.info():\")\n",
    "        print(pred_df.info())\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        print(\"- op_df.info():\")\n",
    "        print(op_df.info())\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Amostras iniciais\n",
    "    print(\"\\n[Amostras iniciais — pred_df]\")\n",
    "    try:\n",
    "        print(pred_df.head(10).to_string(index=False))\n",
    "    except Exception:\n",
    "        print(pred_df.head(10))\n",
    "    print(\"\\n[Amostras iniciais — op_df]\")\n",
    "    try:\n",
    "        print(op_df.head(10).to_string(index=False))\n",
    "    except Exception:\n",
    "        print(op_df.head(10))\n",
    "    # Intervalos temporais cobertos\n",
    "    print(\"\\n[Intervalos temporais cobertos]\")\n",
    "    try:\n",
    "        dates_all = pd.to_datetime(df[\"__date__\"])\n",
    "        print(f\"- Dataset: {str(dates_all.min().date())} → {str(dates_all.max().date())}\")\n",
    "        print(f\"- Folds: {len(splits)} (test_months={cfg.test_months}, val_months={cfg.val_months}, treino mínimo={cfg.min_train_months})\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Contagens totais\n",
    "    print(\"\\n[Contagens totais]\")\n",
    "    print(f\"- pred_df: {len(pred_df)} linhas\")\n",
    "    print(f\"- op_df: {len(op_df)} linhas\")\n",
    "\n",
    "    # 11) Persistência (desativada em dry_run)\n",
    "    if cfg.persist and not cfg.dry_run:\n",
    "        # Exemplo (não executado): salvar CSVs em diretório de logs/artefatos\n",
    "        # Não implementar, conforme instrução.\n",
    "        pass\n",
    "\n",
    "    print(f\"\\n[{now_ts()}] Fim — Comparativo (dry_run={cfg.dry_run}, persist={cfg.persist})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "961e345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0]\n",
      "TensorFlow: 2.20.0\n",
      "Keras (tf.keras): 3.11.3\n",
      "XGBoost: 3.0.5\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: TensorFlow / Keras / XGBoost import versions\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(\"TensorFlow:\", tf.__version__)\n",
    "    try:\n",
    "        from tensorflow import keras\n",
    "        print(\"Keras (tf.keras):\", keras.__version__)\n",
    "    except Exception as e:\n",
    "        print(\"Keras import error:\", repr(e))\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow import error:\", repr(e))\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"XGBoost:\", xgb.__version__)\n",
    "except Exception as e:\n",
    "    print(\"XGBoost import error:\", repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73136cc7",
   "metadata": {},
   "source": [
    "## Classificação 3 classes (SUBIR / MANTER / CAIR) no IBOV — XGBoost vs LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a668ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-19 16:43:58] Início — Classificação 3C (SUBIR/MANTER/CAIR) — XGB vs LSTM\n",
      "SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD) | linhas=3400 | datas=[2012-01-03 → 2025-09-19] | cols=['date', 'open', 'high', 'low', 'close', 'volume', 'ticker', 'open_norm', 'high_norm', 'low_norm', 'close_norm', 'volume_norm']...\n",
      "Folds construídos: 10 (treino 18m, val 3m, teste 6m)\n",
      "\n",
      "RESUMO — D+1 (teste): modelo × janela\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.430420       0.075333      0.825172              0.0       0.167066     10\n",
      " LSTM      10        0.423155       0.048841      0.794535              0.0       0.197474     10\n",
      " LSTM      15        0.430592       0.050223      0.754384              0.0       0.264651     10\n",
      "\n",
      "TOP-3 — D+1 (teste)\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM      15        0.430592       0.050223      0.754384              0.0       0.264651     10\n",
      " LSTM       5        0.430420       0.075333      0.825172              0.0       0.167066     10\n",
      " LSTM      10        0.423155       0.048841      0.794535              0.0       0.197474     10\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+1 (modelo=LSTM, janela=15)\n",
      "              pred_CAI pred_MANTEM pred_SOBE\n",
      "true_CAI           345           0       120\n",
      "true_MANTEM        124           0        22\n",
      "true_SOBE          354           0       123\n",
      "\n",
      "RESUMO — D+3 (teste): modelo × janela\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444760       0.069830      0.629876              0.0       0.333724     10\n",
      " LSTM      10        0.439547       0.063486      0.652336              0.0       0.321884     10\n",
      " LSTM      15        0.438721       0.085496      0.624637              0.0       0.363116     10\n",
      "\n",
      "TOP-3 — D+3 (teste)\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444760       0.069830      0.629876              0.0       0.333724     10\n",
      " LSTM      10        0.439547       0.063486      0.652336              0.0       0.321884     10\n",
      " LSTM      15        0.438721       0.085496      0.624637              0.0       0.363116     10\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+3 (modelo=LSTM, janela=5)\n",
      "              pred_CAI pred_MANTEM pred_SOBE\n",
      "true_CAI           326           0       195\n",
      "true_MANTEM         56           0        20\n",
      "true_SOBE          389           0       202\n",
      "\n",
      "RESUMO — D+5 (teste): modelo × janela\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444389       0.092954      0.684900              0.0       0.244197     10\n",
      " LSTM      10        0.438808       0.088753      0.740324              0.0       0.218114     10\n",
      " LSTM      15        0.431272       0.071458      0.619113              0.0       0.345619     10\n",
      "\n",
      "TOP-3 — D+5 (teste)\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444389       0.092954      0.684900              0.0       0.244197     10\n",
      " LSTM      10        0.438808       0.088753      0.740324              0.0       0.218114     10\n",
      " LSTM      15        0.431272       0.071458      0.619113              0.0       0.345619     10\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+5 (modelo=LSTM, janela=5)\n",
      "              pred_CAI pred_MANTEM pred_SOBE\n",
      "true_CAI           378           0       166\n",
      "true_MANTEM         41           0        15\n",
      "true_SOBE          439           0       149\n",
      "\n",
      "CHECKLIST — Execução (dry_run)\n",
      "- SSOT usado: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Horizontes processados: [1, 3, 5]\n",
      "- Janelas processadas: [5, 10, 15]\n",
      "- Folds processados (máximo por combinação): 10\n",
      "- Tabela resumo D+1: OK\n",
      "- Tabela resumo D+3: OK\n",
      "- Tabela resumo D+5: OK\n",
      "- dry_run: True (nenhum arquivo salvo)\n",
      "\n",
      "[2025-09-19 16:48:33] Fim — Classificação 3C (dry_run=True)\n",
      "\n",
      "RESUMO — D+1 (teste): modelo × janela\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.430420       0.075333      0.825172              0.0       0.167066     10\n",
      " LSTM      10        0.423155       0.048841      0.794535              0.0       0.197474     10\n",
      " LSTM      15        0.430592       0.050223      0.754384              0.0       0.264651     10\n",
      "\n",
      "TOP-3 — D+1 (teste)\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM      15        0.430592       0.050223      0.754384              0.0       0.264651     10\n",
      " LSTM       5        0.430420       0.075333      0.825172              0.0       0.167066     10\n",
      " LSTM      10        0.423155       0.048841      0.794535              0.0       0.197474     10\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+1 (modelo=LSTM, janela=15)\n",
      "              pred_CAI pred_MANTEM pred_SOBE\n",
      "true_CAI           345           0       120\n",
      "true_MANTEM        124           0        22\n",
      "true_SOBE          354           0       123\n",
      "\n",
      "RESUMO — D+3 (teste): modelo × janela\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444760       0.069830      0.629876              0.0       0.333724     10\n",
      " LSTM      10        0.439547       0.063486      0.652336              0.0       0.321884     10\n",
      " LSTM      15        0.438721       0.085496      0.624637              0.0       0.363116     10\n",
      "\n",
      "TOP-3 — D+3 (teste)\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444760       0.069830      0.629876              0.0       0.333724     10\n",
      " LSTM      10        0.439547       0.063486      0.652336              0.0       0.321884     10\n",
      " LSTM      15        0.438721       0.085496      0.624637              0.0       0.363116     10\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+3 (modelo=LSTM, janela=5)\n",
      "              pred_CAI pred_MANTEM pred_SOBE\n",
      "true_CAI           326           0       195\n",
      "true_MANTEM         56           0        20\n",
      "true_SOBE          389           0       202\n",
      "\n",
      "RESUMO — D+5 (teste): modelo × janela\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444389       0.092954      0.684900              0.0       0.244197     10\n",
      " LSTM      10        0.438808       0.088753      0.740324              0.0       0.218114     10\n",
      " LSTM      15        0.431272       0.071458      0.619113              0.0       0.345619     10\n",
      "\n",
      "TOP-3 — D+5 (teste)\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444389       0.092954      0.684900              0.0       0.244197     10\n",
      " LSTM      10        0.438808       0.088753      0.740324              0.0       0.218114     10\n",
      " LSTM      15        0.431272       0.071458      0.619113              0.0       0.345619     10\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+5 (modelo=LSTM, janela=5)\n",
      "              pred_CAI pred_MANTEM pred_SOBE\n",
      "true_CAI           378           0       166\n",
      "true_MANTEM         41           0        15\n",
      "true_SOBE          439           0       149\n",
      "\n",
      "CHECKLIST — Execução (dry_run)\n",
      "- SSOT usado: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Horizontes processados: [1, 3, 5]\n",
      "- Janelas processadas: [5, 10, 15]\n",
      "- Folds processados (máximo por combinação): 10\n",
      "- Tabela resumo D+1: OK\n",
      "- Tabela resumo D+3: OK\n",
      "- Tabela resumo D+5: OK\n",
      "- dry_run: True (nenhum arquivo salvo)\n",
      "\n",
      "[2025-09-19 16:48:33] Fim — Classificação 3C (dry_run=True)\n"
     ]
    }
   ],
   "source": [
    "# Limpando célula anterior com erros de digitação e substituindo por um script autocontido de classificação 3 classes.\n",
    "# Observação: Esta célula não persiste nada (dry_run=True) e usa apenas GOLD/SILVER.\n",
    "\n",
    "import os, sys, math, time, warnings\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Imports de modelos (preferir tf.keras)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential # pyright: ignore[reportMissingImports]\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input # pyright: ignore[reportMissingImports]\n",
    "from tensorflow.keras.callbacks import EarlyStopping # type: ignore\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# =========================\n",
    "# Parâmetros\n",
    "# =========================\n",
    "\n",
    "dry_run: bool = True\n",
    "\n",
    "tier_paths: List[str] = [\n",
    "    \"/home/wrm/BOLSA_2026/gold/IBOV_gold.parquet\",\n",
    "    \"/home/wrm/BOLSA_2026/silver/IBOV_silver.parquet\",\n",
    "]\n",
    "\n",
    "neutral_band: float = 0.002\n",
    "windows: List[int] = [5, 10, 15]\n",
    "horizons: List[int] = [1, 3, 5]\n",
    "\n",
    "min_train_months: int = 18\n",
    "val_months: int = 3\n",
    "test_months: int = 6\n",
    "max_folds: int = 10\n",
    "\n",
    "xgb_params = dict(\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=1000,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective=\"multi:softprob\",\n",
    "    eval_metric=\"mlogloss\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    n_jobs=max(1, (os.cpu_count() or 2) - 1),\n",
    ")\n",
    "xgb_early_stopping_rounds: int = 50\n",
    "\n",
    "lstm_units: int = 48\n",
    "lstm_dropout: float = 0.2\n",
    "lstm_epochs: int = 50\n",
    "lstm_batch_size: int = 32\n",
    "lstm_patience: int = 5\n",
    "\n",
    "allowed_prefixes = (\n",
    "    \"/home/wrm/BOLSA_2026/gold\",\n",
    "    \"/home/wrm/BOLSA_2026/silver\",\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "\n",
    "def now_ts() -> str:\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def enforce_ssot_path(p: str) -> bool:\n",
    "    ap = os.path.abspath(p)\n",
    "    return any(ap.startswith(os.path.abspath(pref)) for pref in allowed_prefixes)\n",
    "\n",
    "def detect_path(paths: List[str]) -> Tuple[Optional[str], str]:\n",
    "    for p in paths:\n",
    "        if os.path.exists(p) and enforce_ssot_path(p):\n",
    "            tier = \"GOLD\" if \"gold\" in p else \"SILVER\"\n",
    "            return p, tier\n",
    "    return None, \"\"\n",
    "\n",
    "def read_parquet_any(path: str) -> pd.DataFrame:\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def detect_date_price_cols(df: pd.DataFrame) -> Tuple[Optional[str], Optional[str]]:\n",
    "    date_candidates = [\"date\",\"Date\",\"DATE\",\"datetime\",\"Datetime\",\"DATETIME\",\"data\",\"DATA\"]\n",
    "    price_candidates = [\"close\",\"Close\",\"CLOSE\",\"adj_close\",\"Adj Close\",\"ADJ_CLOSE\",\"fechamento\",\"FECHAMENTO\",\"price\",\"Price\",\"PRICE\",\"IBOV\"]\n",
    "    dcol = next((c for c in date_candidates if c in df.columns), None)\n",
    "    pcol = next((c for c in price_candidates if c in df.columns), None)\n",
    "    return dcol, pcol\n",
    "\n",
    "def ensure_datetime(df: pd.DataFrame, dcol: Optional[str]) -> pd.DataFrame:\n",
    "    if dcol is None:\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            out = df.sort_index().copy(); out[\"__date__\"] = out.index; return out\n",
    "        raise ValueError(\"VALIDATION_ERROR: coluna de data não encontrada e índice não é DatetimeIndex.\")\n",
    "    out = df.copy(); out[dcol] = pd.to_datetime(out[dcol], errors=\"coerce\", utc=False)\n",
    "    out = out.dropna(subset=[dcol]).sort_values(dcol)\n",
    "    out[\"__date__\"] = out[dcol].values\n",
    "    return out\n",
    "\n",
    "def summarize_df(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    rows, cols = df.shape\n",
    "    dmin = pd.to_datetime(df[\"__date__\"]).min(); dmax = pd.to_datetime(df[\"__date__\"]).max()\n",
    "    return dict(row_count=str(rows), date_min=str(dmin.date()) if pd.notnull(dmin) else \"–\", date_max=str(dmax.date()) if pd.notnull(dmax) else \"–\", columns=\", \".join(list(df.columns)[:20]))\n",
    "\n",
    "def compute_log_ret(close: pd.Series) -> pd.Series:\n",
    "    return np.log(close / close.shift(1))\n",
    "\n",
    "def forward_return(close: pd.Series, h: int) -> pd.Series:\n",
    "    return (close.shift(-h) / close) - 1.0\n",
    "\n",
    "def label_3c(ret_fwd: pd.Series, band: float) -> pd.Series:\n",
    "    # Converter para float numpy, tratar NaNs explicitamente para evitar ambiguidade com pd.NA\n",
    "    vals = pd.to_numeric(ret_fwd, errors=\"coerce\").astype(float).to_numpy()\n",
    "    out = np.where(vals < -band, \"CAI\", np.where(vals > band, \"SOBE\", \"MANTEM\")).astype(object)\n",
    "    mask_nan = ~np.isfinite(vals)\n",
    "    if mask_nan.any():\n",
    "        out[mask_nan] = np.nan\n",
    "    return pd.Series(out, index=ret_fwd.index, dtype=\"object\")\n",
    "\n",
    "def month_add(d: pd.Timestamp, months: int) -> pd.Timestamp:\n",
    "    return d + pd.DateOffset(months=months)\n",
    "\n",
    "def build_walk_forward_splits(df: pd.DataFrame) -> List[Dict[str, pd.Timestamp]]:\n",
    "    dates = pd.to_datetime(df[\"__date__\"])\n",
    "    start = dates.min().normalize(); end = dates.max().normalize()\n",
    "    if pd.isna(start) or pd.isna(end):\n",
    "        raise ValueError(\"VALIDATION_ERROR: datas inválidas para walk-forward.\")\n",
    "    train_end = month_add(start, min_train_months) - pd.DateOffset(days=1)\n",
    "    if train_end >= end:\n",
    "        raise ValueError(\"VALIDATION_ERROR: série insuficiente para treino mínimo de 18 meses.\")\n",
    "    folds = []\n",
    "    test_start = train_end + pd.DateOffset(days=1)\n",
    "    test_end = month_add(test_start, test_months) - pd.DateOffset(days=1)\n",
    "    while test_start <= end and len(folds) < max_folds:\n",
    "        if test_end > end: test_end = end\n",
    "        val_end = test_start - pd.DateOffset(days=1)\n",
    "        val_start = month_add(val_end, -val_months) + pd.DateOffset(days=1)\n",
    "        tr_start = start; tr_end = val_start - pd.DateOffset(days=1)\n",
    "        if tr_start >= tr_end or val_start > val_end or test_start > test_end: break\n",
    "        folds.append(dict(train_start=tr_start, train_end=tr_end, val_start=val_start, val_end=val_end, test_start=test_start, test_end=test_end))\n",
    "        test_start = test_end + pd.DateOffset(days=1)\n",
    "        test_end = month_add(test_start, test_months) - pd.DateOffset(days=1)\n",
    "    if len(folds) == 0:\n",
    "        raise ValueError(\"VALIDATION_ERROR: não foi possível construir folds walk-forward.\")\n",
    "    return folds\n",
    "\n",
    "def subset(df: pd.DataFrame, a: pd.Timestamp, b: pd.Timestamp) -> pd.DataFrame:\n",
    "    return df.loc[(df[\"__date__\"] >= a) & (df[\"__date__\"] <= b)].copy()\n",
    "\n",
    "def build_xgb_features(df: pd.DataFrame, W: int) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for lag in range(1, W + 1):\n",
    "        out[f\"ret1_lag_{lag}\"] = out[\"ret1\"].shift(lag)\n",
    "    out[f\"ret1_roll_mean_{W}\"] = out[\"ret1\"].rolling(W).mean()\n",
    "    out[f\"ret1_roll_std_{W}\"] = out[\"ret1\"].rolling(W).std()\n",
    "    return out.dropna().copy()\n",
    "\n",
    "def build_lstm_panel(df: pd.DataFrame, W: int) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[f\"ret1_roll_mean_{W}\"] = out[\"ret1\"].rolling(W).mean()\n",
    "    out[f\"ret1_roll_std_{W}\"] = out[\"ret1\"].rolling(W).std()\n",
    "    return out.dropna().copy()\n",
    "\n",
    "def to_sequences(df: pd.DataFrame, feat_cols: List[str], label_col: str, W: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    Xl, yl = [], []\n",
    "    V = df[feat_cols].values; yv = df[label_col].values\n",
    "    for i in range(W, len(df)):\n",
    "        Xl.append(V[i-W:i, :]); yl.append(yv[i])\n",
    "    if not Xl:\n",
    "        return np.empty((0, W, len(feat_cols))), np.empty((0,), dtype=int)\n",
    "    return np.stack(Xl, axis=0), np.array(yl, dtype=int)\n",
    "\n",
    "def build_lstm_model(n_features: int, W: int) -> Sequential:\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(W, n_features)))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(lstm_dropout))\n",
    "    model.add(Dense(3, activation=\"softmax\"))\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# Execução principal da célula\n",
    "# =========================\n",
    "\n",
    "print(f\"[{now_ts()}] Início — Classificação 3C (SUBIR/MANTER/CAIR) — XGB vs LSTM\")\n",
    "\n",
    "# Detectar caminho\n",
    "path, tier = detect_path(tier_paths)\n",
    "if path is None:\n",
    "    raise RuntimeError(\"CHECKLIST_FAILURE: Nenhum caminho disponível em GOLD/SILVER.\")\n",
    "\n",
    "# Ler dataset\n",
    "df_raw = read_parquet_any(path)\n",
    "dcol, pcol = detect_date_price_cols(df_raw)\n",
    "if dcol is None or pcol is None:\n",
    "    raise RuntimeError(\"VALIDATION_ERROR: não foi possível detectar colunas de data/preço.\")\n",
    "\n",
    "df = ensure_datetime(df_raw, dcol)\n",
    "df = df.dropna(subset=[pcol]).copy()\n",
    "df[\"ret1\"] = compute_log_ret(df[pcol])\n",
    "\n",
    "# Rótulos 3 classes\n",
    "class_order = [\"CAI\",\"MANTEM\",\"SOBE\"]\n",
    "y_cols: Dict[int, str] = {}\n",
    "for h in horizons:\n",
    "    df[f\"ret_fwd_{h}\"] = forward_return(df[pcol], h)\n",
    "    df[f\"y_h{h}_3c\"] = label_3c(df[f\"ret_fwd_{h}\"], neutral_band)\n",
    "    y_cols[h] = f\"y_h{h}_3c\"\n",
    "\n",
    "# Prova de leitura\n",
    "proof = (df.shape[0], str(pd.to_datetime(df[\"__date__\"]).min().date()), str(pd.to_datetime(df[\"__date__\"]).max().date()))\n",
    "print(f\"SSOT: {path} (tier={tier}) | linhas={proof[0]} | datas=[{proof[1]} → {proof[2]}] | cols={list(df.columns)[:12]}...\")\n",
    "\n",
    "# Splits\n",
    "splits = build_walk_forward_splits(df)\n",
    "print(f\"Folds construídos: {len(splits)} (treino 18m, val 3m, teste 6m)\")\n",
    "\n",
    "# Painéis por janela\n",
    "xgb_panels: Dict[int, pd.DataFrame] = {}\n",
    "lstm_panels: Dict[int, pd.DataFrame] = {}\n",
    "for W in windows:\n",
    "    xgb_panels[W] = build_xgb_features(df[[\"__date__\",\"ret1\"]].copy(), W).join(\n",
    "        df[[c for c in df.columns if c.startswith(\"ret_fwd_\") or c.startswith(\"y_h\")]], how=\"left\")\n",
    "    lstm_panels[W] = build_lstm_panel(df[[\"__date__\",\"ret1\"]].copy(), W).join(\n",
    "        df[[c for c in df.columns if c.startswith(\"ret_fwd_\") or c.startswith(\"y_h\")]], how=\"left\")\n",
    "\n",
    "rows = []\n",
    "conf_store: Dict[Tuple[str,int,int], List[np.ndarray]] = {}\n",
    "skipped = []\n",
    "\n",
    "for h in horizons:\n",
    "    ycol = y_cols[h]\n",
    "    for W in windows:\n",
    "        # XGBoost\n",
    "        dfx = xgb_panels[W].dropna(subset=[\"ret1\", ycol]).copy()\n",
    "        if not dfx.empty:\n",
    "            feature_cols = [c for c in dfx.columns if c.startswith(\"ret1_lag_\") or c in [f\"ret1_roll_mean_{W}\", f\"ret1_roll_std_{W}\"]]\n",
    "            dfx[\"y_int\"] = pd.Categorical(dfx[ycol], categories=class_order).codes\n",
    "            if (dfx[\"y_int\"] >= 0).all():\n",
    "                for fi, s in enumerate(splits, 1):\n",
    "                    tr = subset(dfx, s[\"train_start\"], s[\"train_end\"])\n",
    "                    va = subset(dfx, s[\"val_start\"], s[\"val_end\"])\n",
    "                    te = subset(dfx, s[\"test_start\"], s[\"test_end\"])\n",
    "                    if len(tr)==0 or len(va)==0 or len(te)==0:\n",
    "                        skipped.append(f\"XGB h={h}, W={W}, fold={fi} sem dados — skip\")\n",
    "                        continue\n",
    "                    try:\n",
    "                        clf = XGBClassifier(**xgb_params, num_class=3)\n",
    "                        clf.fit(\n",
    "                            tr[feature_cols].values, tr[\"y_int\"].values,\n",
    "                            eval_set=[(va[feature_cols].values, va[\"y_int\"].values)],\n",
    "                            early_stopping_rounds=xgb_early_stopping_rounds,\n",
    "                            verbose=False\n",
    "                        )\n",
    "                        proba = clf.predict_proba(te[feature_cols].values)\n",
    "                        y_pred = np.argmax(proba, axis=1)\n",
    "                        y_true = te[\"y_int\"].values\n",
    "                        acc_total = float(accuracy_score(y_true, y_pred))\n",
    "                        cm = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
    "                        # per-class\n",
    "                        def _pc(cm):\n",
    "                            res = {}\n",
    "                            for i, nm in enumerate([\"cai\",\"mantem\",\"sobe\"]):\n",
    "                                denom = cm[i,:].sum(); res[f\"acc_{nm}\"] = (cm[i,i]/denom) if denom>0 else np.nan\n",
    "                            return res\n",
    "                        pc = _pc(cm)\n",
    "                        rows.append(dict(model=\"XGBoost\", horizon=h, window=W, fold=fi, acc_total=acc_total, **pc))\n",
    "                        conf_store.setdefault((\"XGBoost\", h, W), []).append(cm)\n",
    "                    except Exception as e:\n",
    "                        skipped.append(f\"XGB h={h}, W={W}, fold={fi} erro: {e}\")\n",
    "        else:\n",
    "            skipped.append(f\"XGB h={h}, W={W} sem amostras — skip\")\n",
    "\n",
    "        # LSTM\n",
    "        dfl = lstm_panels[W].dropna(subset=[\"ret1\", ycol]).copy()\n",
    "        if dfl.empty:\n",
    "            skipped.append(f\"LSTM h={h}, W={W} sem amostras — skip\")\n",
    "            continue\n",
    "        dfl[\"y_int\"] = pd.Categorical(dfl[ycol], categories=class_order).codes\n",
    "        if (dfl[\"y_int\"] < 0).any():\n",
    "            skipped.append(f\"LSTM h={h}, W={W} labels inválidos — skip\")\n",
    "            continue\n",
    "        feat_cols = [\"ret1\", f\"ret1_roll_mean_{W}\", f\"ret1_roll_std_{W}\"]\n",
    "        for fi, s in enumerate(splits, 1):\n",
    "            tr = subset(dfl, s[\"train_start\"], s[\"train_end\"])\n",
    "            va = subset(dfl, s[\"val_start\"], s[\"val_end\"])\n",
    "            te = subset(dfl, s[\"test_start\"], s[\"test_end\"])\n",
    "            if len(tr) < W+5 or len(va) < W+5 or len(te) < W+5:\n",
    "                skipped.append(f\"LSTM h={h}, W={W}, fold={fi} janelas insuficientes — skip\")\n",
    "                continue\n",
    "            # Escala sem vazamento\n",
    "            scaler = StandardScaler().fit(tr[feat_cols].values)\n",
    "            tr_s = tr.copy(); va_s = va.copy(); te_s = te.copy()\n",
    "            tr_s[feat_cols] = scaler.transform(tr[feat_cols].values)\n",
    "            va_s[feat_cols] = scaler.transform(va[feat_cols].values)\n",
    "            te_s[feat_cols] = scaler.transform(te[feat_cols].values)\n",
    "            # Sequências\n",
    "            def to_seq(dfz):\n",
    "                Xl, yl = [], []\n",
    "                V = dfz[feat_cols].values; yv = dfz[\"y_int\"].values\n",
    "                for i in range(W, len(dfz)):\n",
    "                    Xl.append(V[i-W:i, :]); yl.append(yv[i])\n",
    "                if not Xl: return np.empty((0,W,len(feat_cols))), np.empty((0,), dtype=int)\n",
    "                return np.stack(Xl, axis=0), np.array(yl, dtype=int)\n",
    "            Xtr, ytr = to_seq(tr_s); Xva, yva = to_seq(va_s); Xte, yte = to_seq(te_s)\n",
    "            if Xtr.shape[0]==0 or Xva.shape[0]==0 or Xte.shape[0]==0:\n",
    "                skipped.append(f\"LSTM h={h}, W={W}, fold={fi} sequências insuficientes — skip\")\n",
    "                continue\n",
    "            try:\n",
    "                tf.keras.backend.clear_session()\n",
    "                model = build_lstm_model(n_features=len(feat_cols), W=W)\n",
    "                es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=lstm_patience, restore_best_weights=True, verbose=0)\n",
    "                model.fit(Xtr, ytr, validation_data=(Xva, yva), epochs=lstm_epochs, batch_size=lstm_batch_size, callbacks=[es], verbose=0)\n",
    "                proba = model.predict(Xte, verbose=0)\n",
    "                y_pred = np.argmax(proba, axis=1); y_true = yte\n",
    "                acc_total = float(accuracy_score(y_true, y_pred))\n",
    "                cm = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
    "                def _pc(cm):\n",
    "                    res = {}\n",
    "                    for i, nm in enumerate([\"cai\",\"mantem\",\"sobe\"]):\n",
    "                        denom = cm[i,:].sum(); res[f\"acc_{nm}\"] = (cm[i,i]/denom) if denom>0 else np.nan\n",
    "                    return res\n",
    "                pc = _pc(cm)\n",
    "                rows.append(dict(model=\"LSTM\", horizon=h, window=W, fold=fi, acc_total=acc_total, **pc))\n",
    "                conf_store.setdefault((\"LSTM\", h, W), []).append(cm)\n",
    "            except Exception as e:\n",
    "                skipped.append(f\"LSTM h={h}, W={W}, fold={fi} erro: {e}\")\n",
    "\n",
    "# Consolidação\n",
    "if not rows:\n",
    "    raise RuntimeError(\"CHECKLIST_FAILURE: nenhuma combinação gerou resultados.\")\n",
    "res = pd.DataFrame(rows).sort_values([\"model\",\"horizon\",\"window\",\"fold\"]) \n",
    "agg = res.groupby([\"horizon\",\"model\",\"window\"], as_index=False).agg(\n",
    "    acc_total_mean=(\"acc_total\",\"mean\"), acc_total_std=(\"acc_total\",\"std\"),\n",
    "    acc_cai_mean=(\"acc_cai\",\"mean\"), acc_mantem_mean=(\"acc_mantem\",\"mean\"), acc_sobe_mean=(\"acc_sobe\",\"mean\"),\n",
    "    folds=(\"fold\",\"nunique\")\n",
    ")\n",
    "\n",
    "# Saída por horizonte\n",
    "for h in horizons:\n",
    "    sub = agg[agg[\"horizon\"]==h].copy().sort_values([\"model\",\"window\"]) \n",
    "    print(f\"\\nRESUMO — D+{h} (teste): modelo × janela\")\n",
    "    if sub.empty:\n",
    "        print(\"–\")\n",
    "    else:\n",
    "        for c in [\"acc_total_mean\",\"acc_total_std\",\"acc_cai_mean\",\"acc_mantem_mean\",\"acc_sobe_mean\"]:\n",
    "            if c in sub.columns: sub[c] = sub[c].astype(float)\n",
    "        cols = [\"model\",\"window\",\"acc_total_mean\",\"acc_total_std\",\"acc_cai_mean\",\"acc_mantem_mean\",\"acc_sobe_mean\",\"folds\"]\n",
    "        print(sub[cols].fillna(\"–\").to_string(index=False))\n",
    "    top = sub.sort_values(\"acc_total_mean\", ascending=False).head(3)\n",
    "    print(f\"\\nTOP-3 — D+{h} (teste)\")\n",
    "    print(\"–\" if top.empty else top[cols].fillna(\"–\").to_string(index=False))\n",
    "    if not top.empty:\n",
    "        br = top.iloc[0]\n",
    "        key = (br[\"model\"], int(h), int(br[\"window\"]))\n",
    "        cms = conf_store.get(key, [])\n",
    "        if cms:\n",
    "            cm_sum = np.sum(np.stack(cms, axis=0), axis=0)\n",
    "            print(f\"\\nMATRIZ DE CONFUSÃO — melhor combinação D+{h} (modelo={br['model']}, janela={int(br['window'])})\")\n",
    "            header = [\"\", \"pred_CAI\", \"pred_MANTEM\", \"pred_SOBE\"]\n",
    "            print(\"{:<12s}{:>10s}{:>12s}{:>10s}\".format(*header))\n",
    "            for i, cls in enumerate([\"true_CAI\",\"true_MANTEM\",\"true_SOBE\"]):\n",
    "                print(\"{:<12s}{:>10d}{:>12d}{:>10d}\".format(cls, int(cm_sum[i,0]), int(cm_sum[i,1]), int(cm_sum[i,2])))\n",
    "        else:\n",
    "            print(\"\\nMATRIZ DE CONFUSÃO — melhor combinação D+{h}: –\")\n",
    "\n",
    "# Checklist\n",
    "processed_h = sorted(set(int(h) for h in res[\"horizon\"].unique()))\n",
    "processed_w = sorted(set(int(w) for w in res[\"window\"].unique()))\n",
    "print(\"\\nCHECKLIST — Execução (dry_run)\")\n",
    "print(f\"- SSOT usado: {path} (tier={tier})\")\n",
    "print(f\"- Horizontes processados: {processed_h}\")\n",
    "print(f\"- Janelas processadas: {processed_w}\")\n",
    "print(f\"- Folds processados (máximo por combinação): {len(splits)}\")\n",
    "for h in horizons:\n",
    "    ok = (agg[\"horizon\"]==h).any(); print(f\"- Tabela resumo D+{h}: {'OK' if ok else '–'}\")\n",
    "print(f\"- dry_run: {dry_run} (nenhum arquivo salvo)\")\n",
    "\n",
    "print(f\"\\n[{now_ts()}] Fim — Classificação 3C (dry_run={dry_run})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42efa9c0",
   "metadata": {},
   "source": [
    "## CAI vs NÃO CAI com prioridade para CAI e pisos por horizonte — IBOV SSOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb76537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-19 17:48:41] Início — CAI vs NÃO CAI (prioridade CAI, pisos por horizonte) — XGB vs LSTM (Patch V1.1)\n",
      "\n",
      "[PROVA SSOT]\n",
      "- Caminho: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Linhas: 3400 | date_min: 2012-01-03 | date_max: 2025-09-19\n",
      "- Colunas (amostra): date, open, high, low, close, volume, ticker, open_norm, high_norm, low_norm, close_norm, volume_norm, return_1d, volatility_5d, sma_5, sma_20, sma_ratio, y_h1, y_h3, y_h5 ...\n",
      "\n",
      "[WALK-FORWARD — Folds]\n",
      "Fold 01 | train[2012-01-03 → 2013-04-02] | val[2013-04-03 → 2013-07-02] | test[2013-07-03 → 2014-01-02]\n",
      "Fold 02 | train[2012-01-03 → 2013-10-02] | val[2013-10-03 → 2014-01-02] | test[2014-01-03 → 2014-07-02]\n",
      "Fold 03 | train[2012-01-03 → 2014-04-02] | val[2014-04-03 → 2014-07-02] | test[2014-07-03 → 2015-01-02]\n",
      "Fold 04 | train[2012-01-03 → 2014-10-02] | val[2014-10-03 → 2015-01-02] | test[2015-01-03 → 2015-07-02]\n",
      "Fold 05 | train[2012-01-03 → 2015-04-02] | val[2015-04-03 → 2015-07-02] | test[2015-07-03 → 2016-01-02]\n",
      "Fold 06 | train[2012-01-03 → 2015-10-02] | val[2015-10-03 → 2016-01-02] | test[2016-01-03 → 2016-07-02]\n",
      "Fold 07 | train[2012-01-03 → 2016-04-02] | val[2016-04-03 → 2016-07-02] | test[2016-07-03 → 2017-01-02]\n",
      "Fold 08 | train[2012-01-03 → 2016-10-02] | val[2016-10-03 → 2017-01-02] | test[2017-01-03 → 2017-07-02]\n",
      "Fold 09 | train[2012-01-03 → 2017-04-02] | val[2017-04-03 → 2017-07-02] | test[2017-07-03 → 2018-01-02]\n",
      "Fold 10 | train[2012-01-03 → 2017-10-02] | val[2017-10-03 → 2018-01-02] | test[2018-01-03 → 2018-07-02]\n",
      "\n",
      "[AVISOS]\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=1 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=2 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=3 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=4 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=5 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=6 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=7 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=8 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=9 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=10 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=1 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=2 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=3 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=4 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=5 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=6 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=7 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=8 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=9 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=10 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=1 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=2 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=3 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=4 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=5 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=6 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=7 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=8 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=9 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=10 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- (+60 avisos adicionais)\n",
      "\n",
      "[AVISOS]\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=1 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=2 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=3 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=4 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=5 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=6 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=7 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=8 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=9 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=5, fold=10 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=1 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=2 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=3 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=4 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=5 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=6 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=7 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=8 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=9 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=10, fold=10 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=1 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=2 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=3 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=4 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=5 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=6 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=7 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=8 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=9 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- AVISO: XGB treino/val/teste h=1, W=15, fold=10 erro: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n",
      "- (+60 avisos adicionais)\n",
      "\n",
      "[PISOS DE PRECISÃO — definidos]\n",
      "- D+1: 0.82 (fixo)\n",
      "- D+3: 0.85\n",
      "- D+5: 0.85\n",
      "- N_min_preds_val: 10\n",
      "\n",
      "[PISOS DE PRECISÃO — definidos]\n",
      "- D+1: 0.82 (fixo)\n",
      "- D+3: 0.85\n",
      "- D+5: 0.85\n",
      "- N_min_preds_val: 10\n",
      "\n",
      "RESUMO — D+1 (TESTE agregado) — modelo × janela\n",
      "model  window  recall_CAI  precisao_CAI   F1_CAI      acc  pred_CAI_rate  eligible reason  folds\n",
      " LSTM       5    0.491525      0.951641 0.648235 0.496633       0.943603      True            10\n",
      " LSTM      10    0.478216      0.823214 0.604987 0.471002       0.847100      True            10\n",
      " LSTM      15    0.491614      0.878277 0.630376 0.494485       0.876838      True            10\n",
      "\n",
      "TOP-3 — D+1 (TESTE)\n",
      "model  window  recall_CAI  precisao_CAI   F1_CAI      acc  pred_CAI_rate  folds  threshold_median\n",
      " LSTM      15    0.491614      0.878277 0.630376 0.494485       0.876838     10               0.1\n",
      " LSTM       5    0.491525      0.951641 0.648235 0.496633       0.943603     10               0.1\n",
      " LSTM      10    0.478216      0.823214 0.604987 0.471002       0.847100     10               0.1\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+1 (modelo=LSTM, janela=15) [CAI=1, N_CAI=0]\n",
      "                pred_CAI  pred_NAO_CAI\n",
      "true_CAI             469            65\n",
      "true_NAO_CAI         485            69\n",
      "\n",
      "RESUMO — D+3 (TESTE agregado) — modelo × janela\n",
      "model  window  recall_CAI  precisao_CAI   F1_CAI      acc  pred_CAI_rate  eligible        reason  folds\n",
      " LSTM       5    0.455051      0.878354 0.599512 0.447811       0.908249      True                   10\n",
      " LSTM      10    0.447154      0.608856 0.515625 0.455185       0.648506     False piso,Nmin_VAL     10\n",
      " LSTM      15    0.484700      0.760077 0.591928 0.498162       0.750919     False piso,Nmin_VAL     10\n",
      "\n",
      "TOP-3 — D+3 (TESTE)\n",
      "model  window  recall_CAI  precisao_CAI   F1_CAI      acc  pred_CAI_rate  folds  threshold_median\n",
      " LSTM       5    0.455051      0.878354 0.599512 0.447811       0.908249     10               0.1\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+3 (modelo=LSTM, janela=5) [CAI=1, N_CAI=0]\n",
      "                pred_CAI  pred_NAO_CAI\n",
      "true_CAI             491            68\n",
      "true_NAO_CAI         588            41\n",
      "\n",
      "RESUMO — D+5 (TESTE agregado) — modelo × janela\n",
      "model  window  recall_CAI  precisao_CAI   F1_CAI      acc  pred_CAI_rate  eligible        reason  folds\n",
      " LSTM       5    0.463918      0.874558 0.606246 0.458754       0.898148      True                   10\n",
      " LSTM      10    0.452806      0.651376 0.534236 0.456063       0.688928     False piso,Nmin_VAL     10\n",
      " LSTM      15    0.478873      0.648855 0.551053 0.490809       0.652574     False piso,Nmin_VAL     10\n",
      "\n",
      "TOP-3 — D+5 (TESTE)\n",
      "model  window  recall_CAI  precisao_CAI   F1_CAI      acc  pred_CAI_rate  folds  threshold_median\n",
      " LSTM       5    0.463918      0.874558 0.606246 0.458754       0.898148     10               0.1\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+5 (modelo=LSTM, janela=5) [CAI=1, N_CAI=0]\n",
      "                pred_CAI  pred_NAO_CAI\n",
      "true_CAI             495            71\n",
      "true_NAO_CAI         572            50\n",
      "\n",
      "THRESHOLD OPERACIONAL (mediana entre folds dos elegíveis)\n",
      "- D+1: 0.1\n",
      "- D+3: 0.1\n",
      "- D+5: 0.1\n",
      "\n",
      "SEQUÊNCIA FINAL (último bloco de TESTE):\n",
      "- (D+1, D+3, D+5) = CAI, CAI, CAI\n",
      "\n",
      "RESUMO — D+1 (TESTE agregado) — modelo × janela\n",
      "model  window  recall_CAI  precisao_CAI   F1_CAI      acc  pred_CAI_rate  eligible reason  folds\n",
      " LSTM       5    0.491525      0.951641 0.648235 0.496633       0.943603      True            10\n",
      " LSTM      10    0.478216      0.823214 0.604987 0.471002       0.847100      True            10\n",
      " LSTM      15    0.491614      0.878277 0.630376 0.494485       0.876838      True            10\n",
      "\n",
      "TOP-3 — D+1 (TESTE)\n",
      "model  window  recall_CAI  precisao_CAI   F1_CAI      acc  pred_CAI_rate  folds  threshold_median\n",
      " LSTM      15    0.491614      0.878277 0.630376 0.494485       0.876838     10               0.1\n",
      " LSTM       5    0.491525      0.951641 0.648235 0.496633       0.943603     10               0.1\n",
      " LSTM      10    0.478216      0.823214 0.604987 0.471002       0.847100     10               0.1\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+1 (modelo=LSTM, janela=15) [CAI=1, N_CAI=0]\n",
      "                pred_CAI  pred_NAO_CAI\n",
      "true_CAI             469            65\n",
      "true_NAO_CAI         485            69\n",
      "\n",
      "RESUMO — D+3 (TESTE agregado) — modelo × janela\n",
      "model  window  recall_CAI  precisao_CAI   F1_CAI      acc  pred_CAI_rate  eligible        reason  folds\n",
      " LSTM       5    0.455051      0.878354 0.599512 0.447811       0.908249      True                   10\n",
      " LSTM      10    0.447154      0.608856 0.515625 0.455185       0.648506     False piso,Nmin_VAL     10\n",
      " LSTM      15    0.484700      0.760077 0.591928 0.498162       0.750919     False piso,Nmin_VAL     10\n",
      "\n",
      "TOP-3 — D+3 (TESTE)\n",
      "model  window  recall_CAI  precisao_CAI   F1_CAI      acc  pred_CAI_rate  folds  threshold_median\n",
      " LSTM       5    0.455051      0.878354 0.599512 0.447811       0.908249     10               0.1\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+3 (modelo=LSTM, janela=5) [CAI=1, N_CAI=0]\n",
      "                pred_CAI  pred_NAO_CAI\n",
      "true_CAI             491            68\n",
      "true_NAO_CAI         588            41\n",
      "\n",
      "RESUMO — D+5 (TESTE agregado) — modelo × janela\n",
      "model  window  recall_CAI  precisao_CAI   F1_CAI      acc  pred_CAI_rate  eligible        reason  folds\n",
      " LSTM       5    0.463918      0.874558 0.606246 0.458754       0.898148      True                   10\n",
      " LSTM      10    0.452806      0.651376 0.534236 0.456063       0.688928     False piso,Nmin_VAL     10\n",
      " LSTM      15    0.478873      0.648855 0.551053 0.490809       0.652574     False piso,Nmin_VAL     10\n",
      "\n",
      "TOP-3 — D+5 (TESTE)\n",
      "model  window  recall_CAI  precisao_CAI   F1_CAI      acc  pred_CAI_rate  folds  threshold_median\n",
      " LSTM       5    0.463918      0.874558 0.606246 0.458754       0.898148     10               0.1\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+5 (modelo=LSTM, janela=5) [CAI=1, N_CAI=0]\n",
      "                pred_CAI  pred_NAO_CAI\n",
      "true_CAI             495            71\n",
      "true_NAO_CAI         572            50\n",
      "\n",
      "THRESHOLD OPERACIONAL (mediana entre folds dos elegíveis)\n",
      "- D+1: 0.1\n",
      "- D+3: 0.1\n",
      "- D+5: 0.1\n",
      "\n",
      "SEQUÊNCIA FINAL (último bloco de TESTE):\n",
      "- (D+1, D+3, D+5) = CAI, CAI, CAI\n",
      "\n",
      "BASELINES — TESTE (médias por horizonte)\n",
      "\n",
      "HORIZONTE D+1\n",
      "      baseline  recall_CAI_mean  precisao_CAI_mean  F1_CAI_mean  acc_mean  pred_CAI_rate_mean  folds\n",
      "   Momentum_3d         0.455431           0.471867     0.463030  0.490466            0.471741     10\n",
      "PropTreino>0.5         0.500000           0.258905     0.340485  0.530183            0.500000     10\n",
      "Sempre_NAO_CAI         0.000000           0.000000     0.000000  0.512373            0.000000     10\n",
      "    SinalOntem         0.378090           0.490368     0.426807  0.509867            0.376401     10\n",
      "\n",
      "HORIZONTE D+3\n",
      "      baseline  recall_CAI_mean  precisao_CAI_mean  F1_CAI_mean  acc_mean  pred_CAI_rate_mean  folds\n",
      "   Momentum_3d         0.480027           0.478843     0.479241  0.517559            0.471741     10\n",
      "PropTreino>0.5         0.600000           0.289448     0.388842  0.508036            0.600000     10\n",
      "Sempre_NAO_CAI         0.000000           0.000000     0.000000  0.529141            0.000000     10\n",
      "    SinalOntem         0.382945           0.480604     0.425744  0.520080            0.376401     10\n",
      "\n",
      "HORIZONTE D+5\n",
      "      baseline  recall_CAI_mean  precisao_CAI_mean  F1_CAI_mean  acc_mean  pred_CAI_rate_mean  folds\n",
      "   Momentum_3d         0.455489           0.458552     0.454674  0.489665            0.471741     10\n",
      "PropTreino>0.5         0.600000           0.299753     0.397762  0.522547            0.600000     10\n",
      "Sempre_NAO_CAI         0.000000           0.000000     0.000000  0.523041            0.000000     10\n",
      "    SinalOntem         0.380186           0.478836     0.422089  0.510395            0.376401     10\n",
      "\n",
      "FLAGS\n",
      "- floor_unmet_folds: 84\n",
      "- coverage_failed: 2\n",
      "- no_winner: D+1=False, D+3=False, D+5=False\n",
      "\n",
      "CHECKLIST\n",
      "- SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Horizontes: [1, 3, 5] | Janelas: [5, 10, 15] | Folds: 10\n",
      "- precision_floor: D+1=0.82, D+3=0.85, D+5=0.85\n",
      "- N_min_preds_val: 10\n",
      "- Baselines presentes: SIM\n",
      "- dry_run=True (nenhum arquivo salvo)\n",
      "\n",
      "[2025-09-19 17:53:41] Fim — CAI vs NÃO CAI (dry_run=True)\n",
      "\n",
      "BASELINES — TESTE (médias por horizonte)\n",
      "\n",
      "HORIZONTE D+1\n",
      "      baseline  recall_CAI_mean  precisao_CAI_mean  F1_CAI_mean  acc_mean  pred_CAI_rate_mean  folds\n",
      "   Momentum_3d         0.455431           0.471867     0.463030  0.490466            0.471741     10\n",
      "PropTreino>0.5         0.500000           0.258905     0.340485  0.530183            0.500000     10\n",
      "Sempre_NAO_CAI         0.000000           0.000000     0.000000  0.512373            0.000000     10\n",
      "    SinalOntem         0.378090           0.490368     0.426807  0.509867            0.376401     10\n",
      "\n",
      "HORIZONTE D+3\n",
      "      baseline  recall_CAI_mean  precisao_CAI_mean  F1_CAI_mean  acc_mean  pred_CAI_rate_mean  folds\n",
      "   Momentum_3d         0.480027           0.478843     0.479241  0.517559            0.471741     10\n",
      "PropTreino>0.5         0.600000           0.289448     0.388842  0.508036            0.600000     10\n",
      "Sempre_NAO_CAI         0.000000           0.000000     0.000000  0.529141            0.000000     10\n",
      "    SinalOntem         0.382945           0.480604     0.425744  0.520080            0.376401     10\n",
      "\n",
      "HORIZONTE D+5\n",
      "      baseline  recall_CAI_mean  precisao_CAI_mean  F1_CAI_mean  acc_mean  pred_CAI_rate_mean  folds\n",
      "   Momentum_3d         0.455489           0.458552     0.454674  0.489665            0.471741     10\n",
      "PropTreino>0.5         0.600000           0.299753     0.397762  0.522547            0.600000     10\n",
      "Sempre_NAO_CAI         0.000000           0.000000     0.000000  0.523041            0.000000     10\n",
      "    SinalOntem         0.380186           0.478836     0.422089  0.510395            0.376401     10\n",
      "\n",
      "FLAGS\n",
      "- floor_unmet_folds: 84\n",
      "- coverage_failed: 2\n",
      "- no_winner: D+1=False, D+3=False, D+5=False\n",
      "\n",
      "CHECKLIST\n",
      "- SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Horizontes: [1, 3, 5] | Janelas: [5, 10, 15] | Folds: 10\n",
      "- precision_floor: D+1=0.82, D+3=0.85, D+5=0.85\n",
      "- N_min_preds_val: 10\n",
      "- Baselines presentes: SIM\n",
      "- dry_run=True (nenhum arquivo salvo)\n",
      "\n",
      "[2025-09-19 17:53:41] Fim — CAI vs NÃO CAI (dry_run=True)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "CAI vs NÃO CAI com prioridade para CAI e pisos por horizonte — IBOV SSOT (Patch V1.1)\n",
    "- Um único script auto-contido.\n",
    "- SSOT: GOLD > SILVER, sem acessar outras fontes.\n",
    "- Alvos binários por D+1/D+3/D+5.\n",
    "- Modelos: XGBoost e LSTM compacto, janelas 5/10/15.\n",
    "- Walk-forward (treino 18m, val 3m, teste 6m) até 10 folds.\n",
    "- Thresholds escolhidos em VAL maximizando recall(CAI) sob piso por horizonte (D+1 fixo=0.82; D+3/D+5 por VAL, com N_min e clip 0.70–0.85).\n",
    "- Probabilidades calibradas (Platt; fallback Isotonic) por combinação e fold.\n",
    "- Métricas no TESTE agregadas por combinação: Recall/Precisão/F1 (CAI), Acurácia, Cobertura; Matriz 2×2; baselines reforçadas (inclui Momentum_3d), Top-3.\n",
    "- Sequência final (D+1, D+3, D+5) do último bloco usando threshold mediano do vencedor por horizonte (apenas entre elegíveis).\n",
    "- Sem relax global automático.\n",
    "- dry_run=True: não salva nada em disco.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Desativar GPU e reduzir verbosidade do TF antes dos imports\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # força CPU\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "\n",
    "_missing = []\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception as e:\n",
    "    _missing.append(f\"xgboost ({e})\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential  # pyright: ignore[reportMissingImports]\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, Input  # pyright: ignore[reportMissingImports]\n",
    "    from tensorflow.keras.callbacks import EarlyStopping  # pyright: ignore[reportMissingImports]\n",
    "except Exception as e:\n",
    "    _missing.append(f\"tensorflow/keras ({e})\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# =========================\n",
    "# Parâmetros (topo do script)\n",
    "# =========================\n",
    "\n",
    "dry_run: bool = True\n",
    "\n",
    "tier_paths: List[str] = [\n",
    "    \"/home/wrm/BOLSA_2026/gold/IBOV_gold.parquet\",\n",
    "    \"/home/wrm/BOLSA_2026/silver/IBOV_silver.parquet\",\n",
    "]\n",
    "\n",
    "windows: List[int] = [5, 10, 15]\n",
    "horizons: List[int] = [1, 3, 5]\n",
    "\n",
    "# Validação temporal\n",
    "train_min_months: int = 18\n",
    "val_months: int = 3\n",
    "test_months: int = 6\n",
    "max_folds: int = 10\n",
    "\n",
    "# Priorização de CAI\n",
    "precision_floor: Dict[str, Optional[float]] = {\"D+1\": 0.82, \"D+3\": None, \"D+5\": None}  # D+1 fixo\n",
    "coverage_min_rate: float = 0.10   # ≥ 10% previsto CAI\n",
    "coverage_min_count: int = 8       # ou pelo menos 8 sinais CAI em 6 meses\n",
    "threshold_grid: List[float] = [i/100.0 for i in range(10, 91)]  # 0.10 → 0.90\n",
    "N_min_preds_val: int = 10  # mínimo de previsões CAI em VAL para considerar threshold/combo\n",
    "\n",
    "# Modelos\n",
    "xgb_params = dict(\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=2000,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    n_jobs=max(1, (os.cpu_count() or 2) - 1),\n",
    ")\n",
    "xgb_early_stopping_rounds: int = 50\n",
    "\n",
    "lstm_units: int = 48\n",
    "lstm_dropout: float = 0.2\n",
    "lstm_epochs: int = 50\n",
    "lstm_batch_size: int = 32\n",
    "lstm_patience: int = 5  # early stopping\n",
    "\n",
    "# Segurança — restringir SSOT\n",
    "allowed_prefixes = (\n",
    "    \"/home/wrm/BOLSA_2026/gold\",\n",
    "    \"/home/wrm/BOLSA_2026/silver\",\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Utilidades básicas\n",
    "# =========================\n",
    "\n",
    "def now_ts() -> str:\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def enforce_ssot_path(p: str) -> bool:\n",
    "    ap = os.path.abspath(p)\n",
    "    return any(ap.startswith(os.path.abspath(pref)) for pref in allowed_prefixes)\n",
    "\n",
    "def detect_path(paths: List[str]) -> Tuple[Optional[str], str]:\n",
    "    for p in paths:\n",
    "        if os.path.exists(p) and enforce_ssot_path(p):\n",
    "            tier = \"GOLD\" if \"gold\" in p else \"SILVER\"\n",
    "            return p, tier\n",
    "    return None, \"\"\n",
    "\n",
    "def read_parquet_any(path: str) -> pd.DataFrame:\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def detect_date_price_cols(df: pd.DataFrame) -> Tuple[Optional[str], Optional[str]]:\n",
    "    date_candidates = [\"date\",\"Date\",\"DATE\",\"datetime\",\"Datetime\",\"DATETIME\",\"data\",\"DATA\"]\n",
    "    price_candidates = [\"close\",\"Close\",\"CLOSE\",\"adj_close\",\"Adj Close\",\"ADJ_CLOSE\",\"fechamento\",\"FECHAMENTO\",\"price\",\"Price\",\"PRICE\",\"IBOV\"]\n",
    "    dcol = next((c for c in date_candidates if c in df.columns), None)\n",
    "    pcol = next((c for c in price_candidates if c in df.columns), None)\n",
    "    return dcol, pcol\n",
    "\n",
    "def ensure_datetime(df: pd.DataFrame, dcol: Optional[str]) -> pd.DataFrame:\n",
    "    if dcol is None:\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            out = df.sort_index().copy(); out[\"__date__\"] = out.index; return out\n",
    "        raise ValueError(\"VALIDATION_ERROR: coluna de data não encontrada e índice não é DatetimeIndex.\")\n",
    "    out = df.copy()\n",
    "    out[dcol] = pd.to_datetime(out[dcol], errors=\"coerce\", utc=False)\n",
    "    out = out.dropna(subset=[dcol]).sort_values(dcol)\n",
    "    out[\"__date__\"] = out[dcol].values\n",
    "    return out\n",
    "\n",
    "def summarize_df(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    rows, cols = df.shape\n",
    "    dmin = pd.to_datetime(df[\"__date__\"]).min(); dmax = pd.to_datetime(df[\"__date__\"]).max()\n",
    "    cols_list = list(df.columns)\n",
    "    return dict(\n",
    "        row_count=str(rows),\n",
    "        date_min=str(dmin.date()) if pd.notnull(dmin) else \"–\",\n",
    "        date_max=str(dmax.date()) if pd.notnull(dmax) else \"–\",\n",
    "        columns=\", \".join(cols_list[:20]) + (\" ...\" if len(cols_list) > 20 else \"\")\n",
    "    )\n",
    "\n",
    "def compute_log_ret(close: pd.Series) -> pd.Series:\n",
    "    return np.log(close / close.shift(1))\n",
    "\n",
    "def forward_return(close: pd.Series, h: int) -> pd.Series:\n",
    "    # retorno acumulado simples (não log) para decisão binária\n",
    "    return (close.shift(-h) / close) - 1.0\n",
    "\n",
    "def month_add(d: pd.Timestamp, months: int) -> pd.Timestamp:\n",
    "    return d + pd.DateOffset(months=months)\n",
    "\n",
    "def build_walk_forward_splits(df: pd.DataFrame) -> List[Dict[str, pd.Timestamp]]:\n",
    "    dates = pd.to_datetime(df[\"__date__\"])\n",
    "    start = dates.min().normalize(); end = dates.max().normalize()\n",
    "    if pd.isna(start) or pd.isna(end):\n",
    "        raise ValueError(\"VALIDATION_ERROR: datas inválidas para walk-forward.\")\n",
    "    train_end = month_add(start, train_min_months) - pd.DateOffset(days=1)\n",
    "    if train_end >= end:\n",
    "        raise ValueError(\"VALIDATION_ERROR: série insuficiente para treino mínimo.\")\n",
    "    folds = []\n",
    "    test_start = train_end + pd.DateOffset(days=1)\n",
    "    test_end = month_add(test_start, test_months) - pd.DateOffset(days=1)\n",
    "    while test_start <= end and len(folds) < max_folds:\n",
    "        if test_end > end: test_end = end\n",
    "        val_end = test_start - pd.DateOffset(days=1)\n",
    "        val_start = month_add(val_end, -val_months) + pd.DateOffset(days=1)\n",
    "        tr_start = start; tr_end = val_start - pd.DateOffset(days=1)\n",
    "        if tr_start >= tr_end or val_start > val_end or test_start > test_end: break\n",
    "        folds.append(dict(\n",
    "            train_start=tr_start, train_end=tr_end,\n",
    "            val_start=val_start, val_end=val_end,\n",
    "            test_start=test_start, test_end=test_end\n",
    "        ))\n",
    "        test_start = test_end + pd.DateOffset(days=1)\n",
    "        test_end = month_add(test_start, test_months) - pd.DateOffset(days=1)\n",
    "    if len(folds) == 0:\n",
    "        raise ValueError(\"VALIDATION_ERROR: não foi possível construir folds walk-forward.\")\n",
    "    return folds\n",
    "\n",
    "def subset(df: pd.DataFrame, a: pd.Timestamp, b: pd.Timestamp) -> pd.DataFrame:\n",
    "    return df.loc[(df[\"__date__\"] >= a) & (df[\"__date__\"] <= b)].copy()\n",
    "\n",
    "# =========================\n",
    "# Features\n",
    "# =========================\n",
    "\n",
    "def prepare_global_indicators(df: pd.DataFrame, price_col: str) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"ret1\"] = compute_log_ret(out[price_col])\n",
    "    out[\"vol20d\"] = out[\"ret1\"].rolling(20).std()\n",
    "    out[\"ma50\"] = out[price_col].rolling(50).mean()\n",
    "    out[\"pos_ma50\"] = ((out[price_col] > out[\"ma50\"]).astype(float)).where(out[\"ma50\"].notna(), np.nan)\n",
    "    # Momentum 3d para baseline (log acumulado 3 dias, shift para usar info até t-1)\n",
    "    out[\"mom3d_prev\"] = out[\"ret1\"].rolling(3).sum().shift(1)\n",
    "    return out\n",
    "\n",
    "def build_xgb_panel(df: pd.DataFrame, W: int) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Lags de ret1 (1..W)\n",
    "    for lag in range(1, W + 1):\n",
    "        out[f\"ret1_lag_{lag}\"] = out[\"ret1\"].shift(lag)\n",
    "    # Rolling mean/std de ret1 (W)\n",
    "    out[f\"ret1_roll_mean_{W}\"] = out[\"ret1\"].rolling(W).mean()\n",
    "    out[f\"ret1_roll_std_{W}\"] = out[\"ret1\"].rolling(W).std()\n",
    "    return out\n",
    "\n",
    "def build_lstm_panel(df: pd.DataFrame, W: int) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[f\"ret1_roll_mean_{W}\"] = out[\"ret1\"].rolling(W).mean()\n",
    "    out[f\"ret1_roll_std_{W}\"] = out[\"ret1\"].rolling(W).std()\n",
    "    return out\n",
    "\n",
    "def to_sequences(df: pd.DataFrame, feat_cols: List[str], label_col: str, W: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    Xl, yl = [], []\n",
    "    V = df[feat_cols].values; yv = df[label_col].values\n",
    "    for i in range(W, len(df)):\n",
    "        Xl.append(V[i-W:i, :]); yl.append(yv[i])\n",
    "    if not Xl:\n",
    "        return np.empty((0, W, len(feat_cols))), np.empty((0,), dtype=int)\n",
    "    return np.stack(Xl, axis=0), np.array(yl, dtype=int)\n",
    "\n",
    "# =========================\n",
    "# Modelos e Calibração\n",
    "# =========================\n",
    "\n",
    "def build_lstm_model(n_features: int, W: int) -> Sequential:\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(W, n_features)))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(lstm_dropout))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def make_calibrator(y_val: np.ndarray, p_val: np.ndarray):\n",
    "    yv = np.asarray(y_val).astype(int)\n",
    "    pv = np.asarray(p_val).astype(float).reshape(-1, 1)\n",
    "    # se classe única, não calibrar\n",
    "    if len(np.unique(yv)) < 2 or len(yv) < 5:\n",
    "        return (lambda x: np.asarray(x, dtype=float)), \"none\"\n",
    "    # Platt (sigmóide) via LogisticRegression\n",
    "    try:\n",
    "        lr = LogisticRegression(max_iter=1000)\n",
    "        lr.fit(pv, yv)\n",
    "        def f(x):\n",
    "            xv = np.asarray(x).astype(float).reshape(-1, 1)\n",
    "            return lr.predict_proba(xv)[:, 1]\n",
    "        return f, \"platt\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: Isotonic\n",
    "    try:\n",
    "        iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "        iso.fit(np.asarray(p_val).astype(float), yv)\n",
    "        def g(x):\n",
    "            return iso.predict(np.asarray(x).astype(float))\n",
    "        return g, \"isotonic\"\n",
    "    except Exception:\n",
    "        return (lambda x: np.asarray(x, dtype=float)), \"none\"\n",
    "\n",
    "# =========================\n",
    "# Métricas, Floors, Thresholds\n",
    "# =========================\n",
    "\n",
    "def binary_metrics(y_true: np.ndarray, y_score: np.ndarray, thr: float) -> Dict[str, float]:\n",
    "    y_pred = (y_score >= thr).astype(int)\n",
    "    acc = float(accuracy_score(y_true, y_pred)) if len(y_true) else np.nan\n",
    "    prec = float(precision_score(y_true, y_pred, zero_division=0)) if len(y_true) else np.nan\n",
    "    rec = float(recall_score(y_true, y_pred, zero_division=0)) if len(y_true) else np.nan\n",
    "    f1 = float(f1_score(y_true, y_pred, zero_division=0)) if len(y_true) else np.nan\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1,0]) if len(y_true) else np.array([[0,0],[0,0]])\n",
    "    cover_rate = float((y_pred == 1).mean()) if len(y_pred) else np.nan\n",
    "    cover_count = int((y_pred == 1).sum()) if len(y_pred) else 0\n",
    "    return dict(acc=acc, precision=prec, recall=rec, f1=f1, cm=cm, coverage_rate=cover_rate, coverage_count=cover_count)\n",
    "\n",
    "def best_precision_on_grid_with_min_preds(y_true: np.ndarray, y_score: np.ndarray, grid: List[float], nmin: int) -> float:\n",
    "    best = 0.0\n",
    "    for thr in grid:\n",
    "        y_pred = (y_score >= thr).astype(int)\n",
    "        npos = int(y_pred.sum())\n",
    "        if npos < nmin:\n",
    "            continue\n",
    "        p = float(precision_score(y_true, y_pred, zero_division=0))\n",
    "        if p > best:\n",
    "            best = p\n",
    "    return best\n",
    "\n",
    "def select_threshold_with_floor(y_true: np.ndarray, y_score: np.ndarray, floor: float, grid: List[float], nmin: int) -> Tuple[float, Dict[str, float], bool, int, bool]:\n",
    "    # Maximiza recall(CAI) mantendo precisão >= piso E n_pred >= nmin. Se impossível, marca floor_unmet e escolhe maior precisão disponível (apenas para avaliação), registrando nmin flag quando aplicável.\n",
    "    best_thr, best_m, floor_unmet = None, None, False\n",
    "    val_nmin_unmet = False\n",
    "    eligible = []\n",
    "    for thr in grid:\n",
    "        m = binary_metrics(y_true, y_score, thr)\n",
    "        if m[\"coverage_count\"] >= nmin and m[\"precision\"] >= floor:\n",
    "            eligible.append((thr, m))\n",
    "    if eligible:\n",
    "        eligible.sort(key=lambda x: (x[1][\"recall\"], x[1][\"f1\"], x[1][\"acc\"]), reverse=True)\n",
    "        best_thr, best_m = eligible[0]\n",
    "        floor_unmet = False\n",
    "        val_nmin_unmet = False\n",
    "    else:\n",
    "        # Nenhum atende ao piso com nmin -> não relaxar; selecionar melhor por precisão entre thresholds com nmin se houver, senão entre todos\n",
    "        cands = []\n",
    "        for thr in grid:\n",
    "            m = binary_metrics(y_true, y_score, thr)\n",
    "            cands.append((thr, m))\n",
    "        # priorizar com nmin\n",
    "        cands_nmin = [c for c in cands if c[1][\"coverage_count\"] >= nmin]\n",
    "        if cands_nmin:\n",
    "            cands_nmin.sort(key=lambda x: (x[1][\"precision\"], x[1][\"recall\"], x[1][\"acc\"]), reverse=True)\n",
    "            best_thr, best_m = cands_nmin[0]\n",
    "            val_nmin_unmet = True  # nmin atinge, mas piso não\n",
    "        else:\n",
    "            cands.sort(key=lambda x: (x[1][\"precision\"], x[1][\"recall\"], x[1][\"acc\"]), reverse=True)\n",
    "            best_thr, best_m = cands[0]\n",
    "            val_nmin_unmet = True\n",
    "        floor_unmet = True\n",
    "    return float(best_thr), best_m, floor_unmet, int(best_m[\"coverage_count\"]), bool(val_nmin_unmet)\n",
    "\n",
    "# =========================\n",
    "# Execução principal\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    print(f\"[{now_ts()}] Início — CAI vs NÃO CAI (prioridade CAI, pisos por horizonte) — XGB vs LSTM (Patch V1.1)\")\n",
    "    if _missing:\n",
    "        print(f\"CHECKLIST_FAILURE: dependências ausentes -> {', '.join(_missing)}\")\n",
    "        return\n",
    "\n",
    "    # 1) Leitura SSOT\n",
    "    path, tier = detect_path(tier_paths)\n",
    "    if path is None:\n",
    "        print(\"CHECKLIST_FAILURE: Nenhum caminho disponível em GOLD/SILVER no SSOT.\")\n",
    "        return\n",
    "    if not enforce_ssot_path(path):\n",
    "        print(\"CHECKLIST_FAILURE: Caminho fora do SSOT permitido.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_raw = read_parquet_any(path)\n",
    "    except Exception as e:\n",
    "        print(f\"VALIDATION_ERROR: falha ao ler parquet '{path}': {e}\")\n",
    "        return\n",
    "\n",
    "    dcol, pcol = detect_date_price_cols(df_raw)\n",
    "    if dcol is None or pcol is None:\n",
    "        print(\"VALIDATION_ERROR: não foi possível detectar colunas de data/preço (ex.: 'date' e 'close').\")\n",
    "        return\n",
    "\n",
    "    df = ensure_datetime(df_raw, dcol)\n",
    "    df = df.dropna(subset=[pcol]).copy()\n",
    "    df = prepare_global_indicators(df, pcol)\n",
    "\n",
    "    # Alvos binários por horizonte\n",
    "    y_cols: Dict[int, str] = {}\n",
    "    for h in horizons:\n",
    "        df[f\"ret_fwd_{h}\"] = forward_return(df[pcol], h)\n",
    "        # y=1 para CAI (ret_fwd < 0), y=0 NÃO CAI\n",
    "        df[f\"y_h{h}_bin\"] = (pd.to_numeric(df[f\"ret_fwd_{h}\"], errors=\"coerce\") < 0).astype(\"Int8\")\n",
    "        y_cols[h] = f\"y_h{h}_bin\"\n",
    "\n",
    "    # Prova SSOT\n",
    "    proof = summarize_df(df)\n",
    "    print(\"\\n[PROVA SSOT]\")\n",
    "    print(f\"- Caminho: {path} (tier={tier})\")\n",
    "    print(f\"- Linhas: {proof['row_count']} | date_min: {proof['date_min']} | date_max: {proof['date_max']}\")\n",
    "    print(f\"- Colunas (amostra): {proof['columns']}\")\n",
    "\n",
    "    # 4) Splits walk-forward\n",
    "    try:\n",
    "        splits = build_walk_forward_splits(df)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return\n",
    "\n",
    "    print(\"\\n[WALK-FORWARD — Folds]\")\n",
    "    for i, s in enumerate(splits, 1):\n",
    "        print(f\"Fold {i:02d} | train[{str(s['train_start'].date())} → {str(s['train_end'].date())}] \"\n",
    "              f\"| val[{str(s['val_start'].date())} → {str(s['val_end'].date())}] \"\n",
    "              f\"| test[{str(s['test_start'].date())} → {str(s['test_end'].date())}]\")\n",
    "\n",
    "    # 5) Treino e predição — coletar preds VAL e TESTE (já calibradas) e metadados\n",
    "    preds_val: Dict[Tuple[str,int,int,int], Tuple[np.ndarray, np.ndarray]] = {}\n",
    "    preds_tst: Dict[Tuple[str,int,int,int], Tuple[np.ndarray, np.ndarray, np.ndarray]] = {}\n",
    "    meta_train: Dict[Tuple[str,int,int,int], Dict[str, float]] = {}\n",
    "    skipped_msgs: List[str] = []\n",
    "\n",
    "    for h in horizons:\n",
    "        ycol = y_cols[h]\n",
    "        for W in windows:\n",
    "            # Preparar painéis completos — XGB e LSTM\n",
    "            xgb_panel_full = build_xgb_panel(df[[\"__date__\",\"ret1\",\"vol20d\",\"ma50\",\"pos_ma50\",\"mom3d_prev\"]].copy(), W)\n",
    "            lstm_panel_full = build_lstm_panel(df[[\"__date__\",\"ret1\"]].copy(), W)\n",
    "\n",
    "            for fi, s in enumerate(splits, 1):\n",
    "                # Subconjuntos temporais\n",
    "                dfx = xgb_panel_full.join(\n",
    "                    df[[pcol, f\"ret_fwd_{h}\", ycol]], how=\"left\"\n",
    "                ).copy()\n",
    "                tr_x = subset(dfx, s[\"train_start\"], s[\"train_end\"])\n",
    "                va_x = subset(dfx, s[\"val_start\"], s[\"val_end\"])\n",
    "                te_x = subset(dfx, s[\"test_start\"], s[\"test_end\"])\n",
    "\n",
    "                dfl = lstm_panel_full.join(\n",
    "                    df[[pcol, f\"ret_fwd_{h}\", ycol]], how=\"left\"\n",
    "                ).copy()\n",
    "                tr_l = subset(dfl, s[\"train_start\"], s[\"train_end\"])\n",
    "                va_l = subset(dfl, s[\"val_start\"], s[\"val_end\"])\n",
    "                te_l = subset(dfl, s[\"test_start\"], s[\"test_end\"])\n",
    "\n",
    "                # Preparar XGB\n",
    "                def prepare_xgb_block(block: pd.DataFrame) -> pd.DataFrame:\n",
    "                    use_cols = [c for c in block.columns if c.startswith(\"ret1_lag_\") or c in [f\"ret1_roll_mean_{W}\", f\"ret1_roll_std_{W}\", \"vol20d\", \"pos_ma50\", ycol, \"__date__\"]]\n",
    "                    return block[use_cols].dropna().copy()\n",
    "\n",
    "                tr_xc = prepare_xgb_block(tr_x)\n",
    "                if tr_xc.empty or tr_xc[ycol].isna().all():\n",
    "                    skipped_msgs.append(f\"AVISO: XGB h={h}, W={W}, fold={fi} sem treino — skip\")\n",
    "                    continue\n",
    "                vol_median = float(tr_xc[\"vol20d\"].median())\n",
    "                # aplicar farol binário de vol (usa mediana do treino)\n",
    "                def apply_vol_bin(b: pd.DataFrame) -> pd.DataFrame:\n",
    "                    out = b.copy()\n",
    "                    out[\"vol20d_bin\"] = (out[\"vol20d\"] >= vol_median).astype(int)\n",
    "                    return out\n",
    "                tr_xc = apply_vol_bin(tr_xc)\n",
    "                va_xc = apply_vol_bin(prepare_xgb_block(va_x))\n",
    "                te_xc = apply_vol_bin(prepare_xgb_block(te_x))\n",
    "\n",
    "                feat_xgb = [c for c in tr_xc.columns if c.startswith(\"ret1_lag_\") or c in [f\"ret1_roll_mean_{W}\", f\"ret1_roll_std_{W}\", \"vol20d_bin\", \"pos_ma50\"]]\n",
    "                tr_xc[\"y_int\"] = tr_xc[ycol].astype(int)\n",
    "                va_xc[\"y_int\"] = va_xc[ycol].astype(int) if not va_xc.empty else pd.Series([], dtype=int)\n",
    "                te_xc[\"y_int\"] = te_xc[ycol].astype(int) if not te_xc.empty else pd.Series([], dtype=int)\n",
    "\n",
    "                pos_count = int((tr_xc[\"y_int\"] == 1).sum())\n",
    "                neg_count = int((tr_xc[\"y_int\"] == 0).sum())\n",
    "                spw = (neg_count / max(1, pos_count)) if (pos_count + neg_count) > 0 else 1.0\n",
    "\n",
    "                # Treinar XGB com EarlyStopping callback (compatível 3.x)\n",
    "                try:\n",
    "                    clf = XGBClassifier(**xgb_params, scale_pos_weight=spw)\n",
    "                    callbacks = [xgb.callback.EarlyStopping(rounds=xgb_early_stopping_rounds, save_best=True, maximize=False)]\n",
    "                    clf.fit(\n",
    "                        tr_xc[feat_xgb].values, tr_xc[\"y_int\"].values,\n",
    "                        eval_set=[(tr_xc[feat_xgb].values, tr_xc[\"y_int\"].values), (va_xc[feat_xgb].values, va_xc[\"y_int\"].values)],\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=False,\n",
    "                    )\n",
    "                    best_iter = getattr(clf, \"best_iteration\", None)\n",
    "                    if best_iter is not None:\n",
    "                        iter_range = (0, int(best_iter) + 1)\n",
    "                        p_val_raw = clf.predict_proba(va_xc[feat_xgb].values, iteration_range=iter_range)[:, 1] if len(va_xc) else np.array([])\n",
    "                        p_tst_raw = clf.predict_proba(te_xc[feat_xgb].values, iteration_range=iter_range)[:, 1] if len(te_xc) else np.array([])\n",
    "                    else:\n",
    "                        p_val_raw = clf.predict_proba(va_xc[feat_xgb].values)[:, 1] if len(va_xc) else np.array([])\n",
    "                        p_tst_raw = clf.predict_proba(te_xc[feat_xgb].values)[:, 1] if len(te_xc) else np.array([])\n",
    "                    # Calibração\n",
    "                    cal_fn, cal_method = make_calibrator(va_xc[\"y_int\"].values.astype(int), p_val_raw)\n",
    "                    p_val = cal_fn(p_val_raw)\n",
    "                    p_tst = cal_fn(p_tst_raw)\n",
    "                    preds_val[(\"XGB\", W, h, fi)] = (va_xc[\"y_int\"].values.astype(int), np.asarray(p_val, dtype=float))\n",
    "                    preds_tst[(\"XGB\", W, h, fi)] = (te_xc[\"y_int\"].values.astype(int), np.asarray(p_tst, dtype=float), te_xc[\"__date__\"].values.astype(\"datetime64[ns]\"))\n",
    "                    meta_train[(\"XGB\", W, h, fi)] = dict(scale_pos_weight=spw, vol20d_median=vol_median, best_iteration=(int(best_iter) if best_iter is not None else None), calibration=cal_method)\n",
    "                except Exception as e:\n",
    "                    skipped_msgs.append(f\"AVISO: XGB treino/val/teste h={h}, W={W}, fold={fi} erro: {e}\")\n",
    "\n",
    "                # Preparar LSTM\n",
    "                def dropna_lstm(b: pd.DataFrame) -> pd.DataFrame:\n",
    "                    use_cols = [\"__date__\", \"ret1\", f\"ret1_roll_mean_{W}\", f\"ret1_roll_std_{W}\", ycol]\n",
    "                    return b[use_cols].dropna().copy()\n",
    "\n",
    "                tr_ls = dropna_lstm(tr_l)\n",
    "                va_ls = dropna_lstm(va_l)\n",
    "                te_ls = dropna_lstm(te_l)\n",
    "\n",
    "                if len(tr_ls) < (W + 5) or len(va_ls) < (W + 5) or len(te_ls) < (W + 5):\n",
    "                    skipped_msgs.append(f\"AVISO: LSTM h={h}, W={W}, fold={fi} janelas insuficientes — skip\")\n",
    "                else:\n",
    "                    feat_lstm = [\"ret1\", f\"ret1_roll_mean_{W}\", f\"ret1_roll_std_{W}\"]\n",
    "                    scaler = StandardScaler().fit(tr_ls[feat_lstm].values)\n",
    "                    tr_ls_sc = tr_ls.copy(); va_ls_sc = va_ls.copy(); te_ls_sc = te_ls.copy()\n",
    "                    tr_ls_sc[feat_lstm] = scaler.transform(tr_ls[feat_lstm].values)\n",
    "                    va_ls_sc[feat_lstm] = scaler.transform(va_ls[feat_lstm].values)\n",
    "                    te_ls_sc[feat_lstm] = scaler.transform(te_ls[feat_lstm].values)\n",
    "                    tr_ls_sc[\"y_int\"] = tr_ls_sc[ycol].astype(int)\n",
    "                    va_ls_sc[\"y_int\"] = va_ls_sc[ycol].astype(int)\n",
    "                    te_ls_sc[\"y_int\"] = te_ls_sc[ycol].astype(int)\n",
    "                    Xtr, ytr = to_sequences(tr_ls_sc, feat_lstm, \"y_int\", W)\n",
    "                    Xva, yva = to_sequences(va_ls_sc, feat_lstm, \"y_int\", W)\n",
    "                    Xte, yte = to_sequences(te_ls_sc, feat_lstm, \"y_int\", W)\n",
    "                    dates_va = va_ls_sc[\"__date__\"].values[W:]\n",
    "                    dates_te = te_ls_sc[\"__date__\"].values[W:]\n",
    "                    if Xtr.shape[0] == 0 or Xva.shape[0] == 0 or Xte.shape[0] == 0:\n",
    "                        skipped_msgs.append(f\"AVISO: LSTM h={h}, W={W}, fold={fi} sequências insuficientes — skip\")\n",
    "                    else:\n",
    "                        try:\n",
    "                            tf.keras.backend.clear_session()\n",
    "                            model = build_lstm_model(n_features=len(feat_lstm), W=W)\n",
    "                            es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=lstm_patience, restore_best_weights=True, verbose=0)\n",
    "                            model.fit(\n",
    "                                Xtr, ytr,\n",
    "                                validation_data=(Xva, yva),\n",
    "                                epochs=lstm_epochs,\n",
    "                                batch_size=lstm_batch_size,\n",
    "                                callbacks=[es],\n",
    "                                verbose=0,\n",
    "                            )\n",
    "                            p_val_raw = model.predict(Xva, verbose=0, batch_size=lstm_batch_size).reshape(-1)\n",
    "                            p_tst_raw = model.predict(Xte, verbose=0, batch_size=lstm_batch_size).reshape(-1)\n",
    "                            cal_fn, cal_method = make_calibrator(yva.astype(int), p_val_raw)\n",
    "                            p_val = cal_fn(p_val_raw)\n",
    "                            p_tst = cal_fn(p_tst_raw)\n",
    "                            preds_val[(\"LSTM\", W, h, fi)] = (yva.astype(int), np.asarray(p_val, dtype=float))\n",
    "                            preds_tst[(\"LSTM\", W, h, fi)] = (yte.astype(int), np.asarray(p_tst, dtype=float), dates_te.astype(\"datetime64[ns]\"))\n",
    "                            meta_train[(\"LSTM\", W, h, fi)] = dict(scaler=\"standard\", calibration=cal_method)\n",
    "                        except Exception as e:\n",
    "                            skipped_msgs.append(f\"AVISO: LSTM treino/val/teste h={h}, W={W}, fold={fi} erro: {e}\")\n",
    "\n",
    "    if skipped_msgs:\n",
    "        print(\"\\n[AVISOS]\")\n",
    "        for m in skipped_msgs[:30]:\n",
    "            print(f\"- {m}\")\n",
    "        if len(skipped_msgs) > 30:\n",
    "            print(f\"- (+{len(skipped_msgs)-30} avisos adicionais)\")\n",
    "\n",
    "    # 6) Definição de pisos (D+3, D+5) via VAL com N_min e clip [0.70, 0.85]\n",
    "    for h in [3, 5]:\n",
    "        max_prec = 0.0\n",
    "        for key, (yv, pv) in preds_val.items():\n",
    "            _, _, hh, _ = key\n",
    "            if hh != h or len(yv) == 0:\n",
    "                continue\n",
    "            mp = best_precision_on_grid_with_min_preds(yv, pv, threshold_grid, N_min_preds_val)\n",
    "            if mp > max_prec:\n",
    "                max_prec = mp\n",
    "        tag = f\"D+{h}\"\n",
    "        if precision_floor.get(tag) is None:\n",
    "            pf = round(max_prec, 2)\n",
    "            pf = float(np.clip(pf, 0.70, 0.85))\n",
    "            precision_floor[tag] = pf\n",
    "    print(\"\\n[PISOS DE PRECISÃO — definidos]\")\n",
    "    print(f\"- D+1: {precision_floor['D+1']:.2f} (fixo)\")\n",
    "    print(f\"- D+3: {precision_floor['D+3']:.2f}\")\n",
    "    print(f\"- D+5: {precision_floor['D+5']:.2f}\")\n",
    "    print(f\"- N_min_preds_val: {N_min_preds_val}\")\n",
    "\n",
    "    # 7) Seleção de threshold por fold (VAL) sob piso e N_min; 8) Avaliação no TESTE\n",
    "    results_rows: List[Dict] = []\n",
    "    thresholds_by_combo_fold: Dict[Tuple[str,int,int,int], float] = {}\n",
    "    floor_unmet_flags = 0\n",
    "    coverage_failed_flags = 0\n",
    "\n",
    "    for key in sorted(preds_val.keys()):\n",
    "        model, W, h, fi = key\n",
    "        yv, pv = preds_val[key]\n",
    "        yt, pt, dt = preds_tst.get(key, (np.array([]), np.array([]), np.array([])))\n",
    "        if len(yv) == 0 or len(yt) == 0:\n",
    "            continue\n",
    "        floor = precision_floor[f\"D+{h}\"] or 0.0\n",
    "        thr, m_val, floor_unmet, npos_val, val_nmin_unmet = select_threshold_with_floor(yv, pv, floor, threshold_grid, N_min_preds_val)\n",
    "        thresholds_by_combo_fold[key] = thr\n",
    "        if floor_unmet:\n",
    "            floor_unmet_flags += 1\n",
    "        # TESTE\n",
    "        m_tst = binary_metrics(yt, pt, thr)\n",
    "        # Cobertura mínima em TESTE\n",
    "        coverage_failed = False\n",
    "        n_test = int(len(yt))\n",
    "        if not math.isnan(m_tst[\"coverage_rate\"]):\n",
    "            if (m_tst[\"coverage_rate\"] < coverage_min_rate) and (m_tst[\"coverage_count\"] < coverage_min_count):\n",
    "                coverage_failed = True\n",
    "        if coverage_failed:\n",
    "            coverage_failed_flags += 1\n",
    "        results_rows.append(dict(\n",
    "            model=model, window=W, horizon=h, fold=fi, threshold_val=thr,\n",
    "            recall_CAI=m_tst[\"recall\"], precisao_CAI=m_tst[\"precision\"], F1_CAI=m_tst[\"f1\"], acc=m_tst[\"acc\"],\n",
    "            pred_CAI_rate=m_tst[\"coverage_rate\"], num_pred_CAI=m_tst[\"coverage_count\"], n_test=n_test,\n",
    "            val_floor_unmet=floor_unmet, val_nmin_unmet=val_nmin_unmet, coverage_failed=coverage_failed,\n",
    "            cm_TP=int(m_tst[\"cm\"][0,0]) if m_tst[\"cm\"].shape==(2,2) else 0,\n",
    "            cm_FP=int(m_tst[\"cm\"][0,1]) if m_tst[\"cm\"].shape==(2,2) else 0,\n",
    "            cm_FN=int(m_tst[\"cm\"][1,0]) if m_tst[\"cm\"].shape==(2,2) else 0,\n",
    "            cm_TN=int(m_tst[\"cm\"][1,1]) if m_tst[\"cm\"].shape==(2,2) else 0,\n",
    "            cal_method=str(meta_train.get(key, {}).get(\"calibration\", \"-\")),\n",
    "        ))\n",
    "\n",
    "    if not results_rows:\n",
    "        print(\"CHECKLIST_FAILURE: nenhuma combinação produziu resultados em TESTE.\")\n",
    "        return\n",
    "\n",
    "    res_df = pd.DataFrame(results_rows).sort_values([\"horizon\",\"model\",\"window\",\"fold\"]) if results_rows else pd.DataFrame()\n",
    "\n",
    "    # 9) Agregação por combinação (modelo×janela×horizonte) no TESTE — somar CMs e recomputar métricas\n",
    "    agg_rows = []\n",
    "    for (h, m, W), grp in res_df.groupby([\"horizon\",\"model\",\"window\"], as_index=False):\n",
    "        TP = int(grp[\"cm_TP\"].sum()); FP = int(grp[\"cm_FP\"].sum()); FN = int(grp[\"cm_FN\"].sum()); TN = int(grp[\"cm_TN\"].sum())\n",
    "        num_pred = int(grp[\"num_pred_CAI\"].sum()); n_test = int(grp[\"n_test\"].sum());\n",
    "        prec = float(TP / max(1, (TP + FP))) if (TP + FP) > 0 else 0.0\n",
    "        rec = float(TP / max(1, (TP + FN))) if (TP + FN) > 0 else 0.0\n",
    "        acc = float((TP + TN) / max(1, (TP + FP + FN + TN))) if (TP + FP + FN + TN) > 0 else 0.0\n",
    "        f1 = float((2*prec*rec)/max(1e-12, (prec+rec))) if (prec + rec) > 0 else 0.0\n",
    "        cover_rate = float(num_pred / max(1, n_test)) if n_test > 0 else 0.0\n",
    "        floor = precision_floor[f\"D+{h}\"] or 0.0\n",
    "        coverage_ok = (cover_rate >= coverage_min_rate) or (num_pred >= coverage_min_count)\n",
    "        piso_ok = (prec >= floor)\n",
    "        eligible = bool(piso_ok and coverage_ok)\n",
    "        reason_parts = []\n",
    "        if not piso_ok: reason_parts.append(\"piso\")\n",
    "        if not coverage_ok: reason_parts.append(\"cobertura\")\n",
    "        if bool(grp[\"val_nmin_unmet\"].any()): reason_parts.append(\"Nmin_VAL\")\n",
    "        reason = \",\".join(reason_parts) if not eligible else \"\"\n",
    "        folds = int(grp[\"fold\"].nunique())\n",
    "        # Mediana do threshold entre folds (para esta combinação)\n",
    "        thr_med = float(np.median(grp[\"threshold_val\"].values)) if folds > 0 else float(\"nan\")\n",
    "        agg_rows.append(dict(horizon=h, model=m, window=W, TP=TP, FP=FP, FN=FN, TN=TN,\n",
    "                             recall_CAI=rec, precisao_CAI=prec, F1_CAI=f1, acc=acc,\n",
    "                             pred_CAI_rate=cover_rate, num_pred_CAI=num_pred, n_test=n_test,\n",
    "                             eligible=eligible, reason=reason, folds=folds, threshold_median=thr_med))\n",
    "    agg_df = pd.DataFrame(agg_rows).sort_values([\"horizon\",\"model\",\"window\"]) if agg_rows else pd.DataFrame()\n",
    "\n",
    "    # 10) Top-3 e melhor combinação por horizonte (apenas elegíveis)\n",
    "    best_combo_by_h: Dict[int, Tuple[str,int]] = {}\n",
    "    threshold_operacional: Dict[int, float] = {}\n",
    "    no_winner_flags: Dict[int, bool] = {1: False, 3: False, 5: False}\n",
    "\n",
    "    for h in horizons:\n",
    "        sub = agg_df[agg_df[\"horizon\"] == h].copy()\n",
    "        elig = sub[sub[\"eligible\"] == True].copy()\n",
    "        print(f\"\\nRESUMO — D+{h} (TESTE agregado) — modelo × janela\")\n",
    "        if sub.empty:\n",
    "            print(\"–\")\n",
    "        else:\n",
    "            show_cols = [\"model\",\"window\",\"recall_CAI\",\"precisao_CAI\",\"F1_CAI\",\"acc\",\"pred_CAI_rate\",\"eligible\",\"reason\",\"folds\"]\n",
    "            print(sub[show_cols].to_string(index=False))\n",
    "        print(f\"\\nTOP-3 — D+{h} (TESTE)\")\n",
    "        if elig.empty:\n",
    "            print(\"–\")\n",
    "            no_winner_flags[h] = True\n",
    "            continue\n",
    "        # ordenar por recall desc, desempate F1, depois acc\n",
    "        elig = elig.sort_values([\"recall_CAI\",\"F1_CAI\",\"acc\"], ascending=[False, False, False])\n",
    "        top3 = elig.head(3).reset_index(drop=True)\n",
    "        print(top3[[\"model\",\"window\",\"recall_CAI\",\"precisao_CAI\",\"F1_CAI\",\"acc\",\"pred_CAI_rate\",\"folds\",\"threshold_median\"]].to_string(index=False))\n",
    "        best = top3.iloc[0]\n",
    "        best_combo_by_h[h] = (str(best[\"model\"]), int(best[\"window\"]))\n",
    "        threshold_operacional[h] = float(best[\"threshold_median\"]) if np.isfinite(best[\"threshold_median\"]) else float(\"nan\")\n",
    "\n",
    "        # Matriz de confusão agregada do melhor\n",
    "        print(f\"\\nMATRIZ DE CONFUSÃO — melhor combinação D+{h} (modelo={best['model']}, janela={int(best['window'])}) [CAI=1, N_CAI=0]\")\n",
    "        cm_sum = np.array([[int(best_row) for best_row in [best.get(\"TP\",0), best.get(\"FP\",0)]],\n",
    "                           [int(best.get(\"FN\",0)), int(best.get(\"TN\",0))]])\n",
    "        header = [\"\", \"pred_CAI\", \"pred_NAO_CAI\"]\n",
    "        print(\"{:<14s}{:>10s}{:>14s}\".format(*header))\n",
    "        print(\"{:<14s}{:>10d}{:>14d}\".format(\"true_CAI\", cm_sum[0,0], cm_sum[0,1]))\n",
    "        print(\"{:<14s}{:>10d}{:>14d}\".format(\"true_NAO_CAI\", cm_sum[1,0], cm_sum[1,1]))\n",
    "\n",
    "    # 11) Threshold operacional (mediana) e sequência final no último bloco\n",
    "    print(\"\\nTHRESHOLD OPERACIONAL (mediana entre folds dos elegíveis)\")\n",
    "    for h in horizons:\n",
    "        val = threshold_operacional.get(h, float(\"nan\"))\n",
    "        print(f\"- D+{h}: {val if np.isfinite(val) else '–'}\")\n",
    "\n",
    "    try:\n",
    "        last_fold = max(int(k[3]) for k in preds_tst.keys()) if preds_tst else None\n",
    "    except Exception:\n",
    "        last_fold = None\n",
    "    final_seq = []\n",
    "    if last_fold is not None:\n",
    "        for h in horizons:\n",
    "            thr_med = threshold_operacional.get(h, float(\"nan\"))\n",
    "            comb = best_combo_by_h.get(h)\n",
    "            if comb is None or not np.isfinite(thr_med):\n",
    "                final_seq.append(\"–\")\n",
    "                continue\n",
    "            model, W = comb\n",
    "            yt, pt, dt = preds_tst.get((model, W, h, last_fold), (np.array([]), np.array([]), np.array([])))\n",
    "            if len(pt) == 0:\n",
    "                final_seq.append(\"–\")\n",
    "                continue\n",
    "            yhat = (pt >= thr_med).astype(int)\n",
    "            last_label = int(yhat[-1])\n",
    "            final_seq.append(\"CAI\" if last_label == 1 else \"NÃO CAI\")\n",
    "        print(\"\\nSEQUÊNCIA FINAL (último bloco de TESTE):\")\n",
    "        print(f\"- (D+1, D+3, D+5) = {', '.join(final_seq)}\")\n",
    "    else:\n",
    "        print(\"\\nSEQUÊNCIA FINAL: –\")\n",
    "\n",
    "    # Baselines — TESTE (médias por horizonte)\n",
    "    baseline_rows = []\n",
    "    for h in horizons:\n",
    "        seen_folds = set()\n",
    "        for key in sorted(preds_tst.keys()):\n",
    "            m, W, hh, fi = key\n",
    "            if hh != h or fi in seen_folds:\n",
    "                continue\n",
    "            yt, pt, dt = preds_tst[key]\n",
    "            seen_folds.add(fi)\n",
    "            n = len(yt)\n",
    "            if n == 0:\n",
    "                continue\n",
    "            # Sempre NÃO CAI\n",
    "            y_pred0 = np.zeros(n, dtype=int)\n",
    "            cm0 = confusion_matrix(yt, y_pred0, labels=[1,0])\n",
    "            acc0 = float(accuracy_score(yt, y_pred0))\n",
    "            prec0 = float(precision_score(yt, y_pred0, zero_division=0))\n",
    "            rec0 = float(recall_score(yt, y_pred0, zero_division=0))\n",
    "            f10 = float(f1_score(yt, y_pred0, zero_division=0))\n",
    "            baseline_rows.append(dict(horizon=h, baseline=\"Sempre_NAO_CAI\", fold=fi,\n",
    "                                      recall_CAI=rec0, precisao_CAI=prec0, F1_CAI=f10, acc=acc0,\n",
    "                                      pred_CAI_rate=0.0))\n",
    "            # PropTreino>0.5 — proxy via VAL\n",
    "            yv, pv = None, None\n",
    "            for k2, (yvv, pvv) in preds_val.items():\n",
    "                mm, WW, hhh, fii = k2\n",
    "                if hhh == h and fii == fi and len(yvv) > 0:\n",
    "                    yv, pv = yvv, pvv\n",
    "                    break\n",
    "            if yv is not None:\n",
    "                p_train_cai = float((yv == 1).mean())\n",
    "                pred1 = np.ones(n, dtype=int) if p_train_cai > 0.5 else np.zeros(n, dtype=int)\n",
    "                cm1 = confusion_matrix(yt, pred1, labels=[1,0])\n",
    "                acc1 = float(accuracy_score(yt, pred1))\n",
    "                prec1 = float(precision_score(yt, pred1, zero_division=0))\n",
    "                rec1 = float(recall_score(yt, pred1, zero_division=0))\n",
    "                f11 = float(f1_score(yt, pred1, zero_division=0))\n",
    "                baseline_rows.append(dict(horizon=h, baseline=\"PropTreino>0.5\", fold=fi,\n",
    "                                          recall_CAI=rec1, precisao_CAI=prec1, F1_CAI=f11, acc=acc1,\n",
    "                                          pred_CAI_rate=float(pred1.mean())))\n",
    "            # Sinal de ontem\n",
    "            ret1_map = pd.Series(df.set_index(\"__date__\")[\"ret1\"])\n",
    "            pred2 = []\n",
    "            for d in dt:\n",
    "                prev_day = pd.to_datetime(d) - pd.Timedelta(days=1)\n",
    "                val_prev = ret1_map.get(prev_day, np.nan)\n",
    "                pred2.append(1 if (pd.notna(val_prev) and val_prev < 0) else 0)\n",
    "            pred2 = np.array(pred2, dtype=int)\n",
    "            cm2 = confusion_matrix(yt, pred2, labels=[1,0])\n",
    "            acc2 = float(accuracy_score(yt, pred2))\n",
    "            prec2 = float(precision_score(yt, pred2, zero_division=0))\n",
    "            rec2 = float(recall_score(yt, pred2, zero_division=0))\n",
    "            f12 = float(f1_score(yt, pred2, zero_division=0))\n",
    "            baseline_rows.append(dict(horizon=h, baseline=\"SinalOntem\", fold=fi,\n",
    "                                      recall_CAI=rec2, precisao_CAI=prec2, F1_CAI=f12, acc=acc2,\n",
    "                                      pred_CAI_rate=float(pred2.mean())))\n",
    "            # Momentum_3d — CAI se somatório(últimos 3 ret1) < 0\n",
    "            mom_map = pd.Series(df.set_index(\"__date__\")[\"mom3d_prev\"])  # já shiftado\n",
    "            pred3 = []\n",
    "            for d in dt:\n",
    "                val_prev3 = mom_map.get(pd.to_datetime(d), np.nan)\n",
    "                pred3.append(1 if (pd.notna(val_prev3) and val_prev3 < 0) else 0)\n",
    "            pred3 = np.array(pred3, dtype=int)\n",
    "            cm3 = confusion_matrix(yt, pred3, labels=[1,0])\n",
    "            acc3 = float(accuracy_score(yt, pred3))\n",
    "            prec3 = float(precision_score(yt, pred3, zero_division=0))\n",
    "            rec3 = float(recall_score(yt, pred3, zero_division=0))\n",
    "            f13 = float(f1_score(yt, pred3, zero_division=0))\n",
    "            baseline_rows.append(dict(horizon=h, baseline=\"Momentum_3d\", fold=fi,\n",
    "                                      recall_CAI=rec3, precisao_CAI=prec3, F1_CAI=f13, acc=acc3,\n",
    "                                      pred_CAI_rate=float(pred3.mean())))\n",
    "    baseline_df = pd.DataFrame(baseline_rows) if baseline_rows else pd.DataFrame()\n",
    "\n",
    "    if not baseline_df.empty:\n",
    "        print(\"\\nBASELINES — TESTE (médias por horizonte)\")\n",
    "        base_agg = baseline_df.groupby([\"horizon\",\"baseline\"], as_index=False).agg(\n",
    "            recall_CAI_mean=(\"recall_CAI\",\"mean\"),\n",
    "            precisao_CAI_mean=(\"precisao_CAI\",\"mean\"),\n",
    "            F1_CAI_mean=(\"F1_CAI\",\"mean\"),\n",
    "            acc_mean=(\"acc\",\"mean\"),\n",
    "            pred_CAI_rate_mean=(\"pred_CAI_rate\",\"mean\"),\n",
    "            folds=(\"fold\",\"nunique\")\n",
    "        ).sort_values([\"horizon\",\"baseline\"])\n",
    "        for h in horizons:\n",
    "            bs = base_agg[base_agg[\"horizon\"]==h]\n",
    "            print(f\"\\nHORIZONTE D+{h}\")\n",
    "            if bs.empty:\n",
    "                print(\"–\")\n",
    "            else:\n",
    "                print(bs[[\"baseline\",\"recall_CAI_mean\",\"precisao_CAI_mean\",\"F1_CAI_mean\",\"acc_mean\",\"pred_CAI_rate_mean\",\"folds\"]].to_string(index=False))\n",
    "\n",
    "    # Flags e Checklist\n",
    "    print(\"\\nFLAGS\")\n",
    "    print(f\"- floor_unmet_folds: {floor_unmet_flags}\")\n",
    "    print(f\"- coverage_failed: {coverage_failed_flags}\")\n",
    "    print(f\"- no_winner: D+1={no_winner_flags[1]}, D+3={no_winner_flags[3]}, D+5={no_winner_flags[5]}\")\n",
    "\n",
    "    print(\"\\nCHECKLIST\")\n",
    "    print(f\"- SSOT: {path} (tier={tier})\")\n",
    "    print(f\"- Horizontes: {horizons} | Janelas: {windows} | Folds: {len(splits)}\")\n",
    "    print(f\"- precision_floor: D+1={precision_floor['D+1']:.2f}, D+3={precision_floor['D+3']:.2f}, D+5={precision_floor['D+5']:.2f}\")\n",
    "    print(f\"- N_min_preds_val: {N_min_preds_val}\")\n",
    "    print(f\"- Baselines presentes: {'SIM' if not baseline_df.empty else 'NÃO'}\")\n",
    "    print(f\"- dry_run=True (nenhum arquivo salvo)\")\n",
    "\n",
    "    print(f\"\\n[{now_ts()}] Fim — CAI vs NÃO CAI (dry_run={dry_run})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project: BOLSA_2026)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
