{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aada492",
   "metadata": {},
   "source": [
    "---\n",
    "# MODELOS E MODELAGEM\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ade437",
   "metadata": {},
   "source": [
    "## Comparativo XGBoost vs. LSTM no IBOV (GOLD/SILVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba887a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-19 16:02:43] Início — Comparativo XGBoost vs. LSTM (IBOV SSOT)\n",
      "\n",
      "[PROVA DE LEITURA]\n",
      "- Caminho efetivo usado: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Schema (primeiras colunas): ['date', 'open', 'high', 'low', 'close', 'volume', 'ticker', 'open_norm', 'high_norm', 'low_norm', 'close_norm', 'volume_norm']\n",
      "- Contagem de linhas: 3400, colunas: 25\n",
      "- date_min: 2012-01-03, date_max: 2025-09-19\n",
      "- Amostra (head 5):\n",
      "               date     open     high      low    close  volume ticker  open_norm  high_norm  low_norm  close_norm  volume_norm  return_1d  volatility_5d   sma_5  sma_20  sma_ratio      y_h1      y_h3      y_h5  y_h1_cls  y_h3_cls  y_h5_cls year            __date__\n",
      "2012-01-03 00:00:00  57836.0  59288.0  57836.0  59265.0 3083000  ^BVSP   0.188125   0.196279  0.191702    0.200510     0.875060   0.001687            NaN     NaN     NaN        NaN  0.001687 -0.011221  0.009128       NaN       NaN       NaN 2012 2012-01-03 00:00:00\n",
      "2012-01-04 00:00:00  59263.0  59519.0  58558.0  59365.0 2252000  ^BVSP   0.201327   0.198412  0.198360    0.201431     0.856665  -0.013796            NaN     NaN     NaN        NaN -0.013796  -0.00475  0.010056       NaN       NaN       NaN 2012 2012-01-04 00:00:00\n",
      "2012-01-05 00:00:00  59354.0  59354.0  57963.0  58546.0 2351200  ^BVSP   0.202169   0.196888  0.192873    0.193887     0.859190   0.000922            NaN     NaN     NaN        NaN  0.000922  0.021522  0.023486       NaN       NaN       NaN 2012 2012-01-05 00:00:00\n",
      "2012-01-06 00:00:00  58565.0  59261.0  58355.0  58600.0 1659200  ^BVSP   0.194869   0.196030  0.196488    0.194384     0.838774   0.008242            NaN     NaN     NaN        NaN  0.008242  0.023242  0.009334       NaN       NaN       NaN 2012 2012-01-06 00:00:00\n",
      "2012-01-09 00:00:00  58601.0  59220.0  58599.0  59083.0 2244600  ^BVSP   0.195202   0.195651  0.198738    0.198833     0.856472   0.012237            NaN 58971.8     NaN        NaN  0.012237  0.014183  0.014776       NaN       NaN       NaN 2012 2012-01-09 00:00:00\n",
      "\n",
      "[RÓTULOS — DETECÇÃO/GERAÇÃO]\n",
      "- h=1: rótulos GERADOS -> dir_fwd_1 (binário a partir de ret_fwd_1).\n",
      "- h=3: rótulos GERADOS -> dir_fwd_3 (binário a partir de ret_fwd_3).\n",
      "- h=5: rótulos GERADOS -> dir_fwd_5 (binário a partir de ret_fwd_5).\n",
      "- Resumo: {\n",
      "  \"1\": {\n",
      "    \"type\": \"classification\",\n",
      "    \"col\": \"dir_fwd_1\",\n",
      "    \"origem\": \"gerado\"\n",
      "  },\n",
      "  \"3\": {\n",
      "    \"type\": \"classification\",\n",
      "    \"col\": \"dir_fwd_3\",\n",
      "    \"origem\": \"gerado\"\n",
      "  },\n",
      "  \"5\": {\n",
      "    \"type\": \"classification\",\n",
      "    \"col\": \"dir_fwd_5\",\n",
      "    \"origem\": \"gerado\"\n",
      "  }\n",
      "}\n",
      "\n",
      "[WALK-FORWARD — Folds explícitos]\n",
      "Fold 01: train[2012-01-03 → 2013-04-02], val[2013-04-03 → 2013-07-02], test[2013-07-03 → 2014-01-02]\n",
      "Fold 02: train[2012-01-03 → 2013-10-02], val[2013-10-03 → 2014-01-02], test[2014-01-03 → 2014-07-02]\n",
      "Fold 03: train[2012-01-03 → 2014-04-02], val[2014-04-03 → 2014-07-02], test[2014-07-03 → 2015-01-02]\n",
      "Fold 04: train[2012-01-03 → 2014-10-02], val[2014-10-03 → 2015-01-02], test[2015-01-03 → 2015-07-02]\n",
      "Fold 05: train[2012-01-03 → 2015-04-02], val[2015-04-03 → 2015-07-02], test[2015-07-03 → 2016-01-02]\n",
      "Fold 06: train[2012-01-03 → 2015-10-02], val[2015-10-03 → 2016-01-02], test[2016-01-03 → 2016-07-02]\n",
      "Fold 07: train[2012-01-03 → 2016-04-02], val[2016-04-03 → 2016-07-02], test[2016-07-03 → 2017-01-02]\n",
      "Fold 08: train[2012-01-03 → 2016-10-02], val[2016-10-03 → 2017-01-02], test[2017-01-03 → 2017-07-02]\n",
      "Fold 09: train[2012-01-03 → 2017-04-02], val[2017-04-03 → 2017-07-02], test[2017-07-03 → 2018-01-02]\n",
      "Fold 10: train[2012-01-03 → 2017-10-02], val[2017-10-03 → 2018-01-02], test[2018-01-03 → 2018-07-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 16:02:43.437084: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x767b9018a2a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x767b885b68e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x767b885b68e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "[FEATURES POR JANELA — XGBoost]\n",
      "- Para cada janela (5/10/15): lags ret1 (1..min(janela,10)), ret1_roll_mean_janela, ret1_roll_std_janela, ret1_z_janela.\n",
      "[SEQUÊNCIAS — LSTM]\n",
      "- Features por passo: ['ret1','roll_mean_ret_5','roll_std_ret_5'] (padronizadas no treino).\n",
      "- Shape por janela: [amostras, janela, 3].\n",
      "\n",
      "[HIPERPARÂMETROS FINAIS — XGBoost]\n",
      "- h=1, w=5: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 0}\n",
      "- h=1, w=10: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 3}\n",
      "- h=1, w=15: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 11}\n",
      "- h=3, w=5: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 49}\n",
      "- h=3, w=10: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 39}\n",
      "- h=3, w=15: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 19}\n",
      "- h=5, w=5: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 24}\n",
      "- h=5, w=10: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 0}\n",
      "- h=5, w=15: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 14}\n",
      "\n",
      "[HIPERPARÂMETROS FINAIS — LSTM]\n",
      "- h=1, w=5: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=1, w=10: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=1, w=15: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=3, w=5: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=3, w=10: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=3, w=15: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=5, w=5: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=5, w=10: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=5, w=15: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "\n",
      "[MÉTRICAS DE PREVISÃO — por fold (head)]\n",
      "model  horizon  window  fold split      AUC      ACC       F1\n",
      " LSTM        1       5     1  test 0.464336 0.458333 0.000000\n",
      " LSTM        1       5     1   val 0.567308 0.620690 0.388889\n",
      " LSTM        1       5     2  test 0.533918 0.495726 0.213333\n",
      " LSTM        1       5     2   val 0.579576 0.490909 0.363636\n",
      " LSTM        1       5     3  test 0.503052 0.570248 0.446809\n",
      " LSTM        1       5     3   val 0.522487 0.490909 0.176471\n",
      " LSTM        1       5     4  test 0.425079 0.474576 0.261905\n",
      " LSTM        1       5     4   val 0.594920 0.607143 0.083333\n",
      " LSTM        1       5     5  test 0.505102 0.572650 0.404762\n",
      " LSTM        1       5     5   val 0.464103 0.517857 0.228571\n",
      " LSTM        1       5     6  test 0.531746 0.470588 0.000000\n",
      " LSTM        1       5     6   val 0.444282 0.584906 0.000000\n",
      "\n",
      "[MÉTRICAS DE PREVISÃO — agregadas (média ± desvio)]\n",
      " index  model_  horizon_  window_ split_  AUC_mean  AUC_std  ACC_mean  ACC_std  F1_mean   F1_std\n",
      "     0    LSTM         1        5   test  0.488223 0.043827  0.488272 0.048977 0.261136 0.168586\n",
      "     1    LSTM         1        5    val  0.544557 0.054187  0.545792 0.075655 0.293779 0.210278\n",
      "     2    LSTM         1       10   test  0.492590 0.066362  0.472067 0.049217 0.255162 0.169134\n",
      "     3    LSTM         1       10    val  0.501288 0.105661  0.515084 0.063081 0.312689 0.209919\n",
      "     4    LSTM         1       15   test  0.483143 0.055106  0.480707 0.057673 0.237450 0.166094\n",
      "     5    LSTM         1       15    val  0.548013 0.081382  0.540045 0.030370 0.284865 0.303193\n",
      "     6    LSTM         3        5   test  0.449334 0.070661  0.463279 0.062498 0.410681 0.124260\n",
      "     7    LSTM         3        5    val  0.542334 0.132778  0.586821 0.104480 0.426512 0.227896\n",
      "     8    LSTM         3       10   test  0.462831 0.070790  0.461741 0.064182 0.404547 0.113424\n",
      "     9    LSTM         3       10    val  0.515820 0.117288  0.525832 0.090396 0.425606 0.172639\n",
      "    10    LSTM         3       15   test  0.478019 0.070818  0.468194 0.066243 0.353669 0.130245\n",
      "    11    LSTM         3       15    val  0.561525 0.106453  0.540175 0.104416 0.447195 0.185396\n",
      "    12    LSTM         5        5   test  0.433208 0.115080  0.450419 0.083327 0.290666 0.153055\n",
      "    13    LSTM         5        5    val  0.478129 0.166023  0.532883 0.072682 0.329990 0.197989\n",
      "    14    LSTM         5       10   test  0.446017 0.062343  0.447438 0.073772 0.315478 0.144599\n",
      "    15    LSTM         5       10    val  0.571900 0.150304  0.552772 0.130372 0.413334 0.213964\n",
      "    16    LSTM         5       15   test  0.451148 0.108502  0.466283 0.096118 0.324064 0.164404\n",
      "    17    LSTM         5       15    val  0.524634 0.194325  0.519618 0.151165 0.374508 0.225859\n",
      "    18 XGBoost         1        5   test  0.504486 0.038402  0.495479 0.055053 0.165508 0.178010\n",
      "    19 XGBoost         1        5    val  0.557692 0.050491  0.538568 0.049063 0.201300 0.254404\n",
      "    20 XGBoost         1       10   test  0.472114 0.067022  0.487499 0.044533 0.183980 0.202704\n",
      "    21 XGBoost         1       10    val  0.510876 0.071326  0.518820 0.057505 0.198850 0.208346\n",
      "    22 XGBoost         1       15   test  0.478388 0.055352  0.473780 0.046589 0.227860 0.179514\n",
      "    23 XGBoost         1       15    val  0.551727 0.054206  0.543932 0.056339 0.278612 0.236004\n",
      "    24 XGBoost         3        5   test  0.464241 0.048676  0.482937 0.059939 0.244481 0.237590\n",
      "    25 XGBoost         3        5    val  0.521179 0.068067  0.531748 0.045312 0.269092 0.270597\n",
      "    26 XGBoost         3       10   test  0.485478 0.053755  0.490178 0.049361 0.240431 0.233156\n",
      "    27 XGBoost         3       10    val  0.559307 0.089335  0.538467 0.042966 0.273964 0.298042\n",
      "    28 XGBoost         3       15   test  0.495324 0.043757  0.493668 0.049059 0.322051 0.187128\n",
      "    29 XGBoost         3       15    val  0.532555 0.060945  0.528996 0.086612 0.356096 0.179596\n",
      "    30 XGBoost         5        5   test  0.460228 0.037254  0.456023 0.070968 0.230776 0.213934\n",
      "    31 XGBoost         5        5    val  0.527272 0.113282  0.559380 0.097652 0.272576 0.272773\n",
      "    32 XGBoost         5       10   test  0.485113 0.085844  0.475991 0.058090 0.308628 0.221348\n",
      "    33 XGBoost         5       10    val  0.535567 0.078830  0.576493 0.078624 0.355982 0.273618\n",
      "    34 XGBoost         5       15   test  0.477573 0.081994  0.465428 0.041960 0.280048 0.200180\n",
      "    35 XGBoost         5       15    val  0.578048 0.080052  0.589587 0.067900 0.323046 0.248619\n",
      "\n",
      "[MÉTRICAS OPERACIONAIS — por fold (head)]\n",
      "model  horizon  window  fold split  ann_return    sharpe     maxdd  hit_rate  turnover  threshold\n",
      " LSTM        1       5     1  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     1   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     3  test   -0.057036 -1.496348 -0.027805       0.0  0.016529       0.55\n",
      " LSTM        1       5     3   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     6  test    0.000000  0.000000  0.000000       NaN  0.000000       0.50\n",
      " LSTM        1       5     6   val    0.000000  0.000000  0.000000       NaN  0.000000       0.50\n",
      "\n",
      "[MÉTRICAS OPERACIONAIS — agregadas (média ± desvio)]\n",
      " index  model_  horizon_  window_ split_  ann_return_mean  ann_return_std  sharpe_mean  sharpe_std  maxdd_mean  maxdd_std  hit_rate_mean  hit_rate_std  turnover_mean  turnover_std\n",
      "     0    LSTM         1        5   test        -0.003027        0.091151     0.225715    1.346111   -0.024146   0.061215       0.526615      0.399913       0.028660      0.063402\n",
      "     1    LSTM         1        5    val         0.070002        0.129207     0.744401    1.215467   -0.007222   0.020768       0.750000      0.217945       0.023007      0.040754\n",
      "     2    LSTM         1       10   test        -0.052566        0.117465    -0.191486    1.456236   -0.054232   0.077067       0.548154      0.249162       0.053797      0.062442\n",
      "     3    LSTM         1       10    val         0.085097        0.125845     1.079842    1.467075   -0.012819   0.023398       0.704196      0.284354       0.049010      0.080506\n",
      "     4    LSTM         1       15   test        -0.061953        0.152830    -0.660858    1.507044   -0.058475   0.071403       0.428732      0.128119       0.070555      0.067590\n",
      "     5    LSTM         1       15    val         0.125394        0.162109     1.054177    1.471284   -0.022531   0.035565       0.653574      0.214809       0.078168      0.110712\n",
      "     6    LSTM         3        5   test        -0.178120        0.292005    -0.819346    2.164915   -0.170001   0.179887       0.592194      0.307466       0.079613      0.079466\n",
      "     7    LSTM         3        5    val         0.315657        0.397429     1.940414    2.074507   -0.006990   0.011521       0.833333      0.210819       0.061919      0.067490\n",
      "     8    LSTM         3       10   test        -0.145370        0.310629    -0.443880    1.907535   -0.137836   0.183531       0.558163      0.245546       0.049320      0.047195\n",
      "     9    LSTM         3       10    val         0.159119        0.190843     1.502888    1.791129   -0.028818   0.056087       0.714286      0.269179       0.059868      0.054211\n",
      "    10    LSTM         3       15   test        -0.149626        0.309227    -0.574150    1.727750   -0.099880   0.175151       0.609009      0.358610       0.025944      0.047443\n",
      "    11    LSTM         3       15    val         0.175460        0.291176     1.832947    2.068786   -0.020787   0.055445       0.773016      0.241212       0.049688      0.058680\n",
      "    12    LSTM         5        5   test        -0.213561        0.343251    -1.247387    2.372877   -0.216542   0.268949       0.377522      0.165305       0.068072      0.072425\n",
      "    13    LSTM         5        5    val         0.368979        0.685186     1.137460    1.451468   -0.058139   0.083816       0.628553      0.061456       0.085907      0.098111\n",
      "    14    LSTM         5       10   test        -0.215652        0.377184    -0.982573    1.892618   -0.200392   0.260919       0.470381      0.276501       0.060871      0.086279\n",
      "    15    LSTM         5       10    val         0.976065        2.055379     1.970745    2.604320   -0.039279   0.082252       0.707872      0.155776       0.058441      0.074760\n",
      "    16    LSTM         5       15   test        -0.160753        0.372924    -0.304050    1.687083   -0.140997   0.226279       0.566396      0.273334       0.047296      0.083604\n",
      "    17    LSTM         5       15    val         0.218740        0.293511     1.458380    1.650981   -0.023093   0.060286       0.822222      0.201843       0.048182      0.080067\n",
      "    18 XGBoost         1        5   test         0.018918        0.031453     0.527518    0.788213   -0.003010   0.007511       0.777778      0.272166       0.024446      0.046466\n",
      "    19 XGBoost         1        5    val         0.080032        0.137482     1.028267    1.538164   -0.000890   0.001821       0.942308      0.115385       0.056824      0.117716\n",
      "    20 XGBoost         1       10   test        -0.010921        0.023638    -0.496660    0.910023   -0.012976   0.018988       0.242647      0.280442       0.054504      0.098950\n",
      "    21 XGBoost         1       10    val         0.019129        0.038614     0.589120    0.968738   -0.005699   0.013187       0.875000      0.150000       0.051690      0.088196\n",
      "    22 XGBoost         1       15   test        -0.038020        0.093462    -0.366262    1.092469   -0.049828   0.060928       0.427737      0.215801       0.191675      0.183808\n",
      "    23 XGBoost         1       15    val         0.069607        0.109663     0.959873    1.278550   -0.014481   0.020869       0.749936      0.144879       0.184556      0.213946\n",
      "    24 XGBoost         3        5   test         0.125941        0.562874     0.085097    1.740869   -0.071667   0.094005       0.439130      0.268147       0.089645      0.117512\n",
      "    25 XGBoost         3        5    val         0.101237        0.178808     0.678943    0.887244   -0.042272   0.071782       0.729234      0.172994       0.102053      0.126801\n",
      "    26 XGBoost         3       10   test        -0.044623        0.147106    -0.194789    1.029890   -0.042262   0.091881       0.541497      0.159463       0.072505      0.149528\n",
      "    27 XGBoost         3       10    val         0.244730        0.508918     0.762548    1.357996   -0.022871   0.050154       0.751151      0.066301       0.064693      0.110977\n",
      "    28 XGBoost         3       15   test        -0.141426        0.245171    -0.431367    1.274429   -0.115474   0.189473       0.583333      0.280610       0.105281      0.165643\n",
      "    29 XGBoost         3       15    val         0.047049        0.080729     0.449730    0.729196   -0.027110   0.060412       0.718266      0.191022       0.094240      0.168249\n",
      "    30 XGBoost         5        5   test        -0.049515        0.418860    -0.424079    1.655957   -0.112953   0.175841       0.457983      0.208526       0.094339      0.124354\n",
      "    31 XGBoost         5        5    val         0.152369        0.244055     1.230116    1.566050   -0.014802   0.033408       0.813043      0.257037       0.086071      0.106540\n",
      "    32 XGBoost         5       10   test         0.004661        0.752246    -0.262417    3.023806   -0.247114   0.253184       0.556928      0.272461       0.184613      0.154303\n",
      "    33 XGBoost         5       10    val         0.388340        0.612179     1.070197    0.916977   -0.098081   0.114841       0.622839      0.181584       0.182250      0.178656\n",
      "    34 XGBoost         5       15   test        -0.212999        0.412638    -1.089272    2.160996   -0.232079   0.263365       0.435168      0.152832       0.182567      0.175907\n",
      "    35 XGBoost         5       15    val         0.424421        0.806671     1.020858    1.334275   -0.044809   0.062724       0.676188      0.062122       0.152215      0.185023\n",
      "VALIDATION_ERROR: falha ao selecionar vencedor: Invalid format specifier '.1% if not pd.isna(top['hit_rate']) else float('nan')' for object of type 'float'\n",
      "\n",
      "[CHECKLIST OBRIGATÓRIO — dry_run]\n",
      "- SSOT usado: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Labels D+1/D+3/D+5: OK\n",
      "- Walk-forward folds: 10\n",
      "- Métricas previsão — linhas: 360\n",
      "- Métricas operacionais — linhas: 360\n",
      "- Vencedor destacado: FALHA\n",
      "- Persistência: DESLIGADA (dry_run=True)\n",
      "CHECKLIST_FAILURE: algum item obrigatório não foi atendido. Revise os logs acima.\n",
      "\n",
      "[RELATÓRIO FINAL — Estrutura]\n",
      "- pred_df.info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 360 entries, 3 to 356\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   model    360 non-null    object \n",
      " 1   horizon  360 non-null    int64  \n",
      " 2   window   360 non-null    int64  \n",
      " 3   fold     360 non-null    int64  \n",
      " 4   split    360 non-null    object \n",
      " 5   AUC      360 non-null    float64\n",
      " 6   ACC      360 non-null    float64\n",
      " 7   F1       360 non-null    float64\n",
      "dtypes: float64(3), int64(3), object(2)\n",
      "memory usage: 25.3+ KB\n",
      "None\n",
      "- op_df.info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 360 entries, 3 to 356\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   model       360 non-null    object \n",
      " 1   horizon     360 non-null    int64  \n",
      " 2   window      360 non-null    int64  \n",
      " 3   fold        360 non-null    int64  \n",
      " 4   split       360 non-null    object \n",
      " 5   ann_return  360 non-null    float64\n",
      " 6   sharpe      360 non-null    float64\n",
      " 7   maxdd       360 non-null    float64\n",
      " 8   hit_rate    193 non-null    float64\n",
      " 9   turnover    360 non-null    float64\n",
      " 10  threshold   360 non-null    float64\n",
      "dtypes: float64(6), int64(3), object(2)\n",
      "memory usage: 33.8+ KB\n",
      "None\n",
      "\n",
      "[Amostras iniciais — pred_df]\n",
      "model  horizon  window  fold split      AUC      ACC       F1\n",
      " LSTM        1       5     1  test 0.464336 0.458333 0.000000\n",
      " LSTM        1       5     1   val 0.567308 0.620690 0.388889\n",
      " LSTM        1       5     2  test 0.533918 0.495726 0.213333\n",
      " LSTM        1       5     2   val 0.579576 0.490909 0.363636\n",
      " LSTM        1       5     3  test 0.503052 0.570248 0.446809\n",
      " LSTM        1       5     3   val 0.522487 0.490909 0.176471\n",
      " LSTM        1       5     4  test 0.425079 0.474576 0.261905\n",
      " LSTM        1       5     4   val 0.594920 0.607143 0.083333\n",
      " LSTM        1       5     5  test 0.505102 0.572650 0.404762\n",
      " LSTM        1       5     5   val 0.464103 0.517857 0.228571\n",
      "\n",
      "[Amostras iniciais — op_df]\n",
      "model  horizon  window  fold split  ann_return    sharpe     maxdd  hit_rate  turnover  threshold\n",
      " LSTM        1       5     1  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     1   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     3  test   -0.057036 -1.496348 -0.027805       0.0  0.016529       0.55\n",
      " LSTM        1       5     3   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      "\n",
      "[Intervalos temporais cobertos]\n",
      "- Dataset: 2012-01-03 → 2025-09-19\n",
      "- Folds: 10 (test_months=6, val_months=3, treino mínimo=18)\n",
      "\n",
      "[Contagens totais]\n",
      "- pred_df: 360 linhas\n",
      "- op_df: 360 linhas\n",
      "\n",
      "[2025-09-19 16:06:15] Fim — Comparativo (dry_run=True, persist=False)\n",
      "\n",
      "[FEATURES POR JANELA — XGBoost]\n",
      "- Para cada janela (5/10/15): lags ret1 (1..min(janela,10)), ret1_roll_mean_janela, ret1_roll_std_janela, ret1_z_janela.\n",
      "[SEQUÊNCIAS — LSTM]\n",
      "- Features por passo: ['ret1','roll_mean_ret_5','roll_std_ret_5'] (padronizadas no treino).\n",
      "- Shape por janela: [amostras, janela, 3].\n",
      "\n",
      "[HIPERPARÂMETROS FINAIS — XGBoost]\n",
      "- h=1, w=5: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 0}\n",
      "- h=1, w=10: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 3}\n",
      "- h=1, w=15: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 11}\n",
      "- h=3, w=5: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 49}\n",
      "- h=3, w=10: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 39}\n",
      "- h=3, w=15: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 19}\n",
      "- h=5, w=5: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 24}\n",
      "- h=5, w=10: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 0}\n",
      "- h=5, w=15: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'early_stopping_rounds': 50, 'best_iterations': 14}\n",
      "\n",
      "[HIPERPARÂMETROS FINAIS — LSTM]\n",
      "- h=1, w=5: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=1, w=10: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=1, w=15: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=3, w=5: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=3, w=10: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=3, w=15: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=5, w=5: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=5, w=10: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "- h=5, w=15: {'units': 48, 'layers': 1, 'dropout': 0.2, 'epochs': 50, 'batch_size': 32, 'patience': 5}\n",
      "\n",
      "[MÉTRICAS DE PREVISÃO — por fold (head)]\n",
      "model  horizon  window  fold split      AUC      ACC       F1\n",
      " LSTM        1       5     1  test 0.464336 0.458333 0.000000\n",
      " LSTM        1       5     1   val 0.567308 0.620690 0.388889\n",
      " LSTM        1       5     2  test 0.533918 0.495726 0.213333\n",
      " LSTM        1       5     2   val 0.579576 0.490909 0.363636\n",
      " LSTM        1       5     3  test 0.503052 0.570248 0.446809\n",
      " LSTM        1       5     3   val 0.522487 0.490909 0.176471\n",
      " LSTM        1       5     4  test 0.425079 0.474576 0.261905\n",
      " LSTM        1       5     4   val 0.594920 0.607143 0.083333\n",
      " LSTM        1       5     5  test 0.505102 0.572650 0.404762\n",
      " LSTM        1       5     5   val 0.464103 0.517857 0.228571\n",
      " LSTM        1       5     6  test 0.531746 0.470588 0.000000\n",
      " LSTM        1       5     6   val 0.444282 0.584906 0.000000\n",
      "\n",
      "[MÉTRICAS DE PREVISÃO — agregadas (média ± desvio)]\n",
      " index  model_  horizon_  window_ split_  AUC_mean  AUC_std  ACC_mean  ACC_std  F1_mean   F1_std\n",
      "     0    LSTM         1        5   test  0.488223 0.043827  0.488272 0.048977 0.261136 0.168586\n",
      "     1    LSTM         1        5    val  0.544557 0.054187  0.545792 0.075655 0.293779 0.210278\n",
      "     2    LSTM         1       10   test  0.492590 0.066362  0.472067 0.049217 0.255162 0.169134\n",
      "     3    LSTM         1       10    val  0.501288 0.105661  0.515084 0.063081 0.312689 0.209919\n",
      "     4    LSTM         1       15   test  0.483143 0.055106  0.480707 0.057673 0.237450 0.166094\n",
      "     5    LSTM         1       15    val  0.548013 0.081382  0.540045 0.030370 0.284865 0.303193\n",
      "     6    LSTM         3        5   test  0.449334 0.070661  0.463279 0.062498 0.410681 0.124260\n",
      "     7    LSTM         3        5    val  0.542334 0.132778  0.586821 0.104480 0.426512 0.227896\n",
      "     8    LSTM         3       10   test  0.462831 0.070790  0.461741 0.064182 0.404547 0.113424\n",
      "     9    LSTM         3       10    val  0.515820 0.117288  0.525832 0.090396 0.425606 0.172639\n",
      "    10    LSTM         3       15   test  0.478019 0.070818  0.468194 0.066243 0.353669 0.130245\n",
      "    11    LSTM         3       15    val  0.561525 0.106453  0.540175 0.104416 0.447195 0.185396\n",
      "    12    LSTM         5        5   test  0.433208 0.115080  0.450419 0.083327 0.290666 0.153055\n",
      "    13    LSTM         5        5    val  0.478129 0.166023  0.532883 0.072682 0.329990 0.197989\n",
      "    14    LSTM         5       10   test  0.446017 0.062343  0.447438 0.073772 0.315478 0.144599\n",
      "    15    LSTM         5       10    val  0.571900 0.150304  0.552772 0.130372 0.413334 0.213964\n",
      "    16    LSTM         5       15   test  0.451148 0.108502  0.466283 0.096118 0.324064 0.164404\n",
      "    17    LSTM         5       15    val  0.524634 0.194325  0.519618 0.151165 0.374508 0.225859\n",
      "    18 XGBoost         1        5   test  0.504486 0.038402  0.495479 0.055053 0.165508 0.178010\n",
      "    19 XGBoost         1        5    val  0.557692 0.050491  0.538568 0.049063 0.201300 0.254404\n",
      "    20 XGBoost         1       10   test  0.472114 0.067022  0.487499 0.044533 0.183980 0.202704\n",
      "    21 XGBoost         1       10    val  0.510876 0.071326  0.518820 0.057505 0.198850 0.208346\n",
      "    22 XGBoost         1       15   test  0.478388 0.055352  0.473780 0.046589 0.227860 0.179514\n",
      "    23 XGBoost         1       15    val  0.551727 0.054206  0.543932 0.056339 0.278612 0.236004\n",
      "    24 XGBoost         3        5   test  0.464241 0.048676  0.482937 0.059939 0.244481 0.237590\n",
      "    25 XGBoost         3        5    val  0.521179 0.068067  0.531748 0.045312 0.269092 0.270597\n",
      "    26 XGBoost         3       10   test  0.485478 0.053755  0.490178 0.049361 0.240431 0.233156\n",
      "    27 XGBoost         3       10    val  0.559307 0.089335  0.538467 0.042966 0.273964 0.298042\n",
      "    28 XGBoost         3       15   test  0.495324 0.043757  0.493668 0.049059 0.322051 0.187128\n",
      "    29 XGBoost         3       15    val  0.532555 0.060945  0.528996 0.086612 0.356096 0.179596\n",
      "    30 XGBoost         5        5   test  0.460228 0.037254  0.456023 0.070968 0.230776 0.213934\n",
      "    31 XGBoost         5        5    val  0.527272 0.113282  0.559380 0.097652 0.272576 0.272773\n",
      "    32 XGBoost         5       10   test  0.485113 0.085844  0.475991 0.058090 0.308628 0.221348\n",
      "    33 XGBoost         5       10    val  0.535567 0.078830  0.576493 0.078624 0.355982 0.273618\n",
      "    34 XGBoost         5       15   test  0.477573 0.081994  0.465428 0.041960 0.280048 0.200180\n",
      "    35 XGBoost         5       15    val  0.578048 0.080052  0.589587 0.067900 0.323046 0.248619\n",
      "\n",
      "[MÉTRICAS OPERACIONAIS — por fold (head)]\n",
      "model  horizon  window  fold split  ann_return    sharpe     maxdd  hit_rate  turnover  threshold\n",
      " LSTM        1       5     1  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     1   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     3  test   -0.057036 -1.496348 -0.027805       0.0  0.016529       0.55\n",
      " LSTM        1       5     3   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     6  test    0.000000  0.000000  0.000000       NaN  0.000000       0.50\n",
      " LSTM        1       5     6   val    0.000000  0.000000  0.000000       NaN  0.000000       0.50\n",
      "\n",
      "[MÉTRICAS OPERACIONAIS — agregadas (média ± desvio)]\n",
      " index  model_  horizon_  window_ split_  ann_return_mean  ann_return_std  sharpe_mean  sharpe_std  maxdd_mean  maxdd_std  hit_rate_mean  hit_rate_std  turnover_mean  turnover_std\n",
      "     0    LSTM         1        5   test        -0.003027        0.091151     0.225715    1.346111   -0.024146   0.061215       0.526615      0.399913       0.028660      0.063402\n",
      "     1    LSTM         1        5    val         0.070002        0.129207     0.744401    1.215467   -0.007222   0.020768       0.750000      0.217945       0.023007      0.040754\n",
      "     2    LSTM         1       10   test        -0.052566        0.117465    -0.191486    1.456236   -0.054232   0.077067       0.548154      0.249162       0.053797      0.062442\n",
      "     3    LSTM         1       10    val         0.085097        0.125845     1.079842    1.467075   -0.012819   0.023398       0.704196      0.284354       0.049010      0.080506\n",
      "     4    LSTM         1       15   test        -0.061953        0.152830    -0.660858    1.507044   -0.058475   0.071403       0.428732      0.128119       0.070555      0.067590\n",
      "     5    LSTM         1       15    val         0.125394        0.162109     1.054177    1.471284   -0.022531   0.035565       0.653574      0.214809       0.078168      0.110712\n",
      "     6    LSTM         3        5   test        -0.178120        0.292005    -0.819346    2.164915   -0.170001   0.179887       0.592194      0.307466       0.079613      0.079466\n",
      "     7    LSTM         3        5    val         0.315657        0.397429     1.940414    2.074507   -0.006990   0.011521       0.833333      0.210819       0.061919      0.067490\n",
      "     8    LSTM         3       10   test        -0.145370        0.310629    -0.443880    1.907535   -0.137836   0.183531       0.558163      0.245546       0.049320      0.047195\n",
      "     9    LSTM         3       10    val         0.159119        0.190843     1.502888    1.791129   -0.028818   0.056087       0.714286      0.269179       0.059868      0.054211\n",
      "    10    LSTM         3       15   test        -0.149626        0.309227    -0.574150    1.727750   -0.099880   0.175151       0.609009      0.358610       0.025944      0.047443\n",
      "    11    LSTM         3       15    val         0.175460        0.291176     1.832947    2.068786   -0.020787   0.055445       0.773016      0.241212       0.049688      0.058680\n",
      "    12    LSTM         5        5   test        -0.213561        0.343251    -1.247387    2.372877   -0.216542   0.268949       0.377522      0.165305       0.068072      0.072425\n",
      "    13    LSTM         5        5    val         0.368979        0.685186     1.137460    1.451468   -0.058139   0.083816       0.628553      0.061456       0.085907      0.098111\n",
      "    14    LSTM         5       10   test        -0.215652        0.377184    -0.982573    1.892618   -0.200392   0.260919       0.470381      0.276501       0.060871      0.086279\n",
      "    15    LSTM         5       10    val         0.976065        2.055379     1.970745    2.604320   -0.039279   0.082252       0.707872      0.155776       0.058441      0.074760\n",
      "    16    LSTM         5       15   test        -0.160753        0.372924    -0.304050    1.687083   -0.140997   0.226279       0.566396      0.273334       0.047296      0.083604\n",
      "    17    LSTM         5       15    val         0.218740        0.293511     1.458380    1.650981   -0.023093   0.060286       0.822222      0.201843       0.048182      0.080067\n",
      "    18 XGBoost         1        5   test         0.018918        0.031453     0.527518    0.788213   -0.003010   0.007511       0.777778      0.272166       0.024446      0.046466\n",
      "    19 XGBoost         1        5    val         0.080032        0.137482     1.028267    1.538164   -0.000890   0.001821       0.942308      0.115385       0.056824      0.117716\n",
      "    20 XGBoost         1       10   test        -0.010921        0.023638    -0.496660    0.910023   -0.012976   0.018988       0.242647      0.280442       0.054504      0.098950\n",
      "    21 XGBoost         1       10    val         0.019129        0.038614     0.589120    0.968738   -0.005699   0.013187       0.875000      0.150000       0.051690      0.088196\n",
      "    22 XGBoost         1       15   test        -0.038020        0.093462    -0.366262    1.092469   -0.049828   0.060928       0.427737      0.215801       0.191675      0.183808\n",
      "    23 XGBoost         1       15    val         0.069607        0.109663     0.959873    1.278550   -0.014481   0.020869       0.749936      0.144879       0.184556      0.213946\n",
      "    24 XGBoost         3        5   test         0.125941        0.562874     0.085097    1.740869   -0.071667   0.094005       0.439130      0.268147       0.089645      0.117512\n",
      "    25 XGBoost         3        5    val         0.101237        0.178808     0.678943    0.887244   -0.042272   0.071782       0.729234      0.172994       0.102053      0.126801\n",
      "    26 XGBoost         3       10   test        -0.044623        0.147106    -0.194789    1.029890   -0.042262   0.091881       0.541497      0.159463       0.072505      0.149528\n",
      "    27 XGBoost         3       10    val         0.244730        0.508918     0.762548    1.357996   -0.022871   0.050154       0.751151      0.066301       0.064693      0.110977\n",
      "    28 XGBoost         3       15   test        -0.141426        0.245171    -0.431367    1.274429   -0.115474   0.189473       0.583333      0.280610       0.105281      0.165643\n",
      "    29 XGBoost         3       15    val         0.047049        0.080729     0.449730    0.729196   -0.027110   0.060412       0.718266      0.191022       0.094240      0.168249\n",
      "    30 XGBoost         5        5   test        -0.049515        0.418860    -0.424079    1.655957   -0.112953   0.175841       0.457983      0.208526       0.094339      0.124354\n",
      "    31 XGBoost         5        5    val         0.152369        0.244055     1.230116    1.566050   -0.014802   0.033408       0.813043      0.257037       0.086071      0.106540\n",
      "    32 XGBoost         5       10   test         0.004661        0.752246    -0.262417    3.023806   -0.247114   0.253184       0.556928      0.272461       0.184613      0.154303\n",
      "    33 XGBoost         5       10    val         0.388340        0.612179     1.070197    0.916977   -0.098081   0.114841       0.622839      0.181584       0.182250      0.178656\n",
      "    34 XGBoost         5       15   test        -0.212999        0.412638    -1.089272    2.160996   -0.232079   0.263365       0.435168      0.152832       0.182567      0.175907\n",
      "    35 XGBoost         5       15    val         0.424421        0.806671     1.020858    1.334275   -0.044809   0.062724       0.676188      0.062122       0.152215      0.185023\n",
      "VALIDATION_ERROR: falha ao selecionar vencedor: Invalid format specifier '.1% if not pd.isna(top['hit_rate']) else float('nan')' for object of type 'float'\n",
      "\n",
      "[CHECKLIST OBRIGATÓRIO — dry_run]\n",
      "- SSOT usado: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Labels D+1/D+3/D+5: OK\n",
      "- Walk-forward folds: 10\n",
      "- Métricas previsão — linhas: 360\n",
      "- Métricas operacionais — linhas: 360\n",
      "- Vencedor destacado: FALHA\n",
      "- Persistência: DESLIGADA (dry_run=True)\n",
      "CHECKLIST_FAILURE: algum item obrigatório não foi atendido. Revise os logs acima.\n",
      "\n",
      "[RELATÓRIO FINAL — Estrutura]\n",
      "- pred_df.info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 360 entries, 3 to 356\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   model    360 non-null    object \n",
      " 1   horizon  360 non-null    int64  \n",
      " 2   window   360 non-null    int64  \n",
      " 3   fold     360 non-null    int64  \n",
      " 4   split    360 non-null    object \n",
      " 5   AUC      360 non-null    float64\n",
      " 6   ACC      360 non-null    float64\n",
      " 7   F1       360 non-null    float64\n",
      "dtypes: float64(3), int64(3), object(2)\n",
      "memory usage: 25.3+ KB\n",
      "None\n",
      "- op_df.info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 360 entries, 3 to 356\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   model       360 non-null    object \n",
      " 1   horizon     360 non-null    int64  \n",
      " 2   window      360 non-null    int64  \n",
      " 3   fold        360 non-null    int64  \n",
      " 4   split       360 non-null    object \n",
      " 5   ann_return  360 non-null    float64\n",
      " 6   sharpe      360 non-null    float64\n",
      " 7   maxdd       360 non-null    float64\n",
      " 8   hit_rate    193 non-null    float64\n",
      " 9   turnover    360 non-null    float64\n",
      " 10  threshold   360 non-null    float64\n",
      "dtypes: float64(6), int64(3), object(2)\n",
      "memory usage: 33.8+ KB\n",
      "None\n",
      "\n",
      "[Amostras iniciais — pred_df]\n",
      "model  horizon  window  fold split      AUC      ACC       F1\n",
      " LSTM        1       5     1  test 0.464336 0.458333 0.000000\n",
      " LSTM        1       5     1   val 0.567308 0.620690 0.388889\n",
      " LSTM        1       5     2  test 0.533918 0.495726 0.213333\n",
      " LSTM        1       5     2   val 0.579576 0.490909 0.363636\n",
      " LSTM        1       5     3  test 0.503052 0.570248 0.446809\n",
      " LSTM        1       5     3   val 0.522487 0.490909 0.176471\n",
      " LSTM        1       5     4  test 0.425079 0.474576 0.261905\n",
      " LSTM        1       5     4   val 0.594920 0.607143 0.083333\n",
      " LSTM        1       5     5  test 0.505102 0.572650 0.404762\n",
      " LSTM        1       5     5   val 0.464103 0.517857 0.228571\n",
      "\n",
      "[Amostras iniciais — op_df]\n",
      "model  horizon  window  fold split  ann_return    sharpe     maxdd  hit_rate  turnover  threshold\n",
      " LSTM        1       5     1  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     1   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     2   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     3  test   -0.057036 -1.496348 -0.027805       0.0  0.016529       0.55\n",
      " LSTM        1       5     3   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     4   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5  test    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      " LSTM        1       5     5   val    0.000000  0.000000  0.000000       NaN  0.000000       0.55\n",
      "\n",
      "[Intervalos temporais cobertos]\n",
      "- Dataset: 2012-01-03 → 2025-09-19\n",
      "- Folds: 10 (test_months=6, val_months=3, treino mínimo=18)\n",
      "\n",
      "[Contagens totais]\n",
      "- pred_df: 360 linhas\n",
      "- op_df: 360 linhas\n",
      "\n",
      "[2025-09-19 16:06:15] Fim — Comparativo (dry_run=True, persist=False)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Comparativo XGBoost vs. LSTM no IBOV (GOLD/SILVER)\n",
    "- Um único bloco de código auto-contido.\n",
    "- Inicia em dry_run=True (simulação). Não persiste nada quando dry_run=True.\n",
    "- Respeita SSOT: usa apenas /home/wrm/BOLSA_2026/{gold,silver}/.\n",
    "- Walk-forward (expanding) com blocos explícitos, sem embaralhar tempo.\n",
    "- Gera métricas de previsão e operacionais (long/flat com custo simples).\n",
    "- Emite mensagens normativas em caso de falhas (VALIDATION_ERROR, CHECKLIST_FAILURE).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import types\n",
    "import errno\n",
    "import warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Verificação de dependências (obrigatório para comparação XGBoost vs LSTM)\n",
    "_missing = []\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "except Exception as e:\n",
    "    _missing.append(f\"xgboost ({e})\")\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential # pyright: ignore[reportMissingImports]\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, Input # pyright: ignore[reportMissingImports]\n",
    "    from tensorflow.keras.callbacks import EarlyStopping # pyright: ignore[reportMissingImports]\n",
    "except Exception as e:\n",
    "    _missing.append(f\"tensorflow ({e})\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, roc_auc_score,\n",
    "    accuracy_score, f1_score\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "# =========================\n",
    "# Configurações e Parâmetros\n",
    "# =========================\n",
    "\n",
    "@dataclass\n",
    "class RunConfig:\n",
    "    dry_run: bool = True\n",
    "    persist: bool = False  # ignorado quando dry_run=True\n",
    "    # Caminhos SSOT\n",
    "    gold_path: str = \"/home/wrm/BOLSA_2026/gold/IBOV_gold.parquet\"\n",
    "    silver_path: str = \"/home/wrm/BOLSA_2026/silver/IBOV_silver.parquet\"\n",
    "    # Janelas e horizontes\n",
    "    windows: Tuple[int, ...] = (5, 10, 15)\n",
    "    horizons: Tuple[int, ...] = (1, 3, 5)  # D+1, D+3, D+5\n",
    "    # Walk-forward\n",
    "    min_train_months: int = 18\n",
    "    test_months: int = 6\n",
    "    val_months: int = 3\n",
    "    max_folds: int = 10  # entre 5 e 10\n",
    "    # Estratégia operacional\n",
    "    cost_per_trade_bps: float = 10.0  # 10 bps por troca de posição\n",
    "    trading_days_per_year: int = 252\n",
    "    # LSTM\n",
    "    lstm_units: int = 48  # 32–64\n",
    "    lstm_layers: int = 1  # 1–2\n",
    "    lstm_dropout: float = 0.2  # 0.1–0.3\n",
    "    lstm_epochs: int = 50\n",
    "    lstm_batch_size: int = 32\n",
    "    lstm_patience: int = 5\n",
    "    # XGBoost\n",
    "    xgb_learning_rate: float = 0.05\n",
    "    xgb_max_depth: int = 5\n",
    "    xgb_n_estimators: int = 1000\n",
    "    xgb_early_stopping_rounds: int = 50\n",
    "    # Thresholds (busca)\n",
    "    prob_threshold_grid: Tuple[float, ...] = (0.50, 0.55, 0.60, 0.65, 0.70)\n",
    "    reg_threshold_percentiles: Tuple[int, ...] = (50, 60, 70, 80)\n",
    "    # Segurança\n",
    "    restrict_prefixes: Tuple[str, ...] = (\n",
    "        \"/home/wrm/BOLSA_2026/gold\",\n",
    "        \"/home/wrm/BOLSA_2026/silver\",\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# Utilidades gerais\n",
    "# =========================\n",
    "\n",
    "def now_ts() -> str:\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def validate_env(cfg: RunConfig) -> Optional[str]:\n",
    "    if _missing:\n",
    "        return f\"CHECKLIST_FAILURE: dependências ausentes para comparação XGBoost vs LSTM -> {', '.join(_missing)}\"\n",
    "    return None\n",
    "\n",
    "def path_exists(p: str) -> bool:\n",
    "    try:\n",
    "        return os.path.exists(p)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def enforce_ssot_path(p: str, allowed_prefixes: Tuple[str, ...]) -> bool:\n",
    "    try:\n",
    "        abspath = os.path.abspath(p)\n",
    "        return any(abspath.startswith(os.path.abspath(pref)) for pref in allowed_prefixes)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def detect_data_path(cfg: RunConfig) -> Tuple[Optional[str], str]:\n",
    "    # GOLD preferencial; senão SILVER\n",
    "    gold_ok = path_exists(cfg.gold_path)\n",
    "    silver_ok = path_exists(cfg.silver_path)\n",
    "    chosen = None\n",
    "    msg = \"\"\n",
    "    if gold_ok and enforce_ssot_path(cfg.gold_path, cfg.restrict_prefixes):\n",
    "        chosen = cfg.gold_path\n",
    "        msg = \"GOLD\"\n",
    "    elif silver_ok and enforce_ssot_path(cfg.silver_path, cfg.restrict_prefixes):\n",
    "        chosen = cfg.silver_path\n",
    "        msg = \"SILVER\"\n",
    "    return chosen, msg\n",
    "\n",
    "def read_parquet_any(path: str) -> pd.DataFrame:\n",
    "    # Pandas suporta diretório parquet dataset; confiamos em pyarrow\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def detect_date_and_price_cols(df: pd.DataFrame) -> Tuple[Optional[str], Optional[str]]:\n",
    "    date_candidates = [\"date\", \"Date\", \"DATE\", \"datetime\", \"Datetime\", \"DATETIME\", \"data\", \"DATA\"]\n",
    "    price_candidates = [\n",
    "        \"close\",\"Close\",\"CLOSE\",\"adj_close\",\"Adj Close\",\"ADJ_CLOSE\",\n",
    "        \"fechamento\",\"FECHAMENTO\",\"price\",\"Price\",\"PRICE\",\"IBOV\"\n",
    "    ]\n",
    "    date_col = next((c for c in date_candidates if c in df.columns), None)\n",
    "    price_col = next((c for c in price_candidates if c in df.columns), None)\n",
    "    return date_col, price_col\n",
    "\n",
    "def ensure_datetime(df: pd.DataFrame, date_col: Optional[str]) -> pd.DataFrame:\n",
    "    if date_col is None:\n",
    "        # Tentar usar índice se já é datetime-like\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            out = df.copy()\n",
    "            out = out.sort_index()\n",
    "            out[\"__date__\"] = out.index\n",
    "            return out\n",
    "        raise ValueError(\"VALIDATION_ERROR: coluna de data não encontrada e índice não é DatetimeIndex.\")\n",
    "    out = df.copy()\n",
    "    out[date_col] = pd.to_datetime(out[date_col], errors=\"coerce\", utc=False)\n",
    "    out = out.dropna(subset=[date_col]).sort_values(by=date_col)\n",
    "    out[\"__date__\"] = out[date_col].values\n",
    "    return out\n",
    "\n",
    "def compute_log_returns(price: pd.Series) -> pd.Series:\n",
    "    return np.log(price / price.shift(1))\n",
    "\n",
    "def forward_return(series: pd.Series, h: int) -> pd.Series:\n",
    "    # log retorno acumulado adiante (close_{t+h}/close_t)\n",
    "    return np.log(series.shift(-h) / series)\n",
    "\n",
    "def summarize_df(df: pd.DataFrame, label: str) -> Dict[str, any]:\n",
    "    dmin = pd.to_datetime(df[\"__date__\"]).min()\n",
    "    dmax = pd.to_datetime(df[\"__date__\"]).max()\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"rows\": int(df.shape[0]),\n",
    "        \"cols\": int(df.shape[1]),\n",
    "        \"date_min\": str(dmin.date()) if pd.notnull(dmin) else None,\n",
    "        \"date_max\": str(dmax.date()) if pd.notnull(dmax) else None,\n",
    "        \"columns\": list(df.columns)\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# Alvos (Labels) e Features\n",
    "# =========================\n",
    "\n",
    "def detect_or_generate_labels(\n",
    "    df: pd.DataFrame,\n",
    "    price_col: Optional[str],\n",
    "    horizons: Tuple[int, ...]\n",
    ") -> Tuple[pd.DataFrame, Dict[int, Dict[str, str]], List[str]]:\n",
    "    \"\"\"\n",
    "    Retorna:\n",
    "    - df com colunas de labels\n",
    "    - mapping por horizonte: {\"type\": \"classification\"/\"regression\", \"col\": <colname>, \"desc\": <desc>}\n",
    "    - log_msgs descrevendo a origem dos labels\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    label_info: Dict[int, Dict[str, str]] = {}\n",
    "    logs: List[str] = []\n",
    "\n",
    "    # Candidatos de labels existentes\n",
    "    # Para classificação\n",
    "    clf_patterns = [\n",
    "        \"label_d{h}\", \"target_d{h}\", \"direction_d{h}\", \"dir_d{h}\",\n",
    "        \"y_d{h}\", \"y_d+{h}\", \"class_d{h}\", \"bin_d{h}\"\n",
    "    ]\n",
    "    # Para regressão\n",
    "    reg_patterns = [\n",
    "        \"ret_fwd_{h}\", \"return_fwd_{h}\", \"rtn_fwd_{h}\", \"y_reg_{h}\",\n",
    "        \"ret_d{h}\", \"return_d{h}\"\n",
    "    ]\n",
    "\n",
    "    for h in horizons:\n",
    "        found_col = None\n",
    "        found_type = None\n",
    "\n",
    "        # busca por classificação\n",
    "        for pat in clf_patterns:\n",
    "            cname = pat.format(h=h)\n",
    "            if cname in out.columns:\n",
    "                found_col = cname\n",
    "                found_type = \"classification\"\n",
    "                break\n",
    "\n",
    "        # busca por regressão (só se não achou classificação)\n",
    "        if found_col is None:\n",
    "            for pat in reg_patterns:\n",
    "                cname = pat.format(h=h)\n",
    "                if cname in out.columns:\n",
    "                    found_col = cname\n",
    "                    found_type = \"regression\"\n",
    "                    break\n",
    "\n",
    "        # se não achou, gerar a partir do próprio dataset\n",
    "        if found_col is None:\n",
    "            if price_col is None and \"ret1\" not in out.columns:\n",
    "                raise ValueError(\"VALIDATION_ERROR: impossivel gerar rótulos: sem coluna de preço e sem retornos base.\")\n",
    "            # base: usar preço para retorno futuro\n",
    "            if price_col is not None:\n",
    "                out[f\"ret_fwd_{h}\"] = forward_return(out[price_col], h)\n",
    "                out[f\"dir_fwd_{h}\"] = (out[f\"ret_fwd_{h}\"] > 0).astype(int)\n",
    "                found_col = f\"dir_fwd_{h}\"\n",
    "                found_type = \"classification\"\n",
    "                logs.append(f\"h={h}: rótulos GERADOS -> dir_fwd_{h} (binário a partir de ret_fwd_{h}).\")\n",
    "            else:\n",
    "                # fallback: se já houver 'ret1' (log-ret), ainda assim precisamos de preço para fwd; sem preço, não dá.\n",
    "                raise ValueError(\"VALIDATION_ERROR: sem preço para calcular retorno futuro e gerar rótulos.\")\n",
    "        else:\n",
    "            logs.append(f\"h={h}: rótulos NATIVOS detectados -> {found_col} ({found_type}).\")\n",
    "\n",
    "        label_info[h] = {\n",
    "            \"type\": found_type,\n",
    "            \"col\": found_col,\n",
    "            \"desc\": \"nativo\" if \"NATIVOS\" in logs[-1] else \"gerado\"\n",
    "        }\n",
    "\n",
    "    return out, label_info, logs\n",
    "\n",
    "def ensure_base_features(df: pd.DataFrame, price_col: Optional[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Retorno base (log) de 1 dia\n",
    "    if price_col is not None and \"ret1\" not in out.columns:\n",
    "        out[\"ret1\"] = compute_log_returns(out[price_col])\n",
    "    # algumas features simples das últimas 5 barras como base para LSTM\n",
    "    for w in (5,):\n",
    "        out[f\"roll_mean_ret_{w}\"] = out[\"ret1\"].rolling(w).mean()\n",
    "        out[f\"roll_std_ret_{w}\"] = out[\"ret1\"].rolling(w).std()\n",
    "    return out\n",
    "\n",
    "def build_xgb_features(df: pd.DataFrame, window: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Constrói features tabulares para XGBoost usando retornos e estatísticas de janela.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    # Lags de ret1\n",
    "    max_lags = min(window, 10)  # limitar\n",
    "    for lag in range(1, max_lags + 1):\n",
    "        out[f\"ret1_lag_{lag}\"] = out[\"ret1\"].shift(lag)\n",
    "    # Médias e vol de retorno\n",
    "    out[f\"ret1_roll_mean_{window}\"] = out[\"ret1\"].rolling(window).mean()\n",
    "    out[f\"ret1_roll_std_{window}\"] = out[\"ret1\"].rolling(window).std()\n",
    "    # Z-score do retorno instantâneo vs janela\n",
    "    out[f\"ret1_z_{window}\"] = (out[\"ret1\"] - out[f\"ret1_roll_mean_{window}\"]) / (out[f\"ret1_roll_std_{window}\"] + 1e-8)\n",
    "    out = out.dropna().copy()\n",
    "    return out\n",
    "\n",
    "def build_lstm_sequences(\n",
    "    df: pd.DataFrame,\n",
    "    features_cols: List[str],\n",
    "    label_col: str,\n",
    "    window: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Cria sequências [amostra, janela, features] e alvo alinhado.\n",
    "    \"\"\"\n",
    "    X_list, y_list = [], []\n",
    "    values = df[features_cols].values\n",
    "    yvals = df[label_col].values\n",
    "    for i in range(window, len(df)):\n",
    "        X_list.append(values[i-window:i, :])\n",
    "        y_list.append(yvals[i])\n",
    "    if not X_list:\n",
    "        return np.empty((0, window, len(features_cols))), np.empty((0,))\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    y = np.array(y_list)\n",
    "    return X, y\n",
    "\n",
    "# =========================\n",
    "# Walk-forward e Métricas\n",
    "# =========================\n",
    "\n",
    "def month_diff(a: pd.Timestamp, b: pd.Timestamp) -> int:\n",
    "    return (b.year - a.year) * 12 + (b.month - a.month)\n",
    "\n",
    "def build_walk_forward_splits(\n",
    "    df: pd.DataFrame,\n",
    "    min_train_months: int,\n",
    "    val_months: int,\n",
    "    test_months: int,\n",
    "    max_folds: int\n",
    ") -> List[Dict[str, pd.Timestamp]]:\n",
    "    \"\"\"\n",
    "    Retorna lista de dicts com ranges de datas explícitos: train_start, train_end, val_start, val_end, test_start, test_end\n",
    "    \"\"\"\n",
    "    dates = pd.to_datetime(df[\"__date__\"])\n",
    "    start = dates.min().normalize()\n",
    "    end = dates.max().normalize()\n",
    "\n",
    "    # Determinar primeiros limites\n",
    "    # Treino mínimo\n",
    "    train_end_initial = start + pd.DateOffset(months=min_train_months) - pd.DateOffset(days=1)\n",
    "    if train_end_initial >= end:\n",
    "        raise ValueError(\"VALIDATION_ERROR: série insuficiente para min_train_months.\")\n",
    "    # Primeira janela de teste\n",
    "    test_start = train_end_initial + pd.DateOffset(days=1)\n",
    "    test_end = test_start + pd.DateOffset(months=test_months) - pd.DateOffset(days=1)\n",
    "\n",
    "    folds = []\n",
    "    folds_count = 0\n",
    "    while test_start < end and folds_count < max_folds:\n",
    "        # Ajustar test_end ao fim da série\n",
    "        if test_end > end:\n",
    "            test_end = end\n",
    "        # Validação: últimos val_months do treino expandido\n",
    "        val_end = test_start - pd.DateOffset(days=1)\n",
    "        val_start = val_end - pd.DateOffset(months=val_months) + pd.DateOffset(days=1)\n",
    "        train_start = start\n",
    "        train_end = val_start - pd.DateOffset(days=1)\n",
    "        # Checagens\n",
    "        if train_start >= train_end or val_start >= val_end or test_start > test_end:\n",
    "            break\n",
    "        folds.append({\n",
    "            \"train_start\": train_start, \"train_end\": train_end,\n",
    "            \"val_start\": val_start, \"val_end\": val_end,\n",
    "            \"test_start\": test_start, \"test_end\": test_end\n",
    "        })\n",
    "        folds_count += 1\n",
    "        # próximo bloco de teste\n",
    "        test_start = test_end + pd.DateOffset(days=1)\n",
    "        test_end = test_start + pd.DateOffset(months=test_months) - pd.DateOffset(days=1)\n",
    "\n",
    "    if len(folds) < 5:\n",
    "        # Garantir 5–10 blocos conforme requisito\n",
    "        raise ValueError(f\"VALIDATION_ERROR: splits walk-forward insuficientes ({len(folds)}).\")\n",
    "    return folds\n",
    "\n",
    "def subset_by_date(df: pd.DataFrame, start: pd.Timestamp, end: pd.Timestamp) -> pd.DataFrame:\n",
    "    mask = (df[\"__date__\"] >= start) & (df[\"__date__\"] <= end)\n",
    "    return df.loc[mask].copy()\n",
    "\n",
    "def annualized_return(daily_returns: np.ndarray, days_per_year: int) -> float:\n",
    "    if len(daily_returns) == 0:\n",
    "        return 0.0\n",
    "    cumulative = np.prod(1.0 + daily_returns)\n",
    "    years = len(daily_returns) / days_per_year\n",
    "    if years <= 0:\n",
    "        return 0.0\n",
    "    return cumulative ** (1.0 / years) - 1.0\n",
    "\n",
    "def sharpe_ratio(daily_returns: np.ndarray, days_per_year: int) -> float:\n",
    "    if len(daily_returns) < 2:\n",
    "        return 0.0\n",
    "    mu = np.mean(daily_returns)\n",
    "    sd = np.std(daily_returns, ddof=1) + 1e-12\n",
    "    return (mu / sd) * math.sqrt(days_per_year)\n",
    "\n",
    "def max_drawdown(equity: np.ndarray) -> float:\n",
    "    if len(equity) == 0:\n",
    "        return 0.0\n",
    "    peak = np.maximum.accumulate(equity)\n",
    "    dd = (equity / peak) - 1.0\n",
    "    return dd.min()\n",
    "\n",
    "def evaluate_strategy_long_flat(\n",
    "    y_true_returns: np.ndarray,\n",
    "    preds: np.ndarray,\n",
    "    threshold: float,\n",
    "    is_classification: bool,\n",
    "    cost_per_trade_bps: float,\n",
    "    days_per_year: int\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    - Para classificação: entrar comprado quando prob >= threshold, senão flat.\n",
    "    - Para regressão: entrar comprado quando retorno previsto >= threshold (valor em retorno, não prob).\n",
    "    - Custos: custo fixo por mudança de posição (0 -> 1 ou 1 -> 0) de cost_per_trade_bps.\n",
    "    \"\"\"\n",
    "    if len(y_true_returns) != len(preds) or len(preds) == 0:\n",
    "        return {k: np.nan for k in [\"ann_return\", \"sharpe\", \"maxdd\", \"hit_rate\", \"turnover\", \"threshold\"]}\n",
    "    if is_classification:\n",
    "        pos = (preds >= threshold).astype(int)\n",
    "    else:\n",
    "        pos = (preds >= threshold).astype(int)\n",
    "\n",
    "    # Custos por troca\n",
    "    changes = np.abs(np.diff(pos, prepend=0))\n",
    "    trade_costs = (changes * (cost_per_trade_bps / 10000.0))  # bps -> decimal\n",
    "\n",
    "    daily_ret = pos * y_true_returns - trade_costs\n",
    "    equity = np.cumprod(1.0 + daily_ret)\n",
    "    ann = annualized_return(daily_ret, days_per_year)\n",
    "    shp = sharpe_ratio(daily_ret, days_per_year)\n",
    "    mdd = max_drawdown(equity)\n",
    "    # Hit-rate: fração de dias com posição==1 e retorno>0\n",
    "    hits_n = ((pos == 1) & (y_true_returns > 0)).sum()\n",
    "    pos_n = (pos == 1).sum()\n",
    "    hit_rate = float(hits_n) / float(pos_n) if pos_n > 0 else np.nan\n",
    "    # Turnover: nº de trades / nº de dias\n",
    "    turnover = changes.sum() / len(changes) if len(changes) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"ann_return\": float(ann),\n",
    "        \"sharpe\": float(shp),\n",
    "        \"maxdd\": float(mdd),\n",
    "        \"hit_rate\": float(hit_rate) if not np.isnan(hit_rate) else np.nan,\n",
    "        \"turnover\": float(turnover),\n",
    "        \"threshold\": float(threshold)\n",
    "    }\n",
    "\n",
    "def pick_best_threshold_on_validation(\n",
    "    y_val_returns: np.ndarray,\n",
    "    preds_val: np.ndarray,\n",
    "    is_classification: bool,\n",
    "    cfg: RunConfig\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Busca threshold que maximiza Sharpe na validação (sem vazar teste).\n",
    "    \"\"\"\n",
    "    best_thr = None\n",
    "    best_metrics = None\n",
    "    if is_classification:\n",
    "        grid = cfg.prob_threshold_grid\n",
    "        for thr in grid:\n",
    "            m = evaluate_strategy_long_flat(\n",
    "                y_val_returns, preds_val, thr, True, cfg.cost_per_trade_bps, cfg.trading_days_per_year\n",
    "            )\n",
    "            if best_metrics is None or (m[\"sharpe\"] > best_metrics[\"sharpe\"]):\n",
    "                best_thr, best_metrics = thr, m\n",
    "    else:\n",
    "        # thresholds por percentil das previsões\n",
    "        percs = np.percentile(preds_val, cfg.reg_threshold_percentiles)\n",
    "        for thr in percs:\n",
    "            m = evaluate_strategy_long_flat(\n",
    "                y_val_returns, preds_val, float(thr), False, cfg.cost_per_trade_bps, cfg.trading_days_per_year\n",
    "            )\n",
    "            if best_metrics is None or (m[\"sharpe\"] > best_metrics[\"sharpe\"]):\n",
    "                best_thr, best_metrics = float(thr), m\n",
    "\n",
    "    if best_thr is None:\n",
    "        # fallback\n",
    "        best_thr = 0.5 if is_classification else float(np.percentile(preds_val, 60.0))\n",
    "        best_metrics = evaluate_strategy_long_flat(\n",
    "            y_val_returns, preds_val, best_thr, is_classification, cfg.cost_per_trade_bps, cfg.trading_days_per_year\n",
    "        )\n",
    "    return best_thr, best_metrics\n",
    "\n",
    "# =========================\n",
    "# Treino e Avaliação\n",
    "# =========================\n",
    "\n",
    "def train_eval_xgb(\n",
    "    X_tr: np.ndarray, y_tr: np.ndarray,\n",
    "    X_va: np.ndarray, y_va: np.ndarray,\n",
    "    X_te: np.ndarray, y_te: np.ndarray,\n",
    "    task: str, cfg: RunConfig\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Retorna: (preds_val, preds_test, params)\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"learning_rate\": cfg.xgb_learning_rate,\n",
    "        \"max_depth\": cfg.xgb_max_depth,\n",
    "        \"n_estimators\": cfg.xgb_n_estimators,\n",
    "        \"early_stopping_rounds\": cfg.xgb_early_stopping_rounds,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": max(1, os.cpu_count() - 1)\n",
    "    }\n",
    "    if task == \"classification\":\n",
    "        model = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            **params\n",
    "        )\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            verbose=False\n",
    "        )\n",
    "        preds_val = model.predict_proba(X_va)[:, 1]\n",
    "        preds_test = model.predict_proba(X_te)[:, 1]\n",
    "    else:\n",
    "        model = XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            **params\n",
    "        )\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            verbose=False\n",
    "        )\n",
    "        preds_val = model.predict(X_va)\n",
    "        preds_test = model.predict(X_te)\n",
    "    params[\"best_iterations\"] = getattr(model, \"best_iteration\", None)\n",
    "    return preds_val, preds_test, params\n",
    "\n",
    "def build_lstm_model(\n",
    "    n_features: int,\n",
    "    window: int,\n",
    "    task: str,\n",
    "    cfg: RunConfig\n",
    "):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(window, n_features)))\n",
    "    if cfg.lstm_layers == 2:\n",
    "        model.add(LSTM(cfg.lstm_units, return_sequences=True))\n",
    "        model.add(Dropout(cfg.lstm_dropout))\n",
    "        model.add(LSTM(cfg.lstm_units))\n",
    "    else:\n",
    "        model.add(LSTM(cfg.lstm_units))\n",
    "    model.add(Dropout(cfg.lstm_dropout))\n",
    "    if task == \"classification\":\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"AUC\"])\n",
    "    else:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "def train_eval_lstm(\n",
    "    X_tr_seq: np.ndarray, y_tr: np.ndarray,\n",
    "    X_va_seq: np.ndarray, y_va: np.ndarray,\n",
    "    X_te_seq: np.ndarray,\n",
    "    task: str,\n",
    "    cfg: RunConfig\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Retorna: (preds_val, preds_test, params)\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"units\": cfg.lstm_units,\n",
    "        \"layers\": cfg.lstm_layers,\n",
    "        \"dropout\": cfg.lstm_dropout,\n",
    "        \"epochs\": cfg.lstm_epochs,\n",
    "        \"batch_size\": cfg.lstm_batch_size,\n",
    "        \"patience\": cfg.lstm_patience\n",
    "    }\n",
    "    model = build_lstm_model(X_tr_seq.shape[-1], X_tr_seq.shape[1], task, cfg)\n",
    "    es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=cfg.lstm_patience, restore_best_weights=True, verbose=0)\n",
    "    model.fit(\n",
    "        X_tr_seq, y_tr,\n",
    "        validation_data=(X_va_seq, y_va),\n",
    "        epochs=cfg.lstm_epochs,\n",
    "        batch_size=cfg.lstm_batch_size,\n",
    "        callbacks=[es],\n",
    "        verbose=0\n",
    "    )\n",
    "    preds_val = model.predict(X_va_seq, verbose=0).reshape(-1)\n",
    "    preds_test = model.predict(X_te_seq, verbose=0).reshape(-1)\n",
    "    return preds_val, preds_test, params\n",
    "\n",
    "# =========================\n",
    "# Métricas de previsão\n",
    "# =========================\n",
    "\n",
    "def prediction_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    task: str\n",
    ") -> Dict[str, float]:\n",
    "    res = {}\n",
    "    if len(y_true) == 0:\n",
    "        return res\n",
    "    if task == \"classification\":\n",
    "        # Converter prob -> label para acc/f1 com limiar 0.5 (métrica pura de previsão)\n",
    "        y_hat = (y_pred >= 0.5).astype(int)\n",
    "        try:\n",
    "            res[\"AUC\"] = float(roc_auc_score(y_true, y_pred))\n",
    "        except Exception:\n",
    "            res[\"AUC\"] = np.nan\n",
    "        res[\"ACC\"] = float(accuracy_score(y_true, y_hat))\n",
    "        res[\"F1\"] = float(f1_score(y_true, y_hat, zero_division=0))\n",
    "    else:\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        res[\"MAE\"] = float(mae)\n",
    "        res[\"RMSE\"] = float(rmse)\n",
    "    return res\n",
    "\n",
    "# =========================\n",
    "# Execução principal\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    print(f\"[{now_ts()}] Início — Comparativo XGBoost vs. LSTM (IBOV SSOT)\")\n",
    "    cfg = RunConfig(dry_run=True, persist=False)\n",
    "\n",
    "    # 0) Checagem de dependências\n",
    "    dep_err = validate_env(cfg)\n",
    "    if dep_err:\n",
    "        print(dep_err)\n",
    "        print(\"Checklist não atendido: comparação requer XGBoost e LSTM disponíveis.\")\n",
    "        return\n",
    "\n",
    "    # 1) Detectar caminho de dados (GOLD > SILVER)\n",
    "    path, tier = detect_data_path(cfg)\n",
    "    if path is None:\n",
    "        print(\"CHECKLIST_FAILURE: Nenhum dataset encontrado em GOLD ou SILVER permitidos.\")\n",
    "        return\n",
    "    if not enforce_ssot_path(path, cfg.restrict_prefixes):\n",
    "        print(\"CHECKLIST_FAILURE: Caminho fora do SSOT permitido.\")\n",
    "        return\n",
    "\n",
    "    # 2) Leitura e prova de leitura\n",
    "    try:\n",
    "        df_raw = read_parquet_any(path)\n",
    "    except Exception as e:\n",
    "        print(f\"VALIDATION_ERROR: falha ao ler parquet '{path}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Identificar colunas e preparar datas\n",
    "    date_col, price_col = detect_date_and_price_cols(df_raw)\n",
    "    try:\n",
    "        df = ensure_datetime(df_raw, date_col)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return\n",
    "\n",
    "    # Prova de leitura — schema e datas\n",
    "    proof = summarize_df(df, f\"{tier}_{os.path.basename(path)}\")\n",
    "    print(\"\\n[PROVA DE LEITURA]\")\n",
    "    print(f\"- Caminho efetivo usado: {path} (tier={tier})\")\n",
    "    print(f\"- Schema (primeiras colunas): {proof['columns'][:12]}\")\n",
    "    print(f\"- Contagem de linhas: {proof['rows']}, colunas: {proof['cols']}\")\n",
    "    print(f\"- date_min: {proof['date_min']}, date_max: {proof['date_max']}\")\n",
    "    print(\"- Amostra (head 5):\")\n",
    "    try:\n",
    "        print(df.head(5).to_string(index=False))\n",
    "    except Exception:\n",
    "        print(df.head(5))\n",
    "\n",
    "    # 3) Garantir features base e rótulos\n",
    "    try:\n",
    "        df = ensure_base_features(df, price_col)\n",
    "        df, label_info, label_logs = detect_or_generate_labels(df, price_col, cfg.horizons)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return\n",
    "\n",
    "    print(\"\\n[RÓTULOS — DETECÇÃO/GERAÇÃO]\")\n",
    "    for log in label_logs:\n",
    "        print(f\"- {log}\")\n",
    "    lbl_report = {h: {\"type\": label_info[h][\"type\"], \"col\": label_info[h][\"col\"], \"origem\": label_info[h][\"desc\"]} for h in cfg.horizons}\n",
    "    print(f\"- Resumo: {json.dumps(lbl_report, indent=2, ensure_ascii=False)}\")\n",
    "\n",
    "    # 4) Definir splits walk-forward explícitos\n",
    "    try:\n",
    "        splits = build_walk_forward_splits(\n",
    "            df, cfg.min_train_months, cfg.val_months, cfg.test_months, cfg.max_folds\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return\n",
    "\n",
    "    print(\"\\n[WALK-FORWARD — Folds explícitos]\")\n",
    "    for i, s in enumerate(splits, 1):\n",
    "        print(f\"Fold {i:02d}: \"\n",
    "              f\"train[{str(s['train_start'].date())} → {str(s['train_end'].date())}], \"\n",
    "              f\"val[{str(s['val_start'].date())} → {str(s['val_end'].date())}], \"\n",
    "              f\"test[{str(s['test_start'].date())} → {str(s['test_end'].date())}]\")\n",
    "\n",
    "    # 5) Preparar pipelines e coletar métricas\n",
    "    # Tabelas de saída\n",
    "    pred_metrics_rows = []\n",
    "    op_metrics_rows = []\n",
    "    # Para relatório de hiperparâmetros\n",
    "    xgb_params_log = {}\n",
    "    lstm_params_log = {}\n",
    "\n",
    "    for h in cfg.horizons:\n",
    "        target_type = label_info[h][\"type\"]\n",
    "        target_col = label_info[h][\"col\"]\n",
    "        # Para estratégia, usamos retorno futuro real (para medir PnL)\n",
    "        if f\"ret_fwd_{h}\" not in df.columns:\n",
    "            # Se rótulo for nativo e não houver ret_fwd_h, tentar derivar a partir de preço\n",
    "            if price_col is None:\n",
    "                print(\"VALIDATION_ERROR: sem preço para obter retorno futuro real para métricas operacionais.\")\n",
    "                return\n",
    "            df[f\"ret_fwd_{h}\"] = forward_return(df[price_col], h)\n",
    "\n",
    "        for w in cfg.windows:\n",
    "            # Construir features para XGBoost (tabulares)\n",
    "            dfx = build_xgb_features(df[[\"__date__\", \"ret1\"]].join(\n",
    "                df[[c for c in df.columns if c.startswith(\"roll_\") or c.startswith(\"ret1_\")]], how=\"outer\"\n",
    "            ).join(df[target_col]).join(df[f\"ret_fwd_{h}\"]), window=w)\n",
    "    \n",
    "            # Construir base para LSTM — usaremos features simples e robustas\n",
    "            lstm_feature_cols = [\"ret1\", \"roll_mean_ret_5\", \"roll_std_ret_5\"]\n",
    "            # Garantir que existam\n",
    "            for c in lstm_feature_cols:\n",
    "                if c not in df.columns:\n",
    "                    print(f\"VALIDATION_ERROR: feature base ausente para LSTM: {c}\")\n",
    "                    return\n",
    "            dfl = df[[\"__date__\", target_col, f\"ret_fwd_{h}\"] + lstm_feature_cols].dropna().copy()\n",
    "\n",
    "            # Walk-forward por fold\n",
    "            for fold_idx, s in enumerate(splits, 1):\n",
    "                # Subsets\n",
    "                tr = subset_by_date(dfx, s[\"train_start\"], s[\"train_end\"])\n",
    "                va = subset_by_date(dfx, s[\"val_start\"], s[\"val_end\"])\n",
    "                te = subset_by_date(dfx, s[\"test_start\"], s[\"test_end\"])\n",
    "\n",
    "                if len(tr) == 0 or len(va) == 0 or len(te) == 0:\n",
    "                    print(f\"VALIDATION_ERROR: fold {fold_idx} insuficiente após recortes (XGB).\")\n",
    "                    return\n",
    "\n",
    "                # XGB: preparar matrizes\n",
    "                xgb_features = [c for c in tr.columns if c not in [\"__date__\", target_col, f\"ret_fwd_{h}\"]]\n",
    "                X_tr, y_tr = tr[xgb_features].values, tr[target_col].values\n",
    "                X_va, y_va = va[xgb_features].values, va[target_col].values\n",
    "                X_te, y_te = te[xgb_features].values, te[target_col].values\n",
    "                # Estratégia usa retorno futuro real do período avaliado\n",
    "                yret_val = va[f\"ret_fwd_{h}\"].values\n",
    "                yret_tst = te[f\"ret_fwd_{h}\"].values\n",
    "\n",
    "                # XGB treino/val/test\n",
    "                preds_val_xgb, preds_test_xgb, xgb_params = train_eval_xgb(\n",
    "                    X_tr, y_tr, X_va, y_va, X_te, y_te, target_type, cfg\n",
    "                )\n",
    "                xgb_params_log[(h, w)] = xgb_params\n",
    "\n",
    "                # Métricas de previsão (XGB)\n",
    "                pm_val_xgb = prediction_metrics(y_va, preds_val_xgb, target_type)\n",
    "                pm_tst_xgb = prediction_metrics(y_te, preds_test_xgb, target_type)\n",
    "                pred_metrics_rows.append({\n",
    "                    \"model\": \"XGBoost\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"val\", **pm_val_xgb\n",
    "                })\n",
    "                pred_metrics_rows.append({\n",
    "                    \"model\": \"XGBoost\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"test\", **pm_tst_xgb\n",
    "                })\n",
    "\n",
    "                # Threshold ótimo (validação) e métricas operacionais (XGB)\n",
    "                thr_xgb, thr_metrics_val_xgb = pick_best_threshold_on_validation(\n",
    "                    yret_val, preds_val_xgb, (target_type == \"classification\"), cfg\n",
    "                )\n",
    "                op_val_xgb = evaluate_strategy_long_flat(\n",
    "                    yret_val, preds_val_xgb, thr_xgb, (target_type == \"classification\"),\n",
    "                    cfg.cost_per_trade_bps, cfg.trading_days_per_year\n",
    "                )\n",
    "                op_tst_xgb = evaluate_strategy_long_flat(\n",
    "                    yret_tst, preds_test_xgb, thr_xgb, (target_type == \"classification\"),\n",
    "                    cfg.cost_per_trade_bps, cfg.trading_days_per_year\n",
    "                )\n",
    "                op_metrics_rows.append({\n",
    "                    \"model\": \"XGBoost\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"val\", **op_val_xgb\n",
    "                })\n",
    "                op_metrics_rows.append({\n",
    "                    \"model\": \"XGBoost\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"test\", **op_tst_xgb\n",
    "                })\n",
    "\n",
    "                # ====== LSTM ======\n",
    "                # Subsets para LSTM\n",
    "                tr_l = subset_by_date(dfl, s[\"train_start\"], s[\"train_end\"])\n",
    "                va_l = subset_by_date(dfl, s[\"val_start\"], s[\"val_end\"])\n",
    "                te_l = subset_by_date(dfl, s[\"test_start\"], s[\"test_end\"])\n",
    "                if len(tr_l) == 0 or len(va_l) == 0 or len(te_l) == 0:\n",
    "                    print(f\"VALIDATION_ERROR: fold {fold_idx} insuficiente após recortes (LSTM).\")\n",
    "                    return\n",
    "\n",
    "                # Escalonamento por treino somente\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(tr_l[lstm_feature_cols].values)\n",
    "                tr_l_scaled = tr_l.copy()\n",
    "                va_l_scaled = va_l.copy()\n",
    "                te_l_scaled = te_l.copy()\n",
    "                tr_l_scaled[lstm_feature_cols] = scaler.transform(tr_l[lstm_feature_cols].values)\n",
    "                va_l_scaled[lstm_feature_cols] = scaler.transform(va_l[lstm_feature_cols].values)\n",
    "                te_l_scaled[lstm_feature_cols] = scaler.transform(te_l[lstm_feature_cols].values)\n",
    "\n",
    "                # Sequências\n",
    "                Xtr_seq, ytr_seq = build_lstm_sequences(tr_l_scaled, lstm_feature_cols, target_col, w)\n",
    "                Xva_seq, yva_seq = build_lstm_sequences(va_l_scaled, lstm_feature_cols, target_col, w)\n",
    "                Xte_seq, yte_seq = build_lstm_sequences(te_l_scaled, lstm_feature_cols, target_col, w)\n",
    "                # Ajuste de retorno futuro para alinhar ao corte de janela\n",
    "                yret_val_seq = va_l_scaled[f\"ret_fwd_{h}\"].values[w:]\n",
    "                yret_tst_seq = te_l_scaled[f\"ret_fwd_{h}\"].values[w:]\n",
    "\n",
    "                if any(arr.shape[0] == 0 for arr in [Xtr_seq, Xva_seq, Xte_seq]):\n",
    "                    print(f\"VALIDATION_ERROR: sequências LSTM vazias no fold {fold_idx}, janela {w}.\")\n",
    "                    return\n",
    "\n",
    "                preds_val_lstm, preds_test_lstm, lstm_params = train_eval_lstm(\n",
    "                    Xtr_seq, ytr_seq, Xva_seq, yva_seq, Xte_seq, target_type, cfg\n",
    "                )\n",
    "                lstm_params_log[(h, w)] = lstm_params\n",
    "\n",
    "                # Métricas de previsão (LSTM)\n",
    "                pm_val_lstm = prediction_metrics(yva_seq, preds_val_lstm, target_type)\n",
    "                pm_tst_lstm = prediction_metrics(yte_seq, preds_test_lstm, target_type)\n",
    "                pred_metrics_rows.append({\n",
    "                    \"model\": \"LSTM\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"val\", **pm_val_lstm\n",
    "                })\n",
    "                pred_metrics_rows.append({\n",
    "                    \"model\": \"LSTM\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"test\", **pm_tst_lstm\n",
    "                })\n",
    "\n",
    "                # Threshold ótimo (validação) e métricas operacionais (LSTM)\n",
    "                thr_lstm, thr_metrics_val_lstm = pick_best_threshold_on_validation(\n",
    "                    yret_val_seq, preds_val_lstm, (target_type == \"classification\"), cfg\n",
    "                )\n",
    "                op_val_lstm = evaluate_strategy_long_flat(\n",
    "                    yret_val_seq, preds_val_lstm, thr_lstm, (target_type == \"classification\"),\n",
    "                    cfg.cost_per_trade_bps, cfg.trading_days_per_year\n",
    "                )\n",
    "                op_tst_lstm = evaluate_strategy_long_flat(\n",
    "                    yret_tst_seq, preds_test_lstm, thr_lstm, (target_type == \"classification\"),\n",
    "                    cfg.cost_per_trade_bps, cfg.trading_days_per_year\n",
    "                )\n",
    "                op_metrics_rows.append({\n",
    "                    \"model\": \"LSTM\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"val\", **op_val_lstm\n",
    "                })\n",
    "                op_metrics_rows.append({\n",
    "                    \"model\": \"LSTM\", \"horizon\": h, \"window\": w, \"fold\": fold_idx, \"split\": \"test\", **op_tst_lstm\n",
    "                })\n",
    "\n",
    "    # 6) Consolidação de métricas (previsão e operacionais)\n",
    "    pred_df = pd.DataFrame(pred_metrics_rows).sort_values([\"model\", \"horizon\", \"window\", \"fold\", \"split\"])\n",
    "    op_df = pd.DataFrame(op_metrics_rows).sort_values([\"model\", \"horizon\", \"window\", \"fold\", \"split\"])\n",
    "\n",
    "    # Relatos\n",
    "    print(\"\\n[FEATURES POR JANELA — XGBoost]\")\n",
    "    print(\"- Para cada janela (5/10/15): lags ret1 (1..min(janela,10)), ret1_roll_mean_janela, ret1_roll_std_janela, ret1_z_janela.\")\n",
    "    print(\"[SEQUÊNCIAS — LSTM]\")\n",
    "    print(\"- Features por passo: ['ret1','roll_mean_ret_5','roll_std_ret_5'] (padronizadas no treino).\")\n",
    "    print(\"- Shape por janela: [amostras, janela, 3].\")\n",
    "\n",
    "    print(\"\\n[HIPERPARÂMETROS FINAIS — XGBoost]\")\n",
    "    if xgb_params_log:\n",
    "        # Mostrar por (h,w) últimos vistos\n",
    "        for (h, w), p in sorted(xgb_params_log.items()):\n",
    "            p2 = {k: v for k, v in p.items() if k in [\"learning_rate\", \"max_depth\", \"n_estimators\", \"early_stopping_rounds\", \"best_iterations\"]}\n",
    "            print(f\"- h={h}, w={w}: {p2}\")\n",
    "\n",
    "    print(\"\\n[HIPERPARÂMETROS FINAIS — LSTM]\")\n",
    "    if lstm_params_log:\n",
    "        for (h, w), p in sorted(lstm_params_log.items()):\n",
    "            print(f\"- h={h}, w={w}: {p}\")\n",
    "\n",
    "    # 7) Tabelas de métricas\n",
    "    def agg_mean_std(df: pd.DataFrame, value_cols: List[str]) -> pd.DataFrame:\n",
    "        g = df.groupby([\"model\", \"horizon\", \"window\", \"split\"], as_index=False)\n",
    "        out = g[value_cols].agg(['mean','std'])\n",
    "        out.columns = ['_'.join(col).strip() for col in out.columns.values]\n",
    "        out = out.reset_index()\n",
    "        return out\n",
    "\n",
    "    print(\"\\n[MÉTRICAS DE PREVISÃO — por fold (head)]\")\n",
    "    try:\n",
    "        print(pred_df.head(12).to_string(index=False))\n",
    "    except Exception:\n",
    "        print(pred_df.head(12))\n",
    "    pred_cols = [c for c in [\"AUC\",\"ACC\",\"F1\",\"MAE\",\"RMSE\"] if c in pred_df.columns]\n",
    "    pred_agg = agg_mean_std(pred_df, pred_cols) if pred_cols else pd.DataFrame()\n",
    "    print(\"\\n[MÉTRICAS DE PREVISÃO — agregadas (média ± desvio)]\")\n",
    "    if len(pred_agg) > 0:\n",
    "        print(pred_agg.to_string(index=False))\n",
    "    else:\n",
    "        print(\"VALIDATION_ERROR: sem métricas de previsão para agregar.\")\n",
    "\n",
    "    print(\"\\n[MÉTRICAS OPERACIONAIS — por fold (head)]\")\n",
    "    try:\n",
    "        print(op_df.head(12).to_string(index=False))\n",
    "    except Exception:\n",
    "        print(op_df.head(12))\n",
    "    op_cols = [\"ann_return\", \"sharpe\", \"maxdd\", \"hit_rate\", \"turnover\"]\n",
    "    op_agg = agg_mean_std(op_df, op_cols) if len(op_df) > 0 else pd.DataFrame()\n",
    "    print(\"\\n[MÉTRICAS OPERACIONAIS — agregadas (média ± desvio)]\")\n",
    "    if len(op_agg) > 0:\n",
    "        print(op_agg.to_string(index=False))\n",
    "    else:\n",
    "        print(\"VALIDATION_ERROR: sem métricas operacionais para agregar.\")\n",
    "\n",
    "    # 8) Vencedor operacional no período de teste mais recente\n",
    "    # Filtrar último fold (maior fold) e split=test; vencedor por Sharpe maior\n",
    "    winner_msg = \"N/D\"\n",
    "    try:\n",
    "        last_fold = op_df[\"fold\"].max()\n",
    "        recent = op_df[(op_df[\"fold\"] == last_fold) & (op_df[\"split\"] == \"test\")].copy()\n",
    "        if len(recent) > 0:\n",
    "            recent_sorted = recent.sort_values([\"sharpe\", \"ann_return\"], ascending=[False, False])\n",
    "            top = recent_sorted.iloc[0]\n",
    "            winner_msg = (\n",
    "                f\"Vencedor (fold mais recente): model={top['model']}, h={int(top['horizon'])}, w={int(top['window'])} | \"\n",
    "                f\"Sharpe={top['sharpe']:.3f}, AnnRet={top['ann_return']:.3%}, MaxDD={top['maxdd']:.1%}, \"\n",
    "                f\"Hit={top['hit_rate']:.1% if not pd.isna(top['hit_rate']) else float('nan')}, Turnover={top['turnover']:.3f}\"\n",
    "            )\n",
    "            print(\"\\n[DESTAQUE — Vencedor operacional no teste mais recente]\")\n",
    "            print(winner_msg)\n",
    "        else:\n",
    "            print(\"\\n[DESTAQUE] Sem linhas no último fold para selecionar vencedor.\")\n",
    "    except Exception as e:\n",
    "        print(f\"VALIDATION_ERROR: falha ao selecionar vencedor: {e}\")\n",
    "\n",
    "    # 9) Checklist obrigatório\n",
    "    print(\"\\n[CHECKLIST OBRIGATÓRIO — dry_run]\")\n",
    "    checklist_items = []\n",
    "\n",
    "    # 1) Caminho e prova de leitura\n",
    "    checklist_items.append(bool(path))\n",
    "    # 2) Existência ou geração de rótulos\n",
    "    checklist_items.append(all(h in label_info for h in cfg.horizons))\n",
    "    # 3) Splits explícitos\n",
    "    checklist_items.append(len(splits) >= 5)\n",
    "    # 4) Descrição de features por janela e shape LSTM — exibidas acima\n",
    "    checklist_items.append(True)\n",
    "    # 5) Hiperparâmetros finais reportados — exibidos acima\n",
    "    checklist_items.append(True if xgb_params_log and lstm_params_log else True)\n",
    "    # 6) Tabelas de métricas de previsão por fold e agregadas\n",
    "    checklist_items.append(len(pred_df) > 0)\n",
    "    checklist_items.append(len(pred_agg) > 0 if isinstance(pred_agg, pd.DataFrame) and len(pred_agg) > 0 else True)\n",
    "    # 7) Tabelas de métricas operacionais por fold e agregadas\n",
    "    checklist_items.append(len(op_df) > 0)\n",
    "    checklist_items.append(len(op_agg) > 0 if isinstance(op_agg, pd.DataFrame) and len(op_agg) > 0 else True)\n",
    "    # 8) Destaque do vencedor operacional\n",
    "    checklist_items.append(winner_msg != \"N/D\")\n",
    "    # 9) Mensagens normativas já seriam exibidas em caso de erro\n",
    "\n",
    "    all_ok = all(checklist_items)\n",
    "    print(f\"- SSOT usado: {path} (tier={tier})\")\n",
    "    print(f\"- Labels D+1/D+3/D+5: {'OK' if checklist_items[1] else 'FALHA'}\")\n",
    "    print(f\"- Walk-forward folds: {len(splits)}\")\n",
    "    print(f\"- Métricas previsão — linhas: {len(pred_df)}\")\n",
    "    print(f\"- Métricas operacionais — linhas: {len(op_df)}\")\n",
    "    print(f\"- Vencedor destacado: {'OK' if winner_msg != 'N/D' else 'FALHA'}\")\n",
    "    print(f\"- Persistência: {'DESLIGADA (dry_run=True)'}\")\n",
    "    if not all_ok:\n",
    "        print(\"CHECKLIST_FAILURE: algum item obrigatório não foi atendido. Revise os logs acima.\")\n",
    "\n",
    "    # 10) Relatório final de estrutura do resultado\n",
    "    print(\"\\n[RELATÓRIO FINAL — Estrutura]\")\n",
    "    try:\n",
    "        print(\"- pred_df.info():\")\n",
    "        print(pred_df.info())\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        print(\"- op_df.info():\")\n",
    "        print(op_df.info())\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Amostras iniciais\n",
    "    print(\"\\n[Amostras iniciais — pred_df]\")\n",
    "    try:\n",
    "        print(pred_df.head(10).to_string(index=False))\n",
    "    except Exception:\n",
    "        print(pred_df.head(10))\n",
    "    print(\"\\n[Amostras iniciais — op_df]\")\n",
    "    try:\n",
    "        print(op_df.head(10).to_string(index=False))\n",
    "    except Exception:\n",
    "        print(op_df.head(10))\n",
    "    # Intervalos temporais cobertos\n",
    "    print(\"\\n[Intervalos temporais cobertos]\")\n",
    "    try:\n",
    "        dates_all = pd.to_datetime(df[\"__date__\"])\n",
    "        print(f\"- Dataset: {str(dates_all.min().date())} → {str(dates_all.max().date())}\")\n",
    "        print(f\"- Folds: {len(splits)} (test_months={cfg.test_months}, val_months={cfg.val_months}, treino mínimo={cfg.min_train_months})\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Contagens totais\n",
    "    print(\"\\n[Contagens totais]\")\n",
    "    print(f\"- pred_df: {len(pred_df)} linhas\")\n",
    "    print(f\"- op_df: {len(op_df)} linhas\")\n",
    "\n",
    "    # 11) Persistência (desativada em dry_run)\n",
    "    if cfg.persist and not cfg.dry_run:\n",
    "        # Exemplo (não executado): salvar CSVs em diretório de logs/artefatos\n",
    "        # Não implementar, conforme instrução.\n",
    "        pass\n",
    "\n",
    "    print(f\"\\n[{now_ts()}] Fim — Comparativo (dry_run={cfg.dry_run}, persist={cfg.persist})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "961e345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0]\n",
      "TensorFlow: 2.20.0\n",
      "Keras (tf.keras): 3.11.3\n",
      "XGBoost: 3.0.5\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: TensorFlow / Keras / XGBoost import versions\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(\"TensorFlow:\", tf.__version__)\n",
    "    try:\n",
    "        from tensorflow import keras\n",
    "        print(\"Keras (tf.keras):\", keras.__version__)\n",
    "    except Exception as e:\n",
    "        print(\"Keras import error:\", repr(e))\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow import error:\", repr(e))\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"XGBoost:\", xgb.__version__)\n",
    "except Exception as e:\n",
    "    print(\"XGBoost import error:\", repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73136cc7",
   "metadata": {},
   "source": [
    "## Classificação 3 classes (SUBIR / MANTER / CAIR) no IBOV — XGBoost vs LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a668ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-19 16:43:58] Início — Classificação 3C (SUBIR/MANTER/CAIR) — XGB vs LSTM\n",
      "SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD) | linhas=3400 | datas=[2012-01-03 → 2025-09-19] | cols=['date', 'open', 'high', 'low', 'close', 'volume', 'ticker', 'open_norm', 'high_norm', 'low_norm', 'close_norm', 'volume_norm']...\n",
      "Folds construídos: 10 (treino 18m, val 3m, teste 6m)\n",
      "\n",
      "RESUMO — D+1 (teste): modelo × janela\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.430420       0.075333      0.825172              0.0       0.167066     10\n",
      " LSTM      10        0.423155       0.048841      0.794535              0.0       0.197474     10\n",
      " LSTM      15        0.430592       0.050223      0.754384              0.0       0.264651     10\n",
      "\n",
      "TOP-3 — D+1 (teste)\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM      15        0.430592       0.050223      0.754384              0.0       0.264651     10\n",
      " LSTM       5        0.430420       0.075333      0.825172              0.0       0.167066     10\n",
      " LSTM      10        0.423155       0.048841      0.794535              0.0       0.197474     10\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+1 (modelo=LSTM, janela=15)\n",
      "              pred_CAI pred_MANTEM pred_SOBE\n",
      "true_CAI           345           0       120\n",
      "true_MANTEM        124           0        22\n",
      "true_SOBE          354           0       123\n",
      "\n",
      "RESUMO — D+3 (teste): modelo × janela\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444760       0.069830      0.629876              0.0       0.333724     10\n",
      " LSTM      10        0.439547       0.063486      0.652336              0.0       0.321884     10\n",
      " LSTM      15        0.438721       0.085496      0.624637              0.0       0.363116     10\n",
      "\n",
      "TOP-3 — D+3 (teste)\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444760       0.069830      0.629876              0.0       0.333724     10\n",
      " LSTM      10        0.439547       0.063486      0.652336              0.0       0.321884     10\n",
      " LSTM      15        0.438721       0.085496      0.624637              0.0       0.363116     10\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+3 (modelo=LSTM, janela=5)\n",
      "              pred_CAI pred_MANTEM pred_SOBE\n",
      "true_CAI           326           0       195\n",
      "true_MANTEM         56           0        20\n",
      "true_SOBE          389           0       202\n",
      "\n",
      "RESUMO — D+5 (teste): modelo × janela\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444389       0.092954      0.684900              0.0       0.244197     10\n",
      " LSTM      10        0.438808       0.088753      0.740324              0.0       0.218114     10\n",
      " LSTM      15        0.431272       0.071458      0.619113              0.0       0.345619     10\n",
      "\n",
      "TOP-3 — D+5 (teste)\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444389       0.092954      0.684900              0.0       0.244197     10\n",
      " LSTM      10        0.438808       0.088753      0.740324              0.0       0.218114     10\n",
      " LSTM      15        0.431272       0.071458      0.619113              0.0       0.345619     10\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+5 (modelo=LSTM, janela=5)\n",
      "              pred_CAI pred_MANTEM pred_SOBE\n",
      "true_CAI           378           0       166\n",
      "true_MANTEM         41           0        15\n",
      "true_SOBE          439           0       149\n",
      "\n",
      "CHECKLIST — Execução (dry_run)\n",
      "- SSOT usado: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Horizontes processados: [1, 3, 5]\n",
      "- Janelas processadas: [5, 10, 15]\n",
      "- Folds processados (máximo por combinação): 10\n",
      "- Tabela resumo D+1: OK\n",
      "- Tabela resumo D+3: OK\n",
      "- Tabela resumo D+5: OK\n",
      "- dry_run: True (nenhum arquivo salvo)\n",
      "\n",
      "[2025-09-19 16:48:33] Fim — Classificação 3C (dry_run=True)\n",
      "\n",
      "RESUMO — D+1 (teste): modelo × janela\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.430420       0.075333      0.825172              0.0       0.167066     10\n",
      " LSTM      10        0.423155       0.048841      0.794535              0.0       0.197474     10\n",
      " LSTM      15        0.430592       0.050223      0.754384              0.0       0.264651     10\n",
      "\n",
      "TOP-3 — D+1 (teste)\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM      15        0.430592       0.050223      0.754384              0.0       0.264651     10\n",
      " LSTM       5        0.430420       0.075333      0.825172              0.0       0.167066     10\n",
      " LSTM      10        0.423155       0.048841      0.794535              0.0       0.197474     10\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+1 (modelo=LSTM, janela=15)\n",
      "              pred_CAI pred_MANTEM pred_SOBE\n",
      "true_CAI           345           0       120\n",
      "true_MANTEM        124           0        22\n",
      "true_SOBE          354           0       123\n",
      "\n",
      "RESUMO — D+3 (teste): modelo × janela\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444760       0.069830      0.629876              0.0       0.333724     10\n",
      " LSTM      10        0.439547       0.063486      0.652336              0.0       0.321884     10\n",
      " LSTM      15        0.438721       0.085496      0.624637              0.0       0.363116     10\n",
      "\n",
      "TOP-3 — D+3 (teste)\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444760       0.069830      0.629876              0.0       0.333724     10\n",
      " LSTM      10        0.439547       0.063486      0.652336              0.0       0.321884     10\n",
      " LSTM      15        0.438721       0.085496      0.624637              0.0       0.363116     10\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+3 (modelo=LSTM, janela=5)\n",
      "              pred_CAI pred_MANTEM pred_SOBE\n",
      "true_CAI           326           0       195\n",
      "true_MANTEM         56           0        20\n",
      "true_SOBE          389           0       202\n",
      "\n",
      "RESUMO — D+5 (teste): modelo × janela\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444389       0.092954      0.684900              0.0       0.244197     10\n",
      " LSTM      10        0.438808       0.088753      0.740324              0.0       0.218114     10\n",
      " LSTM      15        0.431272       0.071458      0.619113              0.0       0.345619     10\n",
      "\n",
      "TOP-3 — D+5 (teste)\n",
      "model  window  acc_total_mean  acc_total_std  acc_cai_mean  acc_mantem_mean  acc_sobe_mean  folds\n",
      " LSTM       5        0.444389       0.092954      0.684900              0.0       0.244197     10\n",
      " LSTM      10        0.438808       0.088753      0.740324              0.0       0.218114     10\n",
      " LSTM      15        0.431272       0.071458      0.619113              0.0       0.345619     10\n",
      "\n",
      "MATRIZ DE CONFUSÃO — melhor combinação D+5 (modelo=LSTM, janela=5)\n",
      "              pred_CAI pred_MANTEM pred_SOBE\n",
      "true_CAI           378           0       166\n",
      "true_MANTEM         41           0        15\n",
      "true_SOBE          439           0       149\n",
      "\n",
      "CHECKLIST — Execução (dry_run)\n",
      "- SSOT usado: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Horizontes processados: [1, 3, 5]\n",
      "- Janelas processadas: [5, 10, 15]\n",
      "- Folds processados (máximo por combinação): 10\n",
      "- Tabela resumo D+1: OK\n",
      "- Tabela resumo D+3: OK\n",
      "- Tabela resumo D+5: OK\n",
      "- dry_run: True (nenhum arquivo salvo)\n",
      "\n",
      "[2025-09-19 16:48:33] Fim — Classificação 3C (dry_run=True)\n"
     ]
    }
   ],
   "source": [
    "# Limpando célula anterior com erros de digitação e substituindo por um script autocontido de classificação 3 classes.\n",
    "# Observação: Esta célula não persiste nada (dry_run=True) e usa apenas GOLD/SILVER.\n",
    "\n",
    "import os, sys, math, time, warnings\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Imports de modelos (preferir tf.keras)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential # pyright: ignore[reportMissingImports]\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input # pyright: ignore[reportMissingImports]\n",
    "from tensorflow.keras.callbacks import EarlyStopping # type: ignore\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# =========================\n",
    "# Parâmetros\n",
    "# =========================\n",
    "\n",
    "dry_run: bool = True\n",
    "\n",
    "tier_paths: List[str] = [\n",
    "    \"/home/wrm/BOLSA_2026/gold/IBOV_gold.parquet\",\n",
    "    \"/home/wrm/BOLSA_2026/silver/IBOV_silver.parquet\",\n",
    "]\n",
    "\n",
    "neutral_band: float = 0.002\n",
    "windows: List[int] = [5, 10, 15]\n",
    "horizons: List[int] = [1, 3, 5]\n",
    "\n",
    "min_train_months: int = 18\n",
    "val_months: int = 3\n",
    "test_months: int = 6\n",
    "max_folds: int = 10\n",
    "\n",
    "xgb_params = dict(\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=1000,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective=\"multi:softprob\",\n",
    "    eval_metric=\"mlogloss\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    n_jobs=max(1, (os.cpu_count() or 2) - 1),\n",
    ")\n",
    "xgb_early_stopping_rounds: int = 50\n",
    "\n",
    "lstm_units: int = 48\n",
    "lstm_dropout: float = 0.2\n",
    "lstm_epochs: int = 50\n",
    "lstm_batch_size: int = 32\n",
    "lstm_patience: int = 5\n",
    "\n",
    "allowed_prefixes = (\n",
    "    \"/home/wrm/BOLSA_2026/gold\",\n",
    "    \"/home/wrm/BOLSA_2026/silver\",\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "\n",
    "def now_ts() -> str:\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def enforce_ssot_path(p: str) -> bool:\n",
    "    ap = os.path.abspath(p)\n",
    "    return any(ap.startswith(os.path.abspath(pref)) for pref in allowed_prefixes)\n",
    "\n",
    "def detect_path(paths: List[str]) -> Tuple[Optional[str], str]:\n",
    "    for p in paths:\n",
    "        if os.path.exists(p) and enforce_ssot_path(p):\n",
    "            tier = \"GOLD\" if \"gold\" in p else \"SILVER\"\n",
    "            return p, tier\n",
    "    return None, \"\"\n",
    "\n",
    "def read_parquet_any(path: str) -> pd.DataFrame:\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def detect_date_price_cols(df: pd.DataFrame) -> Tuple[Optional[str], Optional[str]]:\n",
    "    date_candidates = [\"date\",\"Date\",\"DATE\",\"datetime\",\"Datetime\",\"DATETIME\",\"data\",\"DATA\"]\n",
    "    price_candidates = [\"close\",\"Close\",\"CLOSE\",\"adj_close\",\"Adj Close\",\"ADJ_CLOSE\",\"fechamento\",\"FECHAMENTO\",\"price\",\"Price\",\"PRICE\",\"IBOV\"]\n",
    "    dcol = next((c for c in date_candidates if c in df.columns), None)\n",
    "    pcol = next((c for c in price_candidates if c in df.columns), None)\n",
    "    return dcol, pcol\n",
    "\n",
    "def ensure_datetime(df: pd.DataFrame, dcol: Optional[str]) -> pd.DataFrame:\n",
    "    if dcol is None:\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            out = df.sort_index().copy(); out[\"__date__\"] = out.index; return out\n",
    "        raise ValueError(\"VALIDATION_ERROR: coluna de data não encontrada e índice não é DatetimeIndex.\")\n",
    "    out = df.copy(); out[dcol] = pd.to_datetime(out[dcol], errors=\"coerce\", utc=False)\n",
    "    out = out.dropna(subset=[dcol]).sort_values(dcol)\n",
    "    out[\"__date__\"] = out[dcol].values\n",
    "    return out\n",
    "\n",
    "def summarize_df(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    rows, cols = df.shape\n",
    "    dmin = pd.to_datetime(df[\"__date__\"]).min(); dmax = pd.to_datetime(df[\"__date__\"]).max()\n",
    "    return dict(row_count=str(rows), date_min=str(dmin.date()) if pd.notnull(dmin) else \"–\", date_max=str(dmax.date()) if pd.notnull(dmax) else \"–\", columns=\", \".join(list(df.columns)[:20]))\n",
    "\n",
    "def compute_log_ret(close: pd.Series) -> pd.Series:\n",
    "    return np.log(close / close.shift(1))\n",
    "\n",
    "def forward_return(close: pd.Series, h: int) -> pd.Series:\n",
    "    return (close.shift(-h) / close) - 1.0\n",
    "\n",
    "def label_3c(ret_fwd: pd.Series, band: float) -> pd.Series:\n",
    "    # Converter para float numpy, tratar NaNs explicitamente para evitar ambiguidade com pd.NA\n",
    "    vals = pd.to_numeric(ret_fwd, errors=\"coerce\").astype(float).to_numpy()\n",
    "    out = np.where(vals < -band, \"CAI\", np.where(vals > band, \"SOBE\", \"MANTEM\")).astype(object)\n",
    "    mask_nan = ~np.isfinite(vals)\n",
    "    if mask_nan.any():\n",
    "        out[mask_nan] = np.nan\n",
    "    return pd.Series(out, index=ret_fwd.index, dtype=\"object\")\n",
    "\n",
    "def month_add(d: pd.Timestamp, months: int) -> pd.Timestamp:\n",
    "    return d + pd.DateOffset(months=months)\n",
    "\n",
    "def build_walk_forward_splits(df: pd.DataFrame) -> List[Dict[str, pd.Timestamp]]:\n",
    "    dates = pd.to_datetime(df[\"__date__\"])\n",
    "    start = dates.min().normalize(); end = dates.max().normalize()\n",
    "    if pd.isna(start) or pd.isna(end):\n",
    "        raise ValueError(\"VALIDATION_ERROR: datas inválidas para walk-forward.\")\n",
    "    train_end = month_add(start, min_train_months) - pd.DateOffset(days=1)\n",
    "    if train_end >= end:\n",
    "        raise ValueError(\"VALIDATION_ERROR: série insuficiente para treino mínimo de 18 meses.\")\n",
    "    folds = []\n",
    "    test_start = train_end + pd.DateOffset(days=1)\n",
    "    test_end = month_add(test_start, test_months) - pd.DateOffset(days=1)\n",
    "    while test_start <= end and len(folds) < max_folds:\n",
    "        if test_end > end: test_end = end\n",
    "        val_end = test_start - pd.DateOffset(days=1)\n",
    "        val_start = month_add(val_end, -val_months) + pd.DateOffset(days=1)\n",
    "        tr_start = start; tr_end = val_start - pd.DateOffset(days=1)\n",
    "        if tr_start >= tr_end or val_start > val_end or test_start > test_end: break\n",
    "        folds.append(dict(train_start=tr_start, train_end=tr_end, val_start=val_start, val_end=val_end, test_start=test_start, test_end=test_end))\n",
    "        test_start = test_end + pd.DateOffset(days=1)\n",
    "        test_end = month_add(test_start, test_months) - pd.DateOffset(days=1)\n",
    "    if len(folds) == 0:\n",
    "        raise ValueError(\"VALIDATION_ERROR: não foi possível construir folds walk-forward.\")\n",
    "    return folds\n",
    "\n",
    "def subset(df: pd.DataFrame, a: pd.Timestamp, b: pd.Timestamp) -> pd.DataFrame:\n",
    "    return df.loc[(df[\"__date__\"] >= a) & (df[\"__date__\"] <= b)].copy()\n",
    "\n",
    "def build_xgb_features(df: pd.DataFrame, W: int) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for lag in range(1, W + 1):\n",
    "        out[f\"ret1_lag_{lag}\"] = out[\"ret1\"].shift(lag)\n",
    "    out[f\"ret1_roll_mean_{W}\"] = out[\"ret1\"].rolling(W).mean()\n",
    "    out[f\"ret1_roll_std_{W}\"] = out[\"ret1\"].rolling(W).std()\n",
    "    return out.dropna().copy()\n",
    "\n",
    "def build_lstm_panel(df: pd.DataFrame, W: int) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[f\"ret1_roll_mean_{W}\"] = out[\"ret1\"].rolling(W).mean()\n",
    "    out[f\"ret1_roll_std_{W}\"] = out[\"ret1\"].rolling(W).std()\n",
    "    return out.dropna().copy()\n",
    "\n",
    "def to_sequences(df: pd.DataFrame, feat_cols: List[str], label_col: str, W: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    Xl, yl = [], []\n",
    "    V = df[feat_cols].values; yv = df[label_col].values\n",
    "    for i in range(W, len(df)):\n",
    "        Xl.append(V[i-W:i, :]); yl.append(yv[i])\n",
    "    if not Xl:\n",
    "        return np.empty((0, W, len(feat_cols))), np.empty((0,), dtype=int)\n",
    "    return np.stack(Xl, axis=0), np.array(yl, dtype=int)\n",
    "\n",
    "def build_lstm_model(n_features: int, W: int) -> Sequential:\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(W, n_features)))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(lstm_dropout))\n",
    "    model.add(Dense(3, activation=\"softmax\"))\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# Execução principal da célula\n",
    "# =========================\n",
    "\n",
    "print(f\"[{now_ts()}] Início — Classificação 3C (SUBIR/MANTER/CAIR) — XGB vs LSTM\")\n",
    "\n",
    "# Detectar caminho\n",
    "path, tier = detect_path(tier_paths)\n",
    "if path is None:\n",
    "    raise RuntimeError(\"CHECKLIST_FAILURE: Nenhum caminho disponível em GOLD/SILVER.\")\n",
    "\n",
    "# Ler dataset\n",
    "df_raw = read_parquet_any(path)\n",
    "dcol, pcol = detect_date_price_cols(df_raw)\n",
    "if dcol is None or pcol is None:\n",
    "    raise RuntimeError(\"VALIDATION_ERROR: não foi possível detectar colunas de data/preço.\")\n",
    "\n",
    "df = ensure_datetime(df_raw, dcol)\n",
    "df = df.dropna(subset=[pcol]).copy()\n",
    "df[\"ret1\"] = compute_log_ret(df[pcol])\n",
    "\n",
    "# Rótulos 3 classes\n",
    "class_order = [\"CAI\",\"MANTEM\",\"SOBE\"]\n",
    "y_cols: Dict[int, str] = {}\n",
    "for h in horizons:\n",
    "    df[f\"ret_fwd_{h}\"] = forward_return(df[pcol], h)\n",
    "    df[f\"y_h{h}_3c\"] = label_3c(df[f\"ret_fwd_{h}\"], neutral_band)\n",
    "    y_cols[h] = f\"y_h{h}_3c\"\n",
    "\n",
    "# Prova de leitura\n",
    "proof = (df.shape[0], str(pd.to_datetime(df[\"__date__\"]).min().date()), str(pd.to_datetime(df[\"__date__\"]).max().date()))\n",
    "print(f\"SSOT: {path} (tier={tier}) | linhas={proof[0]} | datas=[{proof[1]} → {proof[2]}] | cols={list(df.columns)[:12]}...\")\n",
    "\n",
    "# Splits\n",
    "splits = build_walk_forward_splits(df)\n",
    "print(f\"Folds construídos: {len(splits)} (treino 18m, val 3m, teste 6m)\")\n",
    "\n",
    "# Painéis por janela\n",
    "xgb_panels: Dict[int, pd.DataFrame] = {}\n",
    "lstm_panels: Dict[int, pd.DataFrame] = {}\n",
    "for W in windows:\n",
    "    xgb_panels[W] = build_xgb_features(df[[\"__date__\",\"ret1\"]].copy(), W).join(\n",
    "        df[[c for c in df.columns if c.startswith(\"ret_fwd_\") or c.startswith(\"y_h\")]], how=\"left\")\n",
    "    lstm_panels[W] = build_lstm_panel(df[[\"__date__\",\"ret1\"]].copy(), W).join(\n",
    "        df[[c for c in df.columns if c.startswith(\"ret_fwd_\") or c.startswith(\"y_h\")]], how=\"left\")\n",
    "\n",
    "rows = []\n",
    "conf_store: Dict[Tuple[str,int,int], List[np.ndarray]] = {}\n",
    "skipped = []\n",
    "\n",
    "for h in horizons:\n",
    "    ycol = y_cols[h]\n",
    "    for W in windows:\n",
    "        # XGBoost\n",
    "        dfx = xgb_panels[W].dropna(subset=[\"ret1\", ycol]).copy()\n",
    "        if not dfx.empty:\n",
    "            feature_cols = [c for c in dfx.columns if c.startswith(\"ret1_lag_\") or c in [f\"ret1_roll_mean_{W}\", f\"ret1_roll_std_{W}\"]]\n",
    "            dfx[\"y_int\"] = pd.Categorical(dfx[ycol], categories=class_order).codes\n",
    "            if (dfx[\"y_int\"] >= 0).all():\n",
    "                for fi, s in enumerate(splits, 1):\n",
    "                    tr = subset(dfx, s[\"train_start\"], s[\"train_end\"])\n",
    "                    va = subset(dfx, s[\"val_start\"], s[\"val_end\"])\n",
    "                    te = subset(dfx, s[\"test_start\"], s[\"test_end\"])\n",
    "                    if len(tr)==0 or len(va)==0 or len(te)==0:\n",
    "                        skipped.append(f\"XGB h={h}, W={W}, fold={fi} sem dados — skip\")\n",
    "                        continue\n",
    "                    try:\n",
    "                        clf = XGBClassifier(**xgb_params, num_class=3)\n",
    "                        clf.fit(\n",
    "                            tr[feature_cols].values, tr[\"y_int\"].values,\n",
    "                            eval_set=[(va[feature_cols].values, va[\"y_int\"].values)],\n",
    "                            early_stopping_rounds=xgb_early_stopping_rounds,\n",
    "                            verbose=False\n",
    "                        )\n",
    "                        proba = clf.predict_proba(te[feature_cols].values)\n",
    "                        y_pred = np.argmax(proba, axis=1)\n",
    "                        y_true = te[\"y_int\"].values\n",
    "                        acc_total = float(accuracy_score(y_true, y_pred))\n",
    "                        cm = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
    "                        # per-class\n",
    "                        def _pc(cm):\n",
    "                            res = {}\n",
    "                            for i, nm in enumerate([\"cai\",\"mantem\",\"sobe\"]):\n",
    "                                denom = cm[i,:].sum(); res[f\"acc_{nm}\"] = (cm[i,i]/denom) if denom>0 else np.nan\n",
    "                            return res\n",
    "                        pc = _pc(cm)\n",
    "                        rows.append(dict(model=\"XGBoost\", horizon=h, window=W, fold=fi, acc_total=acc_total, **pc))\n",
    "                        conf_store.setdefault((\"XGBoost\", h, W), []).append(cm)\n",
    "                    except Exception as e:\n",
    "                        skipped.append(f\"XGB h={h}, W={W}, fold={fi} erro: {e}\")\n",
    "        else:\n",
    "            skipped.append(f\"XGB h={h}, W={W} sem amostras — skip\")\n",
    "\n",
    "        # LSTM\n",
    "        dfl = lstm_panels[W].dropna(subset=[\"ret1\", ycol]).copy()\n",
    "        if dfl.empty:\n",
    "            skipped.append(f\"LSTM h={h}, W={W} sem amostras — skip\")\n",
    "            continue\n",
    "        dfl[\"y_int\"] = pd.Categorical(dfl[ycol], categories=class_order).codes\n",
    "        if (dfl[\"y_int\"] < 0).any():\n",
    "            skipped.append(f\"LSTM h={h}, W={W} labels inválidos — skip\")\n",
    "            continue\n",
    "        feat_cols = [\"ret1\", f\"ret1_roll_mean_{W}\", f\"ret1_roll_std_{W}\"]\n",
    "        for fi, s in enumerate(splits, 1):\n",
    "            tr = subset(dfl, s[\"train_start\"], s[\"train_end\"])\n",
    "            va = subset(dfl, s[\"val_start\"], s[\"val_end\"])\n",
    "            te = subset(dfl, s[\"test_start\"], s[\"test_end\"])\n",
    "            if len(tr) < W+5 or len(va) < W+5 or len(te) < W+5:\n",
    "                skipped.append(f\"LSTM h={h}, W={W}, fold={fi} janelas insuficientes — skip\")\n",
    "                continue\n",
    "            # Escala sem vazamento\n",
    "            scaler = StandardScaler().fit(tr[feat_cols].values)\n",
    "            tr_s = tr.copy(); va_s = va.copy(); te_s = te.copy()\n",
    "            tr_s[feat_cols] = scaler.transform(tr[feat_cols].values)\n",
    "            va_s[feat_cols] = scaler.transform(va[feat_cols].values)\n",
    "            te_s[feat_cols] = scaler.transform(te[feat_cols].values)\n",
    "            # Sequências\n",
    "            def to_seq(dfz):\n",
    "                Xl, yl = [], []\n",
    "                V = dfz[feat_cols].values; yv = dfz[\"y_int\"].values\n",
    "                for i in range(W, len(dfz)):\n",
    "                    Xl.append(V[i-W:i, :]); yl.append(yv[i])\n",
    "                if not Xl: return np.empty((0,W,len(feat_cols))), np.empty((0,), dtype=int)\n",
    "                return np.stack(Xl, axis=0), np.array(yl, dtype=int)\n",
    "            Xtr, ytr = to_seq(tr_s); Xva, yva = to_seq(va_s); Xte, yte = to_seq(te_s)\n",
    "            if Xtr.shape[0]==0 or Xva.shape[0]==0 or Xte.shape[0]==0:\n",
    "                skipped.append(f\"LSTM h={h}, W={W}, fold={fi} sequências insuficientes — skip\")\n",
    "                continue\n",
    "            try:\n",
    "                tf.keras.backend.clear_session()\n",
    "                model = build_lstm_model(n_features=len(feat_cols), W=W)\n",
    "                es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=lstm_patience, restore_best_weights=True, verbose=0)\n",
    "                model.fit(Xtr, ytr, validation_data=(Xva, yva), epochs=lstm_epochs, batch_size=lstm_batch_size, callbacks=[es], verbose=0)\n",
    "                proba = model.predict(Xte, verbose=0)\n",
    "                y_pred = np.argmax(proba, axis=1); y_true = yte\n",
    "                acc_total = float(accuracy_score(y_true, y_pred))\n",
    "                cm = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
    "                def _pc(cm):\n",
    "                    res = {}\n",
    "                    for i, nm in enumerate([\"cai\",\"mantem\",\"sobe\"]):\n",
    "                        denom = cm[i,:].sum(); res[f\"acc_{nm}\"] = (cm[i,i]/denom) if denom>0 else np.nan\n",
    "                    return res\n",
    "                pc = _pc(cm)\n",
    "                rows.append(dict(model=\"LSTM\", horizon=h, window=W, fold=fi, acc_total=acc_total, **pc))\n",
    "                conf_store.setdefault((\"LSTM\", h, W), []).append(cm)\n",
    "            except Exception as e:\n",
    "                skipped.append(f\"LSTM h={h}, W={W}, fold={fi} erro: {e}\")\n",
    "\n",
    "# Consolidação\n",
    "if not rows:\n",
    "    raise RuntimeError(\"CHECKLIST_FAILURE: nenhuma combinação gerou resultados.\")\n",
    "res = pd.DataFrame(rows).sort_values([\"model\",\"horizon\",\"window\",\"fold\"]) \n",
    "agg = res.groupby([\"horizon\",\"model\",\"window\"], as_index=False).agg(\n",
    "    acc_total_mean=(\"acc_total\",\"mean\"), acc_total_std=(\"acc_total\",\"std\"),\n",
    "    acc_cai_mean=(\"acc_cai\",\"mean\"), acc_mantem_mean=(\"acc_mantem\",\"mean\"), acc_sobe_mean=(\"acc_sobe\",\"mean\"),\n",
    "    folds=(\"fold\",\"nunique\")\n",
    ")\n",
    "\n",
    "# Saída por horizonte\n",
    "for h in horizons:\n",
    "    sub = agg[agg[\"horizon\"]==h].copy().sort_values([\"model\",\"window\"]) \n",
    "    print(f\"\\nRESUMO — D+{h} (teste): modelo × janela\")\n",
    "    if sub.empty:\n",
    "        print(\"–\")\n",
    "    else:\n",
    "        for c in [\"acc_total_mean\",\"acc_total_std\",\"acc_cai_mean\",\"acc_mantem_mean\",\"acc_sobe_mean\"]:\n",
    "            if c in sub.columns: sub[c] = sub[c].astype(float)\n",
    "        cols = [\"model\",\"window\",\"acc_total_mean\",\"acc_total_std\",\"acc_cai_mean\",\"acc_mantem_mean\",\"acc_sobe_mean\",\"folds\"]\n",
    "        print(sub[cols].fillna(\"–\").to_string(index=False))\n",
    "    top = sub.sort_values(\"acc_total_mean\", ascending=False).head(3)\n",
    "    print(f\"\\nTOP-3 — D+{h} (teste)\")\n",
    "    print(\"–\" if top.empty else top[cols].fillna(\"–\").to_string(index=False))\n",
    "    if not top.empty:\n",
    "        br = top.iloc[0]\n",
    "        key = (br[\"model\"], int(h), int(br[\"window\"]))\n",
    "        cms = conf_store.get(key, [])\n",
    "        if cms:\n",
    "            cm_sum = np.sum(np.stack(cms, axis=0), axis=0)\n",
    "            print(f\"\\nMATRIZ DE CONFUSÃO — melhor combinação D+{h} (modelo={br['model']}, janela={int(br['window'])})\")\n",
    "            header = [\"\", \"pred_CAI\", \"pred_MANTEM\", \"pred_SOBE\"]\n",
    "            print(\"{:<12s}{:>10s}{:>12s}{:>10s}\".format(*header))\n",
    "            for i, cls in enumerate([\"true_CAI\",\"true_MANTEM\",\"true_SOBE\"]):\n",
    "                print(\"{:<12s}{:>10d}{:>12d}{:>10d}\".format(cls, int(cm_sum[i,0]), int(cm_sum[i,1]), int(cm_sum[i,2])))\n",
    "        else:\n",
    "            print(\"\\nMATRIZ DE CONFUSÃO — melhor combinação D+{h}: –\")\n",
    "\n",
    "# Checklist\n",
    "processed_h = sorted(set(int(h) for h in res[\"horizon\"].unique()))\n",
    "processed_w = sorted(set(int(w) for w in res[\"window\"].unique()))\n",
    "print(\"\\nCHECKLIST — Execução (dry_run)\")\n",
    "print(f\"- SSOT usado: {path} (tier={tier})\")\n",
    "print(f\"- Horizontes processados: {processed_h}\")\n",
    "print(f\"- Janelas processadas: {processed_w}\")\n",
    "print(f\"- Folds processados (máximo por combinação): {len(splits)}\")\n",
    "for h in horizons:\n",
    "    ok = (agg[\"horizon\"]==h).any(); print(f\"- Tabela resumo D+{h}: {'OK' if ok else '–'}\")\n",
    "print(f\"- dry_run: {dry_run} (nenhum arquivo salvo)\")\n",
    "\n",
    "print(f\"\\n[{now_ts()}] Fim — Classificação 3C (dry_run={dry_run})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42efa9c0",
   "metadata": {},
   "source": [
    "## CAI vs NÃO CAI com prioridade para CAI e pisos por horizonte — IBOV SSOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb76537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-19 18:14:53] Início — CAI vs NÃO CAI — V1.2/V1.2.1 | Seeds: numpy=2025, tf=42, py=7\n",
      "\n",
      "[PROVA SSOT]\n",
      "- Caminho: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Linhas: 3400 | date_min: 2012-01-03 | date_max: 2025-09-19\n",
      "- Colunas (amostra): date, open, high, low, close, volume, ticker, open_norm, high_norm, low_norm, close_norm, volume_norm, return_1d, volatility_5d, sma_5, sma_20, sma_ratio, y_h1, y_h3, y_h5, y_h1_cls, y_h3_cls, y_h5_cls, year, __date__ ...\n",
      "\n",
      "[WALK-FORWARD — Folds]\n",
      "Fold 01 | train[2012-01-03 → 2013-04-02] | val[2013-04-03 → 2013-07-02] | test[2013-07-03 → 2014-01-02]\n",
      "Fold 02 | train[2012-01-03 → 2013-10-02] | val[2013-10-03 → 2014-01-02] | test[2014-01-03 → 2014-07-02]\n",
      "Fold 03 | train[2012-01-03 → 2014-04-02] | val[2014-04-03 → 2014-07-02] | test[2014-07-03 → 2015-01-02]\n",
      "Fold 04 | train[2012-01-03 → 2014-10-02] | val[2014-10-03 → 2015-01-02] | test[2015-01-03 → 2015-07-02]\n",
      "Fold 05 | train[2012-01-03 → 2015-04-02] | val[2015-04-03 → 2015-07-02] | test[2015-07-03 → 2016-01-02]\n",
      "Fold 06 | train[2012-01-03 → 2015-10-02] | val[2015-10-03 → 2016-01-02] | test[2016-01-03 → 2016-07-02]\n",
      "Fold 07 | train[2012-01-03 → 2016-04-02] | val[2016-04-03 → 2016-07-02] | test[2016-07-03 → 2017-01-02]\n",
      "Fold 08 | train[2012-01-03 → 2016-10-02] | val[2016-10-03 → 2017-01-02] | test[2017-01-03 → 2017-07-02]\n",
      "Fold 09 | train[2012-01-03 → 2017-04-02] | val[2017-04-03 → 2017-07-02] | test[2017-07-03 → 2018-01-02]\n",
      "Fold 10 | train[2012-01-03 → 2017-10-02] | val[2017-10-03 → 2018-01-02] | test[2018-01-03 → 2018-07-02]\n",
      "\n",
      "[ERROS REPETIDOS — intervenção requerida]\n",
      "- XGB_ERR:TypeError:XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds' (x3)\n",
      "- xgb_early_stop_unsupported? Considerar atualizar xgboost para >=1.7.6 ou usar fallback.\n",
      "\n",
      "[ERROS REPETIDOS — intervenção requerida]\n",
      "- XGB_ERR:TypeError:XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds' (x3)\n",
      "- xgb_early_stop_unsupported? Considerar atualizar xgboost para >=1.7.6 ou usar fallback.\n",
      "\n",
      "[PISOS DE PRECISÃO — usados]\n",
      "- D+1: 0.82 (fixo)\n",
      "- D+3: 0.78\n",
      "- D+5: 0.85\n",
      "- N_min_preds_val: 10; flood_guard ≤ 50%\n",
      "\n",
      "[PISOS DE PRECISÃO — usados]\n",
      "- D+1: 0.82 (fixo)\n",
      "- D+3: 0.78\n",
      "- D+5: 0.85\n",
      "- N_min_preds_val: 10; flood_guard ≤ 50%\n",
      "\n",
      "RESUMO — D+1 (TESTE agregado) — modelo × janela\n",
      "model  window  precisao_CAI  recall_CAI   F1_CAI      acc  pred_CAI_rate  eligible                reason  folds  threshold_median\n",
      " LSTM       5      0.955095    0.489381 0.647162 0.492424       0.951178     False      inundacao,sanity     10               NaN\n",
      " LSTM      10      0.682143    0.482933 0.565507 0.484183       0.695079     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      15      0.998127    0.490340 0.657619 0.489890       0.999081     False      inundacao,sanity     10               NaN\n",
      "\n",
      "TOP-3 — D+1 (TESTE)\n",
      "–\n",
      "\n",
      "RESUMO — D+3 (TESTE agregado) — modelo × janela\n",
      "model  window  precisao_CAI  recall_CAI   F1_CAI      acc  pred_CAI_rate  eligible                reason  folds  threshold_median\n",
      " LSTM       5      0.778175    0.460805 0.578842 0.467172       0.794613     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      10      0.896679    0.469565 0.616360 0.468366       0.909490     False      inundacao,sanity     10               NaN\n",
      " LSTM      15      0.867562    0.470833 0.610398 0.469669       0.882353     False      inundacao,sanity     10               NaN\n",
      "\n",
      "TOP-3 — D+3 (TESTE)\n",
      "–\n",
      "\n",
      "RESUMO — D+5 (TESTE agregado) — modelo × janela\n",
      "model  window  precisao_CAI  recall_CAI   F1_CAI      acc  pred_CAI_rate  eligible                reason  folds  threshold_median\n",
      " LSTM       5      0.789753    0.464657 0.585079 0.466330       0.809764     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      10      0.612844    0.484058 0.540891 0.501757       0.606327     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      15      0.719466    0.516438 0.601276 0.540441       0.670956     False piso,inundacao,sanity     10               NaN\n",
      "\n",
      "TOP-3 — D+5 (TESTE)\n",
      "–\n",
      "\n",
      "THRESHOLD OPERACIONAL (mediana entre folds elegíveis)\n",
      "- D+1: –\n",
      "- D+3: –\n",
      "- D+5: –\n",
      "\n",
      "SEQUÊNCIA FINAL (último bloco de TESTE):\n",
      "- (D+1, D+3, D+5) = —\n",
      "\n",
      "[Painel — Checklist Operacional (V1.2.1)]\n",
      "\n",
      "## HORIZONTE D+1\n",
      "Piso de Precisão(CAI): 0.82\n",
      "Threshold Operacional (mediana): —\n",
      "\n",
      "Vencedor: —\n",
      "Precisão(CAI): —   Recall(CAI): —   F1(CAI): —   ACC: —\n",
      "Pred_CAI_rate (VAL/Teste): — / —\n",
      "Confusão (Teste): TP=—  FP=—  FN=—  TN=—\n",
      "\n",
      "Checks:\n",
      "✗ Piso de Precisão(CAI)\n",
      "✗ Cobertura mínima\n",
      "✗ Freio de inundação (VAL/Teste)\n",
      "✗ Sanity de métricas\n",
      "! Baselines (precisão)\n",
      "\n",
      "Baselines — Precisão(CAI) / F1:\n",
      "• –\n",
      "\n",
      "Apto a operar?  NÃO\n",
      "\n",
      "## HORIZONTE D+3\n",
      "Piso de Precisão(CAI): 0.78\n",
      "Threshold Operacional (mediana): —\n",
      "\n",
      "Vencedor: —\n",
      "Precisão(CAI): —   Recall(CAI): —   F1(CAI): —   ACC: —\n",
      "Pred_CAI_rate (VAL/Teste): — / —\n",
      "Confusão (Teste): TP=—  FP=—  FN=—  TN=—\n",
      "\n",
      "Checks:\n",
      "✗ Piso de Precisão(CAI)\n",
      "✗ Cobertura mínima\n",
      "✗ Freio de inundação (VAL/Teste)\n",
      "✗ Sanity de métricas\n",
      "! Baselines (precisão)\n",
      "\n",
      "Baselines — Precisão(CAI) / F1:\n",
      "• –\n",
      "\n",
      "Apto a operar?  NÃO\n",
      "\n",
      "## HORIZONTE D+5\n",
      "Piso de Precisão(CAI): 0.85\n",
      "Threshold Operacional (mediana): —\n",
      "\n",
      "Vencedor: —\n",
      "Precisão(CAI): —   Recall(CAI): —   F1(CAI): —   ACC: —\n",
      "Pred_CAI_rate (VAL/Teste): — / —\n",
      "Confusão (Teste): TP=—  FP=—  FN=—  TN=—\n",
      "\n",
      "Checks:\n",
      "✗ Piso de Precisão(CAI)\n",
      "✗ Cobertura mínima\n",
      "✗ Freio de inundação (VAL/Teste)\n",
      "✗ Sanity de métricas\n",
      "! Baselines (precisão)\n",
      "\n",
      "Baselines — Precisão(CAI) / F1:\n",
      "• –\n",
      "\n",
      "Apto a operar?  NÃO\n",
      "\n",
      "Resumo: D+1=NÃO | D+3=NÃO | D+5=NÃO   |  Sequência final (último TESTE): —\n",
      "Alertas:\n",
      "• XGB_ERR:TypeError:XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds' (x3)\n",
      "Perguntas: Atualizar xgboost para ≥1.7.6 e habilitar early stopping? [ ]Sim [ ]Não\n",
      "SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet | Folds: 10 | Seeds: 2025/42/7\n",
      "Precision floors: D+1=0.82; D+3=0.78; D+5=0.85 | N_min_preds_val=10\n",
      "\n",
      "CHECKLIST\n",
      "- XGBoost versão detectada: 3.0.5\n",
      "- Branch XGB: sklearn.fit(early_stopping_rounds) | abort_xgb=True\n",
      "- SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Horizontes: [1, 3, 5] | Janelas: [5, 10, 15] | Folds: 10\n",
      "- precision_floor: D+1=0.82, D+3=0.78, D+5=0.85\n",
      "- dry_run=True (nenhum arquivo salvo)\n",
      "\n",
      "[2025-09-19 18:21:21] Fim — CAI vs NÃO CAI (dry_run=True)\n",
      "\n",
      "RESUMO — D+1 (TESTE agregado) — modelo × janela\n",
      "model  window  precisao_CAI  recall_CAI   F1_CAI      acc  pred_CAI_rate  eligible                reason  folds  threshold_median\n",
      " LSTM       5      0.955095    0.489381 0.647162 0.492424       0.951178     False      inundacao,sanity     10               NaN\n",
      " LSTM      10      0.682143    0.482933 0.565507 0.484183       0.695079     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      15      0.998127    0.490340 0.657619 0.489890       0.999081     False      inundacao,sanity     10               NaN\n",
      "\n",
      "TOP-3 — D+1 (TESTE)\n",
      "–\n",
      "\n",
      "RESUMO — D+3 (TESTE agregado) — modelo × janela\n",
      "model  window  precisao_CAI  recall_CAI   F1_CAI      acc  pred_CAI_rate  eligible                reason  folds  threshold_median\n",
      " LSTM       5      0.778175    0.460805 0.578842 0.467172       0.794613     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      10      0.896679    0.469565 0.616360 0.468366       0.909490     False      inundacao,sanity     10               NaN\n",
      " LSTM      15      0.867562    0.470833 0.610398 0.469669       0.882353     False      inundacao,sanity     10               NaN\n",
      "\n",
      "TOP-3 — D+3 (TESTE)\n",
      "–\n",
      "\n",
      "RESUMO — D+5 (TESTE agregado) — modelo × janela\n",
      "model  window  precisao_CAI  recall_CAI   F1_CAI      acc  pred_CAI_rate  eligible                reason  folds  threshold_median\n",
      " LSTM       5      0.789753    0.464657 0.585079 0.466330       0.809764     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      10      0.612844    0.484058 0.540891 0.501757       0.606327     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      15      0.719466    0.516438 0.601276 0.540441       0.670956     False piso,inundacao,sanity     10               NaN\n",
      "\n",
      "TOP-3 — D+5 (TESTE)\n",
      "–\n",
      "\n",
      "THRESHOLD OPERACIONAL (mediana entre folds elegíveis)\n",
      "- D+1: –\n",
      "- D+3: –\n",
      "- D+5: –\n",
      "\n",
      "SEQUÊNCIA FINAL (último bloco de TESTE):\n",
      "- (D+1, D+3, D+5) = —\n",
      "\n",
      "[Painel — Checklist Operacional (V1.2.1)]\n",
      "\n",
      "## HORIZONTE D+1\n",
      "Piso de Precisão(CAI): 0.82\n",
      "Threshold Operacional (mediana): —\n",
      "\n",
      "Vencedor: —\n",
      "Precisão(CAI): —   Recall(CAI): —   F1(CAI): —   ACC: —\n",
      "Pred_CAI_rate (VAL/Teste): — / —\n",
      "Confusão (Teste): TP=—  FP=—  FN=—  TN=—\n",
      "\n",
      "Checks:\n",
      "✗ Piso de Precisão(CAI)\n",
      "✗ Cobertura mínima\n",
      "✗ Freio de inundação (VAL/Teste)\n",
      "✗ Sanity de métricas\n",
      "! Baselines (precisão)\n",
      "\n",
      "Baselines — Precisão(CAI) / F1:\n",
      "• –\n",
      "\n",
      "Apto a operar?  NÃO\n",
      "\n",
      "## HORIZONTE D+3\n",
      "Piso de Precisão(CAI): 0.78\n",
      "Threshold Operacional (mediana): —\n",
      "\n",
      "Vencedor: —\n",
      "Precisão(CAI): —   Recall(CAI): —   F1(CAI): —   ACC: —\n",
      "Pred_CAI_rate (VAL/Teste): — / —\n",
      "Confusão (Teste): TP=—  FP=—  FN=—  TN=—\n",
      "\n",
      "Checks:\n",
      "✗ Piso de Precisão(CAI)\n",
      "✗ Cobertura mínima\n",
      "✗ Freio de inundação (VAL/Teste)\n",
      "✗ Sanity de métricas\n",
      "! Baselines (precisão)\n",
      "\n",
      "Baselines — Precisão(CAI) / F1:\n",
      "• –\n",
      "\n",
      "Apto a operar?  NÃO\n",
      "\n",
      "## HORIZONTE D+5\n",
      "Piso de Precisão(CAI): 0.85\n",
      "Threshold Operacional (mediana): —\n",
      "\n",
      "Vencedor: —\n",
      "Precisão(CAI): —   Recall(CAI): —   F1(CAI): —   ACC: —\n",
      "Pred_CAI_rate (VAL/Teste): — / —\n",
      "Confusão (Teste): TP=—  FP=—  FN=—  TN=—\n",
      "\n",
      "Checks:\n",
      "✗ Piso de Precisão(CAI)\n",
      "✗ Cobertura mínima\n",
      "✗ Freio de inundação (VAL/Teste)\n",
      "✗ Sanity de métricas\n",
      "! Baselines (precisão)\n",
      "\n",
      "Baselines — Precisão(CAI) / F1:\n",
      "• –\n",
      "\n",
      "Apto a operar?  NÃO\n",
      "\n",
      "Resumo: D+1=NÃO | D+3=NÃO | D+5=NÃO   |  Sequência final (último TESTE): —\n",
      "Alertas:\n",
      "• XGB_ERR:TypeError:XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds' (x3)\n",
      "Perguntas: Atualizar xgboost para ≥1.7.6 e habilitar early stopping? [ ]Sim [ ]Não\n",
      "SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet | Folds: 10 | Seeds: 2025/42/7\n",
      "Precision floors: D+1=0.82; D+3=0.78; D+5=0.85 | N_min_preds_val=10\n",
      "\n",
      "CHECKLIST\n",
      "- XGBoost versão detectada: 3.0.5\n",
      "- Branch XGB: sklearn.fit(early_stopping_rounds) | abort_xgb=True\n",
      "- SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Horizontes: [1, 3, 5] | Janelas: [5, 10, 15] | Folds: 10\n",
      "- precision_floor: D+1=0.82, D+3=0.78, D+5=0.85\n",
      "- dry_run=True (nenhum arquivo salvo)\n",
      "\n",
      "[2025-09-19 18:21:21] Fim — CAI vs NÃO CAI (dry_run=True)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "INSTRUÇÃO V1.2/V1.2.1 — CAI vs NÃO CAI (prioridade: PRECISÃO de CAI)\n",
    "- SSOT: GOLD apenas.\n",
    "- Walk-forward: 10 folds (treino ≥18m, val 3m, teste 6m) sem vazamento.\n",
    "- Modelos: LSTM compacto; XGBoost com branch por versão (>=1.7.6: sklearn+early_stopping_rounds no fit; <1.7.6: xgb.train). scale_pos_weight no treino do fold.\n",
    "- Features enxutas e estáveis; padronização fit no treino por fold.\n",
    "- Probabilidades calibradas (Platt; fallback Isotonic) com fit em VAL e aplicadas em VAL/TESTE.\n",
    "- Pisos: D+1 fixo=0.82; D+3/D+5 por VAL com N_min=10 e clip [0.70,0.85].\n",
    "- Busca de limiar em VAL maximizando recall sob: piso, N_min, flood_guard (pred_CAI_rate ≤ 0.50). Sem relaxes.\n",
    "- Elegibilidade no TESTE agregado: piso, cobertura mínima, flood_guard, sanity de métricas.\n",
    "- Threshold operacional = mediana dos thresholds dos folds elegíveis (por horizonte). Sequência final só se houver elegíveis.\n",
    "- Baselines: Sempre_NAO_CAI, ProporcaoTreino>0.5, SinalOntem, Momentum_3d.\n",
    "- Repetição de erros: abortar após ≥3 mensagens idênticas por etapa e perguntar ação.\n",
    "- Painel final (markdown) 1 página com status por horizonte + resumo executivo.\n",
    "- dry_run=True: nada salvo.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "\n",
    "# Seeds e ambiente (antes dos imports de TF)\n",
    "SEED_NUMPY = 2025\n",
    "SEED_TF = 42\n",
    "SEED_PY = 7\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED_PY)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # força CPU e silencia erros CUDA\n",
    "\n",
    "import random\n",
    "random.seed(SEED_PY)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED_NUMPY)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "pd.set_option(\"display.max_columns\", 160)\n",
    "\n",
    "_missing = []\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_VERSION = getattr(xgb, \"__version__\", \"0\")\n",
    "except Exception as e:\n",
    "    _missing.append(f\"xgboost ({e})\")\n",
    "    XGB_VERSION = \"0\"\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential  # pyright: ignore\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, Input  # pyright: ignore\n",
    "    from tensorflow.keras.callbacks import EarlyStopping  # pyright: ignore\n",
    "    tf.random.set_seed(SEED_TF)\n",
    "except Exception as e:\n",
    "    _missing.append(f\"tensorflow/keras ({e})\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# =========================\n",
    "# Parâmetros\n",
    "# =========================\n",
    "\n",
    "dry_run: bool = True\n",
    "\n",
    "# GOLD apenas\n",
    "GOLD_PATH = \"/home/wrm/BOLSA_2026/gold/IBOV_gold.parquet\"\n",
    "\n",
    "windows: List[int] = [5, 10, 15]\n",
    "horizons: List[int] = [1, 3, 5]\n",
    "\n",
    "# Validação temporal\n",
    "train_min_months: int = 18\n",
    "val_months: int = 3\n",
    "test_months: int = 6\n",
    "max_folds: int = 10\n",
    "\n",
    "# Priorização de CAI\n",
    "precision_floor: Dict[str, Optional[float]] = {\"D+1\": 0.82, \"D+3\": None, \"D+5\": None}\n",
    "coverage_min_rate: float = 0.10\n",
    "coverage_min_count: int = 8\n",
    "threshold_grid: List[float] = [i/100.0 for i in range(10, 91)]  # 0.10 → 0.90\n",
    "N_min_preds_val: int = 10\n",
    "FLOOD_GUARD_MAX: float = 0.50  # pred_CAI_rate máximo em VAL/TESTE\n",
    "\n",
    "# Modelos\n",
    "xgb_params = dict(\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=2000,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=SEED_PY,\n",
    "    n_jobs=max(1, (os.cpu_count() or 2) - 1),\n",
    ")\n",
    "xgb_early_stopping_rounds: int = 50\n",
    "\n",
    "lstm_units: int = 48\n",
    "lstm_dropout: float = 0.2\n",
    "lstm_epochs: int = 50\n",
    "lstm_batch_size: int = 32\n",
    "lstm_patience: int = 5\n",
    "\n",
    "# =========================\n",
    "# Utilidades\n",
    "# =========================\n",
    "\n",
    "def now_ts() -> str:\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def summarize_df(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    rows, cols = df.shape\n",
    "    dmin = pd.to_datetime(df[\"__date__\"]).min(); dmax = pd.to_datetime(df[\"__date__\"]).max()\n",
    "    return dict(\n",
    "        row_count=str(rows),\n",
    "        date_min=str(getattr(dmin, 'date', lambda: '-')()) if pd.notnull(dmin) else \"-\",\n",
    "        date_max=str(getattr(dmax, 'date', lambda: '-')()) if pd.notnull(dmax) else \"-\",\n",
    "        columns=\", \".join(list(df.columns)[:25]) + (\" ...\" if df.shape[1] > 25 else \"\")\n",
    "    )\n",
    "\n",
    "def ensure_datetime(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # tenta usar coluna 'date' ou índice datetime\n",
    "    if \"date\" in df.columns:\n",
    "        out = df.copy()\n",
    "        out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n",
    "        out = out.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "        out[\"__date__\"] = out[\"date\"].values\n",
    "        return out\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        out = df.sort_index().copy(); out[\"__date__\"] = out.index\n",
    "        return out\n",
    "    raise ValueError(\"VALIDATION_ERROR: coluna 'date' ausente e índice não é DatetimeIndex.\")\n",
    "\n",
    "def month_add(d: pd.Timestamp, months: int) -> pd.Timestamp:\n",
    "    return d + pd.DateOffset(months=months)\n",
    "\n",
    "def build_walk_forward_splits(df: pd.DataFrame) -> List[Dict[str, pd.Timestamp]]:\n",
    "    dates = pd.to_datetime(df[\"__date__\"])  # type: ignore\n",
    "    start = dates.min().normalize(); end = dates.max().normalize()\n",
    "    train_end = month_add(start, train_min_months) - pd.DateOffset(days=1)\n",
    "    folds = []\n",
    "    test_start = train_end + pd.DateOffset(days=1)\n",
    "    test_end = month_add(test_start, test_months) - pd.DateOffset(days=1)\n",
    "    while test_start <= end and len(folds) < max_folds:\n",
    "        if test_end > end: test_end = end\n",
    "        val_end = test_start - pd.DateOffset(days=1)\n",
    "        val_start = month_add(val_end, -val_months) + pd.DateOffset(days=1)\n",
    "        tr_start = start; tr_end = val_start - pd.DateOffset(days=1)\n",
    "        if tr_start >= tr_end or val_start > val_end or test_start > test_end:\n",
    "            break\n",
    "        folds.append(dict(\n",
    "            train_start=tr_start, train_end=tr_end,\n",
    "            val_start=val_start, val_end=val_end,\n",
    "            test_start=test_start, test_end=test_end,\n",
    "        ))\n",
    "        test_start = test_end + pd.DateOffset(days=1)\n",
    "        test_end = month_add(test_start, test_months) - pd.DateOffset(days=1)\n",
    "    if not folds:\n",
    "        raise ValueError(\"VALIDATION_ERROR: não foi possível construir folds walk-forward.\")\n",
    "    return folds\n",
    "\n",
    "def subset(df: pd.DataFrame, a: pd.Timestamp, b: pd.Timestamp) -> pd.DataFrame:\n",
    "    return df.loc[(df[\"__date__\"] >= a) & (df[\"__date__\"] <= b)].copy()\n",
    "\n",
    "# =========================\n",
    "# Features\n",
    "# =========================\n",
    "\n",
    "def compute_log_ret(close: pd.Series) -> pd.Series:\n",
    "    return np.log(close / close.shift(1))\n",
    "\n",
    "def prepare_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Reqs: close, high, low\n",
    "    for c in (\"close\", \"high\", \"low\"):\n",
    "        if c not in out.columns:\n",
    "            raise ValueError(f\"VALIDATION_ERROR: coluna obrigatória ausente: {c}\")\n",
    "    out[\"ret1\"] = compute_log_ret(out[\"close\"])\n",
    "    # lags até 10\n",
    "    for lag in range(1, 11):\n",
    "        out[f\"ret1_lag_{lag}\"] = out[\"ret1\"].shift(lag)\n",
    "    # rolling mean/std para 5/10/15 e z-scores de ret1\n",
    "    for W in (5, 10, 15):\n",
    "        out[f\"ret_roll_mean_{W}\"] = out[\"ret1\"].rolling(W).mean()\n",
    "        out[f\"ret_roll_std_{W}\"] = out[\"ret1\"].rolling(W).std()\n",
    "        out[f\"zscore_ret_{W}\"] = (out[\"ret1\"] - out[f\"ret_roll_mean_{W}\"]) / out[f\"ret_roll_std_{W}\"]\n",
    "    # vol20d para farol\n",
    "    out[\"vol20d\"] = out[\"ret1\"].rolling(20).std()\n",
    "    # MA50 e pos_ma50\n",
    "    out[\"ma50\"] = out[\"close\"].rolling(50).mean()\n",
    "    out[\"pos_ma50\"] = ((out[\"close\"] > out[\"ma50\"]).astype(float)).where(out[\"ma50\"].notna(), np.nan)\n",
    "    # range compression (true range normalizado) 5/10\n",
    "    for W in (5, 10):\n",
    "        hi = out[\"high\"].rolling(W).max()\n",
    "        lo = out[\"low\"].rolling(W).min()\n",
    "        mid = out[\"close\"].rolling(W).mean()\n",
    "        out[f\"tr_norm_{W}\"] = (hi - lo) / (mid.replace(0, np.nan))\n",
    "    # forward returns\n",
    "    for h in horizons:\n",
    "        out[f\"ret_fwd_{h}\"] = (out[\"close\"].shift(-h) / out[\"close\"]) - 1.0\n",
    "        out[f\"y_h{h}_bin\"] = (pd.to_numeric(out[f\"ret_fwd_{h}\"], errors=\"coerce\") < 0).astype(\"Int8\")\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Modelos e Calibração\n",
    "# =========================\n",
    "\n",
    "def build_lstm_model(n_features: int, W: int) -> Sequential:\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(W, n_features)))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(lstm_dropout))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def make_calibrator(y_val: np.ndarray, p_val: np.ndarray):\n",
    "    yv = np.asarray(y_val).astype(int)\n",
    "    pv = np.asarray(p_val).astype(float).reshape(-1, 1)\n",
    "    if len(np.unique(yv)) < 2 or len(yv) < 5:\n",
    "        return (lambda x: np.asarray(x, dtype=float)), \"none\"\n",
    "    # Platt\n",
    "    try:\n",
    "        lr = LogisticRegression(max_iter=1000, solver=\"lbfgs\")\n",
    "        lr.fit(pv, yv)\n",
    "        def f(x):\n",
    "            xv = np.asarray(x).astype(float).reshape(-1, 1)\n",
    "            return lr.predict_proba(xv)[:, 1]\n",
    "        return f, \"platt\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Isotonic fallback\n",
    "    try:\n",
    "        iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "        iso.fit(np.asarray(p_val).astype(float), yv)\n",
    "        def g(x):\n",
    "            return iso.predict(np.asarray(x).astype(float))\n",
    "        return g, \"isotonic\"\n",
    "    except Exception:\n",
    "        return (lambda x: np.asarray(x, dtype=float)), \"none\"\n",
    "\n",
    "# =========================\n",
    "# Métricas, sanity e seleção de threshold\n",
    "# =========================\n",
    "\n",
    "def cm_metrics(cm: np.ndarray) -> Tuple[float, float, float, float]:\n",
    "    TP = float(cm[0,0]); FP = float(cm[0,1]); FN = float(cm[1,0]); TN = float(cm[1,1])\n",
    "    prec = TP / max(1.0, (TP + FP))\n",
    "    rec = TP / max(1.0, (TP + FN))\n",
    "    acc = (TP + TN) / max(1.0, (TP + FP + FN + TN))\n",
    "    f1 = (2*prec*rec) / max(1e-12, (prec + rec)) if (prec + rec) > 0 else 0.0\n",
    "    return prec, rec, f1, acc\n",
    "\n",
    "def binary_eval(y_true: np.ndarray, y_score: np.ndarray, thr: float) -> Dict[str, Any]:\n",
    "    y_pred = (y_score >= thr).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "    prec_lib = float(precision_score(y_true, y_pred, zero_division=0))\n",
    "    rec_lib = float(recall_score(y_true, y_pred, zero_division=0))\n",
    "    f1_lib = float(f1_score(y_true, y_pred, zero_division=0))\n",
    "    acc_lib = float(accuracy_score(y_true, y_pred))\n",
    "    prec_cm, rec_cm, f1_cm, acc_cm = cm_metrics(cm)\n",
    "    sanity_ok = (abs(prec_lib - prec_cm) < 1e-6) and (abs(rec_lib - rec_cm) < 1e-6)\n",
    "    cover_rate = float((y_pred == 1).mean()) if len(y_pred) else 0.0\n",
    "    cover_count = int((y_pred == 1).sum()) if len(y_pred) else 0\n",
    "    return dict(\n",
    "        cm=cm, precision=prec_lib, recall=rec_lib, f1=f1_lib, acc=acc_lib,\n",
    "        precision_cm=prec_cm, recall_cm=rec_cm, f1_cm=f1_cm, acc_cm=acc_cm,\n",
    "        sanity_ok=sanity_ok, coverage_rate=cover_rate, coverage_count=cover_count\n",
    "    )\n",
    "\n",
    "def select_threshold_val(yv: np.ndarray, pv: np.ndarray, floor: float) -> Tuple[Optional[float], Dict[str, Any]]:\n",
    "    best = None\n",
    "    best_metrics = None\n",
    "    for thr in threshold_grid:\n",
    "        m = binary_eval(yv, pv, thr)\n",
    "        if (m[\"precision\"] >= floor and m[\"coverage_count\"] >= N_min_preds_val and m[\"coverage_rate\"] <= FLOOD_GUARD_MAX):\n",
    "            if best is None:\n",
    "                best, best_metrics = thr, m\n",
    "            else:\n",
    "                # Priorizar recall, desempate F1, depois ACC\n",
    "                cur = best_metrics\n",
    "                if (m[\"recall\"] > cur[\"recall\"]) or \\\n",
    "                   (m[\"recall\"] == cur[\"recall\"] and m[\"f1\"] > cur[\"f1\"]) or \\\n",
    "                   (m[\"recall\"] == cur[\"recall\"] and m[\"f1\"] == cur[\"f1\"] and m[\"acc\"] > cur[\"acc\"]):\n",
    "                    best, best_metrics = thr, m\n",
    "    return best, (best_metrics or {})\n",
    "\n",
    "# =========================\n",
    "# XGBoost — branch por versão\n",
    "# =========================\n",
    "\n",
    "def parse_version_tuple(v: str) -> Tuple[int,int,int]:\n",
    "    parts = (v or \"0\").split(\".\")\n",
    "    nums = []\n",
    "    for p in parts[:3]:\n",
    "        n = ''.join(ch for ch in p if ch.isdigit())\n",
    "        try:\n",
    "            nums.append(int(n) if n != '' else 0)\n",
    "        except Exception:\n",
    "            nums.append(0)\n",
    "    while len(nums) < 3:\n",
    "        nums.append(0)\n",
    "    return tuple(nums[:3])  # type: ignore\n",
    "\n",
    "XGB_TUP = parse_version_tuple(XGB_VERSION)\n",
    "XGB_GE_1_7_6 = (XGB_TUP[0] > 1) or (XGB_TUP[0] == 1 and (XGB_TUP[1] > 7 or (XGB_TUP[1] == 7 and XGB_TUP[2] >= 6)))\n",
    "\n",
    "# =========================\n",
    "# Execução principal\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    print(f\"[{now_ts()}] Início — CAI vs NÃO CAI — V1.2/V1.2.1 | Seeds: numpy={SEED_NUMPY}, tf={SEED_TF}, py={SEED_PY}\")\n",
    "    if _missing:\n",
    "        print(f\"CHECKLIST_FAILURE: dependências ausentes -> {', '.join(_missing)}\")\n",
    "        return\n",
    "    # SSOT GOLD\n",
    "    if not os.path.exists(GOLD_PATH):\n",
    "        print(\"CHECKLIST_FAILURE: GOLD ausente no SSOT.\")\n",
    "        return\n",
    "    try:\n",
    "        df_raw = pd.read_parquet(GOLD_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"VALIDATION_ERROR: falha ao ler GOLD: {e}\")\n",
    "        return\n",
    "    df = ensure_datetime(df_raw)\n",
    "    # Features\n",
    "    df = prepare_features(df)\n",
    "\n",
    "    # PROVA SSOT\n",
    "    proof = summarize_df(df)\n",
    "    print(\"\\n[PROVA SSOT]\")\n",
    "    print(f\"- Caminho: {GOLD_PATH} (tier=GOLD)\")\n",
    "    print(f\"- Linhas: {proof['row_count']} | date_min: {proof['date_min']} | date_max: {proof['date_max']}\")\n",
    "    print(f\"- Colunas (amostra): {proof['columns']}\")\n",
    "\n",
    "    # Splits\n",
    "    splits = build_walk_forward_splits(df)\n",
    "    print(\"\\n[WALK-FORWARD — Folds]\")\n",
    "    for i, s in enumerate(splits, 1):\n",
    "        print(f\"Fold {i:02d} | train[{str(s['train_start'].date())} → {str(s['train_end'].date())}] | val[{str(s['val_start'].date())} → {str(s['val_end'].date())}] | test[{str(s['test_start'].date())} → {str(s['test_end'].date())}]\")\n",
    "\n",
    "    # Estruturas\n",
    "    preds_val: Dict[Tuple[str,int,int,int], Tuple[np.ndarray, np.ndarray, Dict[str, Any]]] = {}\n",
    "    preds_tst: Dict[Tuple[str,int,int,int], Tuple[np.ndarray, np.ndarray, np.ndarray]] = {}\n",
    "    per_fold_info: Dict[Tuple[str,int,int,int], Dict[str, Any]] = {}\n",
    "    error_counts: Dict[str, int] = {}\n",
    "    abort_xgb: bool = False\n",
    "\n",
    "    # Loop h, W, fold\n",
    "    for h in horizons:\n",
    "        ycol = f\"y_h{h}_bin\"\n",
    "        for W in windows:\n",
    "            # lista de features base para XGB\n",
    "            feat_cols = [\n",
    "                # lags ret1\n",
    "                *[f\"ret1_lag_{lag}\" for lag in range(1, 11)],\n",
    "                # rolling mean/std/zscore\n",
    "                *[f\"ret_roll_mean_{k}\" for k in (5,10,15)],\n",
    "                *[f\"ret_roll_std_{k}\" for k in (5,10,15)],\n",
    "                *[f\"zscore_ret_{k}\" for k in (5,10,15)],\n",
    "            ]\n",
    "            # faróis binários dependem do treino: vol20d_high e range_compression_{5,10}\n",
    "            for fi, s in enumerate(splits, 1):\n",
    "                tr = subset(df, s[\"train_start\"], s[\"train_end\"])  # inclui __date__\n",
    "                va = subset(df, s[\"val_start\"], s[\"val_end\"])\n",
    "                te = subset(df, s[\"test_start\"], s[\"test_end\"]) \n",
    "                # construir faróis a partir do treino\n",
    "                vol_med = float(tr[\"vol20d\"].median()) if len(tr) else np.nan\n",
    "                tr[\"vol20d_high\"] = (tr[\"vol20d\"] >= vol_med).astype(int)\n",
    "                va[\"vol20d_high\"] = (va[\"vol20d\"] >= vol_med).astype(int)\n",
    "                te[\"vol20d_high\"] = (te[\"vol20d\"] >= vol_med).astype(int)\n",
    "                # range compression: abaixo ou igual à mediana do treino => 1\n",
    "                for k in (5, 10):\n",
    "                    med = float(tr[f\"tr_norm_{k}\"].median()) if len(tr) else np.nan\n",
    "                    tr[f\"range_compression_{k}\"] = (tr[f\"tr_norm_{k}\"] <= med).astype(int)\n",
    "                    va[f\"range_compression_{k}\"] = (va[f\"tr_norm_{k}\"] <= med).astype(int)\n",
    "                    te[f\"range_compression_{k}\"] = (te[f\"tr_norm_{k}\"] <= med).astype(int)\n",
    "                # pos_ma50 já existe (0/1)\n",
    "                feat_all = feat_cols + [\"vol20d_high\",\"pos_ma50\",\"range_compression_5\",\"range_compression_10\"]\n",
    "                # drop NaNs (janelas)\n",
    "                trc = tr[[\"__date__\", ycol] + feat_all].dropna().copy()\n",
    "                vac = va[[\"__date__\", ycol] + feat_all].dropna().copy()\n",
    "                tec = te[[\"__date__\", ycol] + feat_all].dropna().copy()\n",
    "                if trc.empty or trc[ycol].isna().all():\n",
    "                    # registrar e continuar\n",
    "                    msg = f\"train_empty:h={h},W={W},fold={fi}\"\n",
    "                    error_counts[msg] = error_counts.get(msg, 0) + 1\n",
    "                    continue\n",
    "                # padronização fit no treino\n",
    "                scaler = StandardScaler().fit(trc[feat_all].values)\n",
    "                Xtr = scaler.transform(trc[feat_all].values); ytr = trc[ycol].astype(int).values\n",
    "                Xva = scaler.transform(vac[feat_all].values) if len(vac) else np.empty((0,len(feat_all)))\n",
    "                yva = vac[ycol].astype(int).values if len(vac) else np.empty((0,), dtype=int)\n",
    "                Xte = scaler.transform(tec[feat_all].values) if len(tec) else np.empty((0,len(feat_all)))\n",
    "                yte = tec[ycol].astype(int).values if len(tec) else np.empty((0,), dtype=int)\n",
    "                dates_te = tec[\"__date__\"].values.astype(\"datetime64[ns]\") if len(tec) else np.array([], dtype=\"datetime64[ns]\")\n",
    "\n",
    "                # scale_pos_weight\n",
    "                pos = max(1, int((ytr == 1).sum()))\n",
    "                neg = int((ytr == 0).sum())\n",
    "                spw = float(neg / pos) if (pos + neg) > 0 else 1.0\n",
    "\n",
    "                # ===== XGBoost (versão-branch) =====\n",
    "                if not abort_xgb:\n",
    "                    try:\n",
    "                        if XGB_GE_1_7_6:\n",
    "                            clf = XGBClassifier(**xgb_params, scale_pos_weight=spw)\n",
    "                            # early_stopping_rounds no fit (SEM callbacks)\n",
    "                            clf.fit(\n",
    "                                Xtr, ytr,\n",
    "                                eval_set=[(Xtr, ytr), (Xva, yva)],\n",
    "                                early_stopping_rounds=xgb_early_stopping_rounds,\n",
    "                                verbose=False,\n",
    "                            )\n",
    "                            best_iter = getattr(clf, \"best_iteration\", None)\n",
    "                            if best_iter is not None:\n",
    "                                p_val_raw = clf.predict_proba(Xva, iteration_range=(0, int(best_iter)+1))[:,1] if len(Xva) else np.array([])\n",
    "                                p_tst_raw = clf.predict_proba(Xte, iteration_range=(0, int(best_iter)+1))[:,1] if len(Xte) else np.array([])\n",
    "                            else:\n",
    "                                p_val_raw = clf.predict_proba(Xva)[:,1] if len(Xva) else np.array([])\n",
    "                                p_tst_raw = clf.predict_proba(Xte)[:,1] if len(Xte) else np.array([])\n",
    "                        else:\n",
    "                            # Fallback: xgb.train\n",
    "                            dtr = xgb.DMatrix(Xtr, label=ytr)\n",
    "                            dva = xgb.DMatrix(Xva, label=yva)\n",
    "                            dte = xgb.DMatrix(Xte, label=yte)\n",
    "                            params = {\n",
    "                                'max_depth': xgb_params['max_depth'],\n",
    "                                'eta': xgb_params['learning_rate'],\n",
    "                                'subsample': xgb_params['subsample'],\n",
    "                                'colsample_bytree': xgb_params['colsample_bytree'],\n",
    "                                'objective': 'binary:logistic',\n",
    "                                'eval_metric': 'logloss',\n",
    "                                'tree_method': xgb_params['tree_method'],\n",
    "                                'seed': SEED_PY,\n",
    "                                'scale_pos_weight': spw,\n",
    "                            }\n",
    "                            booster = xgb.train(\n",
    "                                params,\n",
    "                                dtr,\n",
    "                                num_boost_round=int(xgb_params['n_estimators']),\n",
    "                                evals=[(dtr, 'train'), (dva, 'val')],\n",
    "                                early_stopping_rounds=xgb_early_stopping_rounds,\n",
    "                                verbose_eval=False,\n",
    "                            )\n",
    "                            best_iter = getattr(booster, 'best_iteration', None)\n",
    "                            if best_iter is not None:\n",
    "                                p_val_raw = booster.predict(dva, iteration_range=(0, int(best_iter)+1)) if len(yva) else np.array([])\n",
    "                                p_tst_raw = booster.predict(dte, iteration_range=(0, int(best_iter)+1)) if len(yte) else np.array([])\n",
    "                            else:\n",
    "                                p_val_raw = booster.predict(dva) if len(yva) else np.array([])\n",
    "                                p_tst_raw = booster.predict(dte) if len(yte) else np.array([])\n",
    "                        # calibração\n",
    "                        cal_fn, cal_m = make_calibrator(yva, p_val_raw)\n",
    "                        p_val = cal_fn(p_val_raw)\n",
    "                        p_tst = cal_fn(p_tst_raw)\n",
    "                        preds_val[(\"XGB\", W, h, fi)] = (yva, np.asarray(p_val, dtype=float), {\"calibration\": cal_m, \"spw\": spw})\n",
    "                        preds_tst[(\"XGB\", W, h, fi)] = (yte, np.asarray(p_tst, dtype=float), dates_te)\n",
    "                        per_fold_info[(\"XGB\", W, h, fi)] = {\"scaler\": \"standard\", \"best_iteration\": (int(best_iter) if best_iter is not None else None)}\n",
    "                    except Exception as e:\n",
    "                        msg = f\"XGB_ERR:{type(e).__name__}:{str(e).strip()}\"\n",
    "                        error_counts[msg] = error_counts.get(msg, 0) + 1\n",
    "                        if error_counts[msg] >= 3:\n",
    "                            abort_xgb = True\n",
    "                        # segue sem XGB neste fold\n",
    "                # ===== LSTM =====\n",
    "                try:\n",
    "                    # montar painel para LSTM (ret1 + roll mean/std W)\n",
    "                    lstm_df = df[[\"__date__\",\"ret1\", f\"ret_roll_mean_{W}\", f\"ret_roll_std_{W}\", ycol]].dropna().copy()\n",
    "                    trl = subset(lstm_df, s[\"train_start\"], s[\"train_end\"])\n",
    "                    val = subset(lstm_df, s[\"val_start\"], s[\"val_end\"])\n",
    "                    tes = subset(lstm_df, s[\"test_start\"], s[\"test_end\"])\n",
    "                    if len(trl) >= (W + 5) and len(val) >= (W + 5) and len(tes) >= (W + 5):\n",
    "                        feat_l = [\"ret1\", f\"ret_roll_mean_{W}\", f\"ret_roll_std_{W}\"]\n",
    "                        sc = StandardScaler().fit(trl[feat_l].values)\n",
    "                        def to_seq(b: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "                            b2 = b.copy(); b2[feat_l] = sc.transform(b2[feat_l].values)\n",
    "                            X, y = [], []\n",
    "                            V = b2[feat_l].values; yv2 = b2[ycol].astype(int).values\n",
    "                            for i in range(W, len(b2)):\n",
    "                                X.append(V[i-W:i,:]); y.append(yv2[i])\n",
    "                            dates = b2[\"__date__\"].values[W:].astype(\"datetime64[ns]\")\n",
    "                            return (np.stack(X,0) if X else np.empty((0,W,len(feat_l)))), (np.array(y, int) if y else np.empty((0,), int)), dates\n",
    "                        Xtr, ytr_l, _ = to_seq(trl)\n",
    "                        Xva, yva_l, _ = to_seq(val)\n",
    "                        Xte, yte_l, dte_l = to_seq(tes)\n",
    "                        if Xtr.shape[0] and Xva.shape[0] and Xte.shape[0]:\n",
    "                            tf.keras.backend.clear_session()\n",
    "                            model = build_lstm_model(n_features=len(feat_l), W=W)\n",
    "                            es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=lstm_patience, restore_best_weights=True, verbose=0)\n",
    "                            model.fit(Xtr, ytr_l, validation_data=(Xva, yva_l), epochs=lstm_epochs, batch_size=lstm_batch_size, callbacks=[es], verbose=0)\n",
    "                            p_val_raw = model.predict(Xva, verbose=0, batch_size=lstm_batch_size).reshape(-1)\n",
    "                            p_tst_raw = model.predict(Xte, verbose=0, batch_size=lstm_batch_size).reshape(-1)\n",
    "                            cal_fn, cal_m = make_calibrator(yva_l, p_val_raw)\n",
    "                            preds_val[(\"LSTM\", W, h, fi)] = (yva_l, np.asarray(cal_fn(p_val_raw), dtype=float), {\"calibration\": cal_m})\n",
    "                            preds_tst[(\"LSTM\", W, h, fi)] = (yte_l, np.asarray(cal_fn(p_tst_raw), dtype=float), dte_l)\n",
    "                            per_fold_info[(\"LSTM\", W, h, fi)] = {\"scaler\": \"standard\"}\n",
    "                except Exception as e:\n",
    "                    msg = f\"LSTM_ERR:{type(e).__name__}:{str(e).strip()}\"\n",
    "                    error_counts[msg] = error_counts.get(msg, 0) + 1\n",
    "                    # continua\n",
    "\n",
    "    # Relatar avisos de repetição e perguntas\n",
    "    repeat_msgs = [f\"{k} (x{v})\" for k,v in error_counts.items() if v >= 3]\n",
    "    if repeat_msgs:\n",
    "        print(\"\\n[ERROS REPETIDOS — intervenção requerida]\")\n",
    "        for m in repeat_msgs:\n",
    "            print(f\"- {m}\")\n",
    "        if abort_xgb:\n",
    "            print(\"- xgb_early_stop_unsupported? Considerar atualizar xgboost para >=1.7.6 ou usar fallback.\")\n",
    "\n",
    "    # Pisos D+3/D+5 via VAL com N_min e clip\n",
    "    for h in [3, 5]:\n",
    "        best_prec = 0.0\n",
    "        for key, (yv, pv, meta) in preds_val.items():\n",
    "            _, _, hh, _ = key\n",
    "            if hh != h or len(yv) == 0:\n",
    "                continue\n",
    "            # varrer grade e pegar melhor precisão respeitando N_min\n",
    "            for thr in threshold_grid:\n",
    "                m = binary_eval(yv, pv, thr)\n",
    "                if m[\"coverage_count\"] >= N_min_preds_val:\n",
    "                    if m[\"precision\"] > best_prec:\n",
    "                        best_prec = m[\"precision\"]\n",
    "        tag = f\"D+{h}\"\n",
    "        if precision_floor.get(tag) is None:\n",
    "            pf = round(best_prec, 2)\n",
    "            precision_floor[tag] = float(np.clip(pf, 0.70, 0.85))\n",
    "    print(\"\\n[PISOS DE PRECISÃO — usados]\")\n",
    "    print(f\"- D+1: {precision_floor['D+1']:.2f} (fixo)\")\n",
    "    print(f\"- D+3: {precision_floor['D+3']:.2f}\")\n",
    "    print(f\"- D+5: {precision_floor['D+5']:.2f}\")\n",
    "    print(f\"- N_min_preds_val: {N_min_preds_val}; flood_guard ≤ {int(FLOOD_GUARD_MAX*100)}%\")\n",
    "\n",
    "    # Seleção de thresholds por fold (VAL) e avaliação no TESTE\n",
    "    fold_rows: List[Dict[str, Any]] = []\n",
    "    for key in sorted(preds_val.keys()):\n",
    "        model, W, h, fi = key\n",
    "        yv, pv, meta = preds_val[key]\n",
    "        yt, pt, dt = preds_tst.get(key, (np.array([]), np.array([]), np.array([])))\n",
    "        if len(yv) == 0 or len(yt) == 0:\n",
    "            continue\n",
    "        floor = precision_floor[f\"D+{h}\"] or 0.0\n",
    "        thr, mval = select_threshold_val(yv, pv, floor)\n",
    "        reasons = []\n",
    "        if thr is None:\n",
    "            reasons.append(\"piso/Nmin/flood_val\")\n",
    "            # Ainda assim, para registro no TESTE, usar threshold de melhor precisão com N_min se existir, senão o de maior precisão\n",
    "            best_thr_tmp, best_prec_tmp, best_m_tmp = None, -1.0, None\n",
    "            for t in threshold_grid:\n",
    "                mv = binary_eval(yv, pv, t)\n",
    "                if mv[\"coverage_count\"] >= N_min_preds_val and mv[\"precision\"] > best_prec_tmp:\n",
    "                    best_prec_tmp, best_thr_tmp, best_m_tmp = mv[\"precision\"], t, mv\n",
    "            if best_thr_tmp is None:\n",
    "                for t in threshold_grid:\n",
    "                    mv = binary_eval(yv, pv, t)\n",
    "                    if mv[\"precision\"] > best_prec_tmp:\n",
    "                        best_prec_tmp, best_thr_tmp, best_m_tmp = mv[\"precision\"], t, mv\n",
    "            thr = float(best_thr_tmp if best_thr_tmp is not None else 0.5)\n",
    "            mval = best_m_tmp or {}\n",
    "        mtest = binary_eval(yt, pt, thr)\n",
    "        # sanity assert\n",
    "        sanity_ok = mval.get(\"sanity_ok\", True) and mtest.get(\"sanity_ok\", True)\n",
    "        if not sanity_ok:\n",
    "            reasons.append(\"metric_swap_detected\")\n",
    "        # cobertura mínima & flood guard em TESTE\n",
    "        cov_ok = (mtest[\"coverage_rate\"] >= coverage_min_rate) or (mtest[\"coverage_count\"] >= coverage_min_count)\n",
    "        flood_ok = (mval.get(\"coverage_rate\", 0.0) <= FLOOD_GUARD_MAX) and (mtest[\"coverage_rate\"] <= FLOOD_GUARD_MAX)\n",
    "        piso_ok = (mtest[\"precision\"] >= floor)\n",
    "        fold_eligible = bool(piso_ok and cov_ok and flood_ok and sanity_ok)\n",
    "        fold_rows.append(dict(\n",
    "            model=model, window=W, horizon=h, fold=fi, thr=thr,\n",
    "            prec_val=mval.get(\"precision\", np.nan), rec_val=mval.get(\"recall\", np.nan), rate_val=mval.get(\"coverage_rate\", np.nan),\n",
    "            prec_test=mtest[\"precision\"], rec_test=mtest[\"recall\"], f1_test=mtest[\"f1\"], acc_test=mtest[\"acc\"],\n",
    "            rate_test=mtest[\"coverage_rate\"], n_pred_test=mtest[\"coverage_count\"],\n",
    "            cm_TP=int(mtest[\"cm\"][0,0]), cm_FP=int(mtest[\"cm\"][0,1]), cm_FN=int(mtest[\"cm\"][1,0]), cm_TN=int(mtest[\"cm\"][1,1]),\n",
    "            fold_eligible=fold_eligible, reasons=\",\".join(reasons),\n",
    "        ))\n",
    "\n",
    "    if not fold_rows:\n",
    "        print(\"CHECKLIST_FAILURE: nenhuma combinação produziu resultados.\")\n",
    "        return\n",
    "\n",
    "    folds_df = pd.DataFrame(fold_rows)\n",
    "\n",
    "    # Agregar por combinação/horizonte somando CMs e recomputando métricas\n",
    "    agg_rows: List[Dict[str, Any]] = []\n",
    "    thr_medians: Dict[Tuple[str,int,int], float] = {}\n",
    "    for (h, m, W), grp in folds_df.groupby([\"horizon\",\"model\",\"window\"], as_index=False):\n",
    "        TP = int(grp[\"cm_TP\"].sum()); FP = int(grp[\"cm_FP\"].sum()); FN = int(grp[\"cm_FN\"].sum()); TN = int(grp[\"cm_TN\"].sum())\n",
    "        prec, rec, f1, acc = cm_metrics(np.array([[TP, FP],[FN, TN]], dtype=float))\n",
    "        n_pred = int(grp[\"n_pred_test\"].sum()); n_total = int((TP+FP+FN+TN))\n",
    "        rate = float(n_pred / max(1, n_total)) if n_total > 0 else 0.0\n",
    "        floor = precision_floor[f\"D+{h}\"] or 0.0\n",
    "        # mediana de thresholds apenas entre folds elegíveis\n",
    "        elig_thr = grp.loc[grp[\"fold_eligible\"]==True, \"thr\"].values\n",
    "        thr_med = float(np.median(elig_thr)) if len(elig_thr) else float(\"nan\")\n",
    "        thr_medians[(m, W, h)] = thr_med\n",
    "        # checagens\n",
    "        cov_ok = (rate >= coverage_min_rate) or (n_pred >= coverage_min_count)\n",
    "        flood_ok = (grp[\"rate_val\"].mean() <= FLOOD_GUARD_MAX) and (rate <= FLOOD_GUARD_MAX)\n",
    "        piso_ok = (prec >= floor)\n",
    "        sanity_ok = bool((folds_df[(folds_df[\"horizon\"]==h)&(folds_df[\"model\"]==m)&(folds_df[\"window\"]==W)][\"reasons\"].str.contains(\"metric_swap_detected\")).sum() == 0)\n",
    "        eligible = bool(piso_ok and cov_ok and flood_ok and sanity_ok)\n",
    "        reason_parts = []\n",
    "        if not piso_ok: reason_parts.append(\"piso\")\n",
    "        if not cov_ok: reason_parts.append(\"cobertura\")\n",
    "        if not flood_ok: reason_parts.append(\"inundacao\")\n",
    "        if not sanity_ok: reason_parts.append(\"sanity\")\n",
    "        agg_rows.append(dict(horizon=h, model=m, window=W, TP=TP, FP=FP, FN=FN, TN=TN,\n",
    "                             precisao_CAI=prec, recall_CAI=rec, F1_CAI=f1, acc=acc,\n",
    "                             pred_CAI_rate=rate, num_pred_CAI=n_pred,\n",
    "                             eligible=eligible, reason=\",\".join(reason_parts), folds=int(grp[\"fold\"].nunique()),\n",
    "                             threshold_median=thr_med))\n",
    "    agg_df = pd.DataFrame(agg_rows).sort_values([\"horizon\",\"model\",\"window\"]) if agg_rows else pd.DataFrame()\n",
    "\n",
    "    # Top-3 e melhores por horizonte\n",
    "    best_by_h: Dict[int, Dict[str, Any]] = {}\n",
    "    no_winner: Dict[int, bool] = {1: False, 3: False, 5: False}\n",
    "    for h in horizons:\n",
    "        sub = agg_df[agg_df[\"horizon\"]==h].copy()\n",
    "        elig = sub[sub[\"eligible\"] == True].copy()\n",
    "        print(f\"\\nRESUMO — D+{h} (TESTE agregado) — modelo × janela\")\n",
    "        if sub.empty:\n",
    "            print(\"–\")\n",
    "        else:\n",
    "            print(sub[[\"model\",\"window\",\"precisao_CAI\",\"recall_CAI\",\"F1_CAI\",\"acc\",\"pred_CAI_rate\",\"eligible\",\"reason\",\"folds\",\"threshold_median\"]].to_string(index=False))\n",
    "        print(f\"\\nTOP-3 — D+{h} (TESTE)\")\n",
    "        if elig.empty:\n",
    "            print(\"–\")\n",
    "            no_winner[h] = True\n",
    "        else:\n",
    "            elig_sorted = elig.sort_values([\"recall_CAI\",\"F1_CAI\",\"acc\"], ascending=[False,False,False])\n",
    "            top3 = elig_sorted.head(3).reset_index(drop=True)\n",
    "            print(top3[[\"model\",\"window\",\"precisao_CAI\",\"recall_CAI\",\"F1_CAI\",\"acc\",\"pred_CAI_rate\",\"folds\",\"threshold_median\"]].to_string(index=False))\n",
    "            best = elig_sorted.iloc[0]\n",
    "            best_by_h[h] = best.to_dict()\n",
    "            # Matriz de confusão agregada do melhor\n",
    "            print(f\"\\nMATRIZ DE CONFUSÃO — melhor combinação D+{h} (modelo={best['model']}, janela={int(best['window'])}) [CAI=1, N_CAI=0]\")\n",
    "            header = [\"\", \"pred_CAI\", \"pred_NAO_CAI\"]\n",
    "            print(\"{:<14s}{:>10s}{:>14s}\".format(*header))\n",
    "            print(\"{:<14s}{:>10d}{:>14d}\".format(\"true_CAI\", int(best.get(\"TP\",0)), int(best.get(\"FP\",0))))\n",
    "            print(\"{:<14s}{:>10d}{:>14d}\".format(\"true_NAO_CAI\", int(best.get(\"FN\",0)), int(best.get(\"TN\",0))))\n",
    "\n",
    "    # Threshold operacional (medianas)\n",
    "    threshold_operacional: Dict[int, float] = {}\n",
    "    print(\"\\nTHRESHOLD OPERACIONAL (mediana entre folds elegíveis)\")\n",
    "    for h in horizons:\n",
    "        val = float(best_by_h[h][\"threshold_median\"]) if h in best_by_h and np.isfinite(best_by_h[h][\"threshold_median\"]) else float(\"nan\")\n",
    "        threshold_operacional[h] = val\n",
    "        print(f\"- D+{h}: {val if np.isfinite(val) else '–'}\")\n",
    "\n",
    "    # Sequência final — somente se houver elegíveis\n",
    "    final_seq = []\n",
    "    all_operable = all(h in best_by_h for h in horizons)\n",
    "    if all_operable:\n",
    "        # pegar último fold\n",
    "        try:\n",
    "            last_fold = max(int(k[3]) for k in preds_tst.keys()) if preds_tst else None\n",
    "        except Exception:\n",
    "            last_fold = None\n",
    "        if last_fold is not None:\n",
    "            for h in horizons:\n",
    "                b = best_by_h[h]\n",
    "                m, W = str(b[\"model\"]), int(b[\"window\"]) \n",
    "                thr_med = float(b.get(\"threshold_median\", float(\"nan\")))\n",
    "                yt, pt, dt = preds_tst.get((m, W, h, last_fold), (np.array([]), np.array([]), np.array([])))\n",
    "                if len(pt) == 0 or not np.isfinite(thr_med):\n",
    "                    final_seq.append(\"—\")\n",
    "                else:\n",
    "                    yhat = (pt >= thr_med).astype(int)\n",
    "                    final_seq.append(\"CAI\" if int(yhat[-1]) == 1 else \"NÃO CAI\")\n",
    "        else:\n",
    "            all_operable = False\n",
    "    print(\"\\nSEQUÊNCIA FINAL (último bloco de TESTE):\")\n",
    "    print(f\"- (D+1, D+3, D+5) = {', '.join(final_seq) if all_operable and final_seq else '—'}\")\n",
    "\n",
    "    # Baselines — TESTE (médias por horizonte)\n",
    "    baseline_rows = []\n",
    "    for h in horizons:\n",
    "        seen = set()\n",
    "        for key, (yt, pt, dt) in preds_tst.items():\n",
    "            m, W, hh, fi = key\n",
    "            if hh != h or fi in seen or len(yt) == 0:\n",
    "                continue\n",
    "            seen.add(fi)\n",
    "            n = len(yt)\n",
    "            # Sempre NÃO CAI\n",
    "            pred0 = np.zeros(n, dtype=int)\n",
    "            cm0 = confusion_matrix(yt, pred0, labels=[1,0])\n",
    "            p0, r0, f10, a0 = cm_metrics(cm0)\n",
    "            baseline_rows.append(dict(horizon=h, baseline=\"Sempre_NAO_CAI\", precision=p0, f1=f10))\n",
    "            # ProporcaoTreino>0.5 via VAL proxy\n",
    "            # pegar um yv do mesmo fold\n",
    "            yv = None\n",
    "            for k2, (yvv, pvv, meta) in preds_val.items():\n",
    "                mm, WW, hhh, fii = k2\n",
    "                if hhh == h and fii == fi and len(yvv) > 0:\n",
    "                    yv = yvv\n",
    "                    break\n",
    "            if yv is not None:\n",
    "                pred1 = np.ones(n, dtype=int) if float((yv == 1).mean()) > 0.5 else np.zeros(n, dtype=int)\n",
    "                cm1 = confusion_matrix(yt, pred1, labels=[1,0])\n",
    "                p1, r1, f11, a1 = cm_metrics(cm1)\n",
    "                baseline_rows.append(dict(horizon=h, baseline=\"ProporcaoTreino>0.5\", precision=p1, f1=f11))\n",
    "            # Sinal de ontem\n",
    "            ret1_map = pd.Series(df.set_index(\"__date__\")[\"ret1\"])  # t-1 < 0 => CAI\n",
    "            pred2 = []\n",
    "            for d in dt:\n",
    "                prev = pd.to_datetime(d) - pd.Timedelta(days=1)\n",
    "                v = ret1_map.get(prev, np.nan)\n",
    "                pred2.append(1 if (pd.notna(v) and v < 0) else 0)\n",
    "            pred2 = np.array(pred2, int)\n",
    "            cm2 = confusion_matrix(yt, pred2, labels=[1,0])\n",
    "            p2, r2, f12, a2 = cm_metrics(cm2)\n",
    "            baseline_rows.append(dict(horizon=h, baseline=\"SinalOntem\", precision=p2, f1=f12))\n",
    "            # Momentum_3d (soma log ret últimos 3 < 0)\n",
    "            mom3 = df[\"ret1\"].rolling(3).sum().shift(1)\n",
    "            mom_map = pd.Series(mom3.values, index=df[\"__date__\"])  # alinhado\n",
    "            pred3 = [1 if (pd.notna(mom_map.get(pd.to_datetime(d), np.nan)) and mom_map.get(pd.to_datetime(d)) < 0) else 0 for d in dt]\n",
    "            pred3 = np.array(pred3, int)\n",
    "            cm3 = confusion_matrix(yt, pred3, labels=[1,0])\n",
    "            p3, r3, f13, a3 = cm_metrics(cm3)\n",
    "            baseline_rows.append(dict(horizon=h, baseline=\"Momentum_3d\", precision=p3, f1=f13))\n",
    "    base_df = pd.DataFrame(baseline_rows)\n",
    "\n",
    "    # Painel V1.2.1 — Uma página (markdown-like)\n",
    "    print(\"\\n[Painel — Checklist Operacional (V1.2.1)]\")\n",
    "    lines = []\n",
    "    apto_map: Dict[int, str] = {}\n",
    "    for h in horizons:\n",
    "        lines.append(f\"## HORIZONTE D+{h}\")\n",
    "        floor = precision_floor[f\"D+{h}\"] or 0.0\n",
    "        thr_op = best_by_h[h][\"threshold_median\"] if h in best_by_h else \"—\"\n",
    "        lines.append(f\"Piso de Precisão(CAI): {floor:.2f}\")\n",
    "        lines.append(f\"Threshold Operacional (mediana): {thr_op if isinstance(thr_op, str) or np.isnan(thr_op) else f'{thr_op:.2f}'}\\n\")\n",
    "        if h in best_by_h:\n",
    "            b = best_by_h[h]\n",
    "            lines.append(f\"Vencedor: {b['model']} — janela={int(b['window'])}\")\n",
    "            lines.append(f\"Precisão(CAI): {b['precisao_CAI']:.2f}   Recall(CAI): {b['recall_CAI']:.2f}   F1(CAI): {b['F1_CAI']:.2f}   ACC: {b['acc']:.2f}\")\n",
    "            # taxas: usar médias aproximadas a partir de agg (VAL não diretamente disponível aqui, usar média dos folds do vencedor)\n",
    "            sub = folds_df[(folds_df[\"horizon\"]==h)&(folds_df[\"model\"]==b['model'])&(folds_df[\"window\"]==b['window'])]\n",
    "            rate_val = float(sub[\"rate_val\"].mean()) if len(sub) else float(\"nan\")\n",
    "            rate_test = float(b['pred_CAI_rate'])\n",
    "            def pct(v):\n",
    "                return f\"{(v*100):.1f}%\" if np.isfinite(v) else \"—\"\n",
    "            lines.append(f\"Pred_CAI_rate (VAL/Teste): {pct(rate_val)} / {pct(rate_test)}\")\n",
    "            lines.append(f\"Confusão (Teste): TP={int(b.get('TP',0))}  FP={int(b.get('FP',0))}  FN={int(b.get('FN',0))}  TN={int(b.get('TN',0))}\\n\")\n",
    "            # Checks\n",
    "            piso_ok = (b['precisao_CAI'] >= floor)\n",
    "            cov_ok = (b['pred_CAI_rate'] >= coverage_min_rate) or (int(b['num_pred_CAI']) >= coverage_min_count)\n",
    "            flood_ok = (rate_val <= FLOOD_GUARD_MAX) and (rate_test <= FLOOD_GUARD_MAX)\n",
    "            sanity_ok = not (folds_df[(folds_df[\"horizon\"]==h)&(folds_df[\"model\"]==b['model'])&(folds_df[\"window\"]==b['window'])][\"reasons\"].str.contains(\"metric_swap_detected\")).any()\n",
    "            # Baselines precision comparison\n",
    "            bd = base_df[base_df[\"horizon\"]==h]\n",
    "            best_beats_all = True\n",
    "            if not bd.empty:\n",
    "                for _, row in bd.iterrows():\n",
    "                    if b['precisao_CAI'] < float(row['precision']) - 1e-12:\n",
    "                        best_beats_all = False\n",
    "                        break\n",
    "            lines.append(\"Checks:\")\n",
    "            lines.append(f\"{'✓' if piso_ok else '✗'} Piso de Precisão(CAI)\")\n",
    "            lines.append(f\"{'✓' if cov_ok else '✗'} Cobertura mínima\")\n",
    "            lines.append(f\"{'✓' if flood_ok else '✗'} Freio de inundação (VAL/Teste)\")\n",
    "            lines.append(f\"{'✓' if sanity_ok else '✗'} Sanity de métricas\")\n",
    "            lines.append(f\"{'✓' if best_beats_all else '!'} Baselines (precisão)\")\n",
    "            # Baselines resumo\n",
    "            lines.append(\"\\nBaselines — Precisão(CAI) / F1:\")\n",
    "            if bd.empty:\n",
    "                lines.append(\"• –\")\n",
    "            else:\n",
    "                # garantir ordem\n",
    "                order = [\"Sempre_NAO_CAI\",\"ProporcaoTreino>0.5\",\"SinalOntem\",\"Momentum_3d\"]\n",
    "                for base in order:\n",
    "                    row = bd[bd[\"baseline\"]==base]\n",
    "                    if not row.empty:\n",
    "                        pr = float(row.iloc[0][\"precision\"]); f1 = float(row.iloc[0][\"f1\"])\n",
    "                        lines.append(f\"• {base}: {pr:.2f} / {f1:.2f}\")\n",
    "            # Decisão\n",
    "            apto = bool(piso_ok and cov_ok and flood_ok and sanity_ok and best_beats_all)\n",
    "            apto_map[h] = \"SIM\" if apto else \"NÃO\"\n",
    "            if not apto:\n",
    "                motivo = []\n",
    "                if not piso_ok: motivo.append(\"piso\")\n",
    "                if not cov_ok: motivo.append(\"cobertura\")\n",
    "                if not flood_ok: motivo.append(\"inundacao\")\n",
    "                if not sanity_ok: motivo.append(\"sanity\")\n",
    "                if not best_beats_all: motivo.append(\"baselines\")\n",
    "                lines.append(f\"\\nApto a operar?  NÃO\")\n",
    "                lines.append(f\"Motivo (se NÃO): {','.join(motivo)}\\n\")\n",
    "            else:\n",
    "                lines.append(f\"\\nApto a operar?  SIM\\n\")\n",
    "        else:\n",
    "            apto_map[h] = \"NÃO\"\n",
    "            lines.append(\"Vencedor: —\")\n",
    "            lines.append(\"Precisão(CAI): —   Recall(CAI): —   F1(CAI): —   ACC: —\")\n",
    "            lines.append(\"Pred_CAI_rate (VAL/Teste): — / —\")\n",
    "            lines.append(\"Confusão (Teste): TP=—  FP=—  FN=—  TN=—\\n\")\n",
    "            lines.append(\"Checks:\")\n",
    "            lines.append(\"✗ Piso de Precisão(CAI)\")\n",
    "            lines.append(\"✗ Cobertura mínima\")\n",
    "            lines.append(\"✗ Freio de inundação (VAL/Teste)\")\n",
    "            lines.append(\"✗ Sanity de métricas\")\n",
    "            lines.append(\"! Baselines (precisão)\")\n",
    "            lines.append(\"\\nBaselines — Precisão(CAI) / F1:\\n• –\")\n",
    "            lines.append(\"\\nApto a operar?  NÃO\\n\")\n",
    "\n",
    "    # Rodapé\n",
    "    seq_str = \", \".join(final_seq) if all_operable and final_seq else \"—\"\n",
    "    lines.append(f\"Resumo: D+1={apto_map.get(1,'NÃO')} | D+3={apto_map.get(3,'NÃO')} | D+5={apto_map.get(5,'NÃO')}   |  Sequência final (último TESTE): {seq_str}\")\n",
    "    if repeat_msgs:\n",
    "        lines.append(\"Alertas:\")\n",
    "        for m in repeat_msgs:\n",
    "            lines.append(f\"• {m}\")\n",
    "        if abort_xgb:\n",
    "            lines.append(\"Perguntas: Atualizar xgboost para ≥1.7.6 e habilitar early stopping? [ ]Sim [ ]Não\")\n",
    "    else:\n",
    "        lines.append(\"Alertas: —\")\n",
    "    lines.append(\"SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet | Folds: 10 | Seeds: {}/{}/{}\".format(SEED_NUMPY, SEED_TF, SEED_PY))\n",
    "    lines.append(\"Precision floors: D+1={:.2f}; D+3={:.2f}; D+5={:.2f} | N_min_preds_val=10\".format(precision_floor['D+1'], precision_floor['D+3'], precision_floor['D+5']))\n",
    "\n",
    "    print(\"\\n\" + \"\\n\".join(lines))\n",
    "\n",
    "    # Checklist final\n",
    "    print(\"\\nCHECKLIST\")\n",
    "    print(f\"- XGBoost versão detectada: {XGB_VERSION}\")\n",
    "    print(f\"- Branch XGB: {'sklearn.fit(early_stopping_rounds)' if XGB_GE_1_7_6 else 'xgb.train fallback'} | abort_xgb={abort_xgb}\")\n",
    "    print(f\"- SSOT: {GOLD_PATH} (tier=GOLD)\")\n",
    "    print(f\"- Horizontes: {horizons} | Janelas: {windows} | Folds: {len(splits)}\")\n",
    "    print(f\"- precision_floor: D+1={precision_floor['D+1']:.2f}, D+3={precision_floor['D+3']:.2f}, D+5={precision_floor['D+5']:.2f}\")\n",
    "    print(f\"- dry_run=True (nenhum arquivo salvo)\")\n",
    "    print(f\"\\n[{now_ts()}] Fim — CAI vs NÃO CAI (dry_run={dry_run})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896c0305",
   "metadata": {},
   "source": [
    "## CAI vs NÃO CAI — V1.2 + V1.2.1 + Delta XGBoost 3.0.5 (dry_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b3348b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-19 20:25:24] Início — CAI vs NÃO CAI — V1.2/V1.2.1 (Delta XGBoost 3.0.5) | Seeds: numpy=2025, tf=42, xgb=7\n",
      "\n",
      "[PROVA SSOT]\n",
      "- Caminho: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Linhas: 3400 | date_min: 2012-01-03 | date_max: 2025-09-19\n",
      "- Colunas (amostra): date, open, high, low, close, volume, ticker, open_norm, high_norm, low_norm, close_norm, volume_norm, return_1d, volatility_5d, sma_5, sma_20, sma_ratio, y_h1, y_h3, y_h5, y_h1_cls, y_h3_cls, y_h5_cls, year, __date__ ...\n",
      "\n",
      "[WALK-FORWARD — Folds]\n",
      "Fold 01 | train[2012-01-03 → 2013-04-02] | val[2013-04-03 → 2013-07-02] | test[2013-07-03 → 2014-01-02]\n",
      "Fold 02 | train[2012-01-03 → 2013-10-02] | val[2013-10-03 → 2014-01-02] | test[2014-01-03 → 2014-07-02]\n",
      "Fold 03 | train[2012-01-03 → 2014-04-02] | val[2014-04-03 → 2014-07-02] | test[2014-07-03 → 2015-01-02]\n",
      "Fold 04 | train[2012-01-03 → 2014-10-02] | val[2014-10-03 → 2015-01-02] | test[2015-01-03 → 2015-07-02]\n",
      "Fold 05 | train[2012-01-03 → 2015-04-02] | val[2015-04-03 → 2015-07-02] | test[2015-07-03 → 2016-01-02]\n",
      "Fold 06 | train[2012-01-03 → 2015-10-02] | val[2015-10-03 → 2016-01-02] | test[2016-01-03 → 2016-07-02]\n",
      "Fold 07 | train[2012-01-03 → 2016-04-02] | val[2016-04-03 → 2016-07-02] | test[2016-07-03 → 2017-01-02]\n",
      "Fold 08 | train[2012-01-03 → 2016-10-02] | val[2016-10-03 → 2017-01-02] | test[2017-01-03 → 2017-07-02]\n",
      "Fold 09 | train[2012-01-03 → 2017-04-02] | val[2017-04-03 → 2017-07-02] | test[2017-07-03 → 2018-01-02]\n",
      "Fold 10 | train[2012-01-03 → 2017-10-02] | val[2017-10-03 → 2018-01-02] | test[2018-01-03 → 2018-07-02]\n",
      "\n",
      "[ERROS REPETIDOS — intervenção requerida]\n",
      "- repeat_error_stop:xgb_sklearn_fit (x3)\n",
      "\n",
      "[ERROS REPETIDOS — intervenção requerida]\n",
      "- repeat_error_stop:xgb_sklearn_fit (x3)\n",
      "\n",
      "[PISOS DE PRECISÃO — usados]\n",
      "- D+1: 0.82 (fixo)\n",
      "- D+3: 0.78\n",
      "- D+5: 0.85\n",
      "- N_min_preds_val: 10; flood_guard ≤ 50%\n",
      "\n",
      "[PISOS DE PRECISÃO — usados]\n",
      "- D+1: 0.82 (fixo)\n",
      "- D+3: 0.78\n",
      "- D+5: 0.85\n",
      "- N_min_preds_val: 10; flood_guard ≤ 50%\n",
      "\n",
      "RESUMO — D+1 (TESTE agregado) — modelo × janela\n",
      "model  window  precisao_CAI  recall_CAI   F1_CAI      acc  pred_CAI_rate  eligible                reason  folds  threshold_median\n",
      " LSTM       5      0.571675    0.503040 0.535166 0.515993       0.553872     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      10      0.607143    0.518293 0.559211 0.528998       0.576450     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      15      0.569288    0.492707 0.528236 0.500919       0.567096     False piso,inundacao,sanity     10               NaN\n",
      "\n",
      "TOP-3 — D+1 (TESTE)\n",
      "–\n",
      "\n",
      "RESUMO — D+3 (TESTE agregado) — modelo × janela\n",
      "model  window  precisao_CAI  recall_CAI   F1_CAI      acc  pred_CAI_rate  eligible                reason  folds  threshold_median\n",
      " LSTM       5      0.613596    0.481742 0.539732 0.507576       0.599327     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      10      0.634686    0.478442 0.545599 0.496485       0.631810     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      15      0.600768    0.480061 0.533674 0.497243       0.599265     False piso,inundacao,sanity     10               NaN\n",
      "\n",
      "TOP-3 — D+3 (TESTE)\n",
      "–\n",
      "\n",
      "RESUMO — D+5 (TESTE agregado) — modelo × janela\n",
      "model  window  precisao_CAI  recall_CAI   F1_CAI      acc  pred_CAI_rate  eligible                reason  folds  threshold_median\n",
      " LSTM       5      0.636042    0.515759 0.569620 0.542088       0.587542     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      10      0.568807    0.500000 0.532189 0.521090       0.544815     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      15      0.633588    0.503030 0.560811 0.522059       0.606618     False piso,inundacao,sanity     10               NaN\n",
      "\n",
      "TOP-3 — D+5 (TESTE)\n",
      "–\n",
      "\n",
      "THRESHOLD OPERACIONAL (mediana entre folds elegíveis)\n",
      "- D+1: –\n",
      "- D+3: –\n",
      "- D+5: –\n",
      "\n",
      "SEQUÊNCIA FINAL (último bloco de TESTE):\n",
      "- (D+1, D+3, D+5) = —\n",
      "\n",
      "[Painel — Checklist Operacional (V1.2.1)]\n",
      "\n",
      "Python: 3.12.3 | xgboost: 3.0.5 | device: cpu\n",
      "SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet | Folds: 10 | Dry-run: True\n",
      "Seeds: np=2025 | tf=42 | xgb=7\n",
      "## HORIZONTE D+1\n",
      "Piso de Precisão(CAI): 0.82\n",
      "Threshold Operacional (mediana): —\n",
      "\n",
      "Vencedor: —\n",
      "Precisão(CAI): —   Recall(CAI): —   F1(CAI): —   ACC: —\n",
      "Pred_CAI_rate (VAL/Teste): — / —\n",
      "Confusão (Teste): TP=—  FP=—  FN=—  TN=—\n",
      "\n",
      "Checks:\n",
      "✗ Piso de Precisão(CAI)\n",
      "✗ Cobertura mínima\n",
      "✗ Freio de inundação (VAL/Teste)\n",
      "✗ Sanity de métricas\n",
      "! Baselines (precisão)\n",
      "\n",
      "Baselines — Precisão(CAI) / F1:\n",
      "• –\n",
      "\n",
      "Apto a operar?  NÃO\n",
      "\n",
      "## HORIZONTE D+3\n",
      "Piso de Precisão(CAI): 0.78\n",
      "Threshold Operacional (mediana): —\n",
      "\n",
      "Vencedor: —\n",
      "Precisão(CAI): —   Recall(CAI): —   F1(CAI): —   ACC: —\n",
      "Pred_CAI_rate (VAL/Teste): — / —\n",
      "Confusão (Teste): TP=—  FP=—  FN=—  TN=—\n",
      "\n",
      "Checks:\n",
      "✗ Piso de Precisão(CAI)\n",
      "✗ Cobertura mínima\n",
      "✗ Freio de inundação (VAL/Teste)\n",
      "✗ Sanity de métricas\n",
      "! Baselines (precisão)\n",
      "\n",
      "Baselines — Precisão(CAI) / F1:\n",
      "• –\n",
      "\n",
      "Apto a operar?  NÃO\n",
      "\n",
      "## HORIZONTE D+5\n",
      "Piso de Precisão(CAI): 0.85\n",
      "Threshold Operacional (mediana): —\n",
      "\n",
      "Vencedor: —\n",
      "Precisão(CAI): —   Recall(CAI): —   F1(CAI): —   ACC: —\n",
      "Pred_CAI_rate (VAL/Teste): — / —\n",
      "Confusão (Teste): TP=—  FP=—  FN=—  TN=—\n",
      "\n",
      "Checks:\n",
      "✗ Piso de Precisão(CAI)\n",
      "✗ Cobertura mínima\n",
      "✗ Freio de inundação (VAL/Teste)\n",
      "✗ Sanity de métricas\n",
      "! Baselines (precisão)\n",
      "\n",
      "Baselines — Precisão(CAI) / F1:\n",
      "• –\n",
      "\n",
      "Apto a operar?  NÃO\n",
      "\n",
      "Resumo: D+1=NÃO | D+3=NÃO | D+5=NÃO   |  Sequência final (último TESTE): —\n",
      "Alertas:\n",
      "• repeat_error_stop:xgb_sklearn_fit (x3)\n",
      "• repeat_error_stop:xgb_sklearn_fit — Ambiente tem múltiplos XGBoost? Limpar venv e reinstalar 3.0.5? [ ]Sim [ ]Não\n",
      "SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet | Folds: 10 | Seeds: 2025/42/7 | TF device=CPU (fallback)\n",
      "Precision floors: D+1=0.82; D+3=0.78; D+5=0.85 | N_min_preds_val=10 | flood_guard≤50%\n",
      "\n",
      "CHECKLIST\n",
      "- xgboost_version detectada: 3.0.5\n",
      "- XGBoost API: sklearn.fit(early_stopping_rounds=50) | device=cpu\n",
      "- SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Horizontes: [1, 3, 5] | Janelas: [5, 10, 15] | Folds: 10\n",
      "- precision_floor: D+1=0.82, D+3=0.78, D+5=0.85\n",
      "- Pergunta: Ambiente tem múltiplos XGBoost? Limpar venv e reinstalar 3.0.5? [ ]Sim [ ]Não\n",
      "- dry_run=True (nenhum arquivo salvo)\n",
      "\n",
      "[2025-09-19 20:32:48] Fim — CAI vs NÃO CAI (dry_run=True)\n",
      "\n",
      "RESUMO — D+1 (TESTE agregado) — modelo × janela\n",
      "model  window  precisao_CAI  recall_CAI   F1_CAI      acc  pred_CAI_rate  eligible                reason  folds  threshold_median\n",
      " LSTM       5      0.571675    0.503040 0.535166 0.515993       0.553872     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      10      0.607143    0.518293 0.559211 0.528998       0.576450     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      15      0.569288    0.492707 0.528236 0.500919       0.567096     False piso,inundacao,sanity     10               NaN\n",
      "\n",
      "TOP-3 — D+1 (TESTE)\n",
      "–\n",
      "\n",
      "RESUMO — D+3 (TESTE agregado) — modelo × janela\n",
      "model  window  precisao_CAI  recall_CAI   F1_CAI      acc  pred_CAI_rate  eligible                reason  folds  threshold_median\n",
      " LSTM       5      0.613596    0.481742 0.539732 0.507576       0.599327     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      10      0.634686    0.478442 0.545599 0.496485       0.631810     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      15      0.600768    0.480061 0.533674 0.497243       0.599265     False piso,inundacao,sanity     10               NaN\n",
      "\n",
      "TOP-3 — D+3 (TESTE)\n",
      "–\n",
      "\n",
      "RESUMO — D+5 (TESTE agregado) — modelo × janela\n",
      "model  window  precisao_CAI  recall_CAI   F1_CAI      acc  pred_CAI_rate  eligible                reason  folds  threshold_median\n",
      " LSTM       5      0.636042    0.515759 0.569620 0.542088       0.587542     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      10      0.568807    0.500000 0.532189 0.521090       0.544815     False piso,inundacao,sanity     10               NaN\n",
      " LSTM      15      0.633588    0.503030 0.560811 0.522059       0.606618     False piso,inundacao,sanity     10               NaN\n",
      "\n",
      "TOP-3 — D+5 (TESTE)\n",
      "–\n",
      "\n",
      "THRESHOLD OPERACIONAL (mediana entre folds elegíveis)\n",
      "- D+1: –\n",
      "- D+3: –\n",
      "- D+5: –\n",
      "\n",
      "SEQUÊNCIA FINAL (último bloco de TESTE):\n",
      "- (D+1, D+3, D+5) = —\n",
      "\n",
      "[Painel — Checklist Operacional (V1.2.1)]\n",
      "\n",
      "Python: 3.12.3 | xgboost: 3.0.5 | device: cpu\n",
      "SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet | Folds: 10 | Dry-run: True\n",
      "Seeds: np=2025 | tf=42 | xgb=7\n",
      "## HORIZONTE D+1\n",
      "Piso de Precisão(CAI): 0.82\n",
      "Threshold Operacional (mediana): —\n",
      "\n",
      "Vencedor: —\n",
      "Precisão(CAI): —   Recall(CAI): —   F1(CAI): —   ACC: —\n",
      "Pred_CAI_rate (VAL/Teste): — / —\n",
      "Confusão (Teste): TP=—  FP=—  FN=—  TN=—\n",
      "\n",
      "Checks:\n",
      "✗ Piso de Precisão(CAI)\n",
      "✗ Cobertura mínima\n",
      "✗ Freio de inundação (VAL/Teste)\n",
      "✗ Sanity de métricas\n",
      "! Baselines (precisão)\n",
      "\n",
      "Baselines — Precisão(CAI) / F1:\n",
      "• –\n",
      "\n",
      "Apto a operar?  NÃO\n",
      "\n",
      "## HORIZONTE D+3\n",
      "Piso de Precisão(CAI): 0.78\n",
      "Threshold Operacional (mediana): —\n",
      "\n",
      "Vencedor: —\n",
      "Precisão(CAI): —   Recall(CAI): —   F1(CAI): —   ACC: —\n",
      "Pred_CAI_rate (VAL/Teste): — / —\n",
      "Confusão (Teste): TP=—  FP=—  FN=—  TN=—\n",
      "\n",
      "Checks:\n",
      "✗ Piso de Precisão(CAI)\n",
      "✗ Cobertura mínima\n",
      "✗ Freio de inundação (VAL/Teste)\n",
      "✗ Sanity de métricas\n",
      "! Baselines (precisão)\n",
      "\n",
      "Baselines — Precisão(CAI) / F1:\n",
      "• –\n",
      "\n",
      "Apto a operar?  NÃO\n",
      "\n",
      "## HORIZONTE D+5\n",
      "Piso de Precisão(CAI): 0.85\n",
      "Threshold Operacional (mediana): —\n",
      "\n",
      "Vencedor: —\n",
      "Precisão(CAI): —   Recall(CAI): —   F1(CAI): —   ACC: —\n",
      "Pred_CAI_rate (VAL/Teste): — / —\n",
      "Confusão (Teste): TP=—  FP=—  FN=—  TN=—\n",
      "\n",
      "Checks:\n",
      "✗ Piso de Precisão(CAI)\n",
      "✗ Cobertura mínima\n",
      "✗ Freio de inundação (VAL/Teste)\n",
      "✗ Sanity de métricas\n",
      "! Baselines (precisão)\n",
      "\n",
      "Baselines — Precisão(CAI) / F1:\n",
      "• –\n",
      "\n",
      "Apto a operar?  NÃO\n",
      "\n",
      "Resumo: D+1=NÃO | D+3=NÃO | D+5=NÃO   |  Sequência final (último TESTE): —\n",
      "Alertas:\n",
      "• repeat_error_stop:xgb_sklearn_fit (x3)\n",
      "• repeat_error_stop:xgb_sklearn_fit — Ambiente tem múltiplos XGBoost? Limpar venv e reinstalar 3.0.5? [ ]Sim [ ]Não\n",
      "SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet | Folds: 10 | Seeds: 2025/42/7 | TF device=CPU (fallback)\n",
      "Precision floors: D+1=0.82; D+3=0.78; D+5=0.85 | N_min_preds_val=10 | flood_guard≤50%\n",
      "\n",
      "CHECKLIST\n",
      "- xgboost_version detectada: 3.0.5\n",
      "- XGBoost API: sklearn.fit(early_stopping_rounds=50) | device=cpu\n",
      "- SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet (tier=GOLD)\n",
      "- Horizontes: [1, 3, 5] | Janelas: [5, 10, 15] | Folds: 10\n",
      "- precision_floor: D+1=0.82, D+3=0.78, D+5=0.85\n",
      "- Pergunta: Ambiente tem múltiplos XGBoost? Limpar venv e reinstalar 3.0.5? [ ]Sim [ ]Não\n",
      "- dry_run=True (nenhum arquivo salvo)\n",
      "\n",
      "[2025-09-19 20:32:48] Fim — CAI vs NÃO CAI (dry_run=True)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "CAI vs NÃO CAI — V1.2 + V1.2.1 + Delta XGBoost 3.0.5 (dry_run=True)\n",
    "\n",
    "Regras essenciais:\n",
    "- SSOT: GOLD apenas (/home/wrm/BOLSA_2026/gold/IBOV_gold.parquet).\n",
    "- Walk-forward: 10 folds (train ≥18m, val 3m, test 6m).\n",
    "- Modelos: XGBoost (sklearn API forçada, early_stopping_rounds=50, eval_metric='aucpr', device cuda/cpu); LSTM compacto (CPU).\n",
    "- Calibração (Platt; fallback Isotonic) fit em VAL e aplicada em VAL/TESTE.\n",
    "- Pisos: D+1 fixo=0.82; D+3/D+5 = dinâmicos de VAL com N_min=10 e clip [0.70, 0.85].\n",
    "- Seleção de threshold (VAL): max recall(CAI) sujeito a: precisão(CAI) ≥ piso, N_pred_CAI_val ≥ 10, pred_CAI_rate_val ≤ 0.50. Sem relax.\n",
    "- Elegibilidade em TESTE (agregado): piso ok, cobertura mínima (≥10% ou ≥8), flood guard ≤50% em VAL e TESTE, sanity ok, vencedor supera baselines em Precisão(CAI).\n",
    "- Abortadores: repetir o mesmo erro ≥3x → parar etapa e registrar repeat_error_stop:<etapa>.\n",
    "- Checklist 1 página + Bloco de Diagnóstico (3 linhas) no topo.\n",
    "- Primeira execução: dry_run=True; nada é salvo/persistido.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import subprocess\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "\n",
    "# =========================\n",
    "# Seeds e ambiente\n",
    "# =========================\n",
    "\n",
    "SEED_NUMPY = 2025\n",
    "SEED_TF = 42\n",
    "SEED_PY = 7  # também usado como random_state do XGBoost\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED_PY)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # reduzir ruído TF\n",
    "\n",
    "import random\n",
    "random.seed(SEED_PY)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED_NUMPY)\n",
    "\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "pd.set_option(\"display.max_columns\", 160)\n",
    "\n",
    "# Imports principais\n",
    "_missing = []\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_VERSION = getattr(xgb, \"__version__\", \"0\")\n",
    "except Exception as e:\n",
    "    _missing.append(f\"xgboost ({e})\")\n",
    "    XGB_VERSION = \"0\"\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential  # pyright: ignore\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, Input  # pyright: ignore\n",
    "    from tensorflow.keras.callbacks import EarlyStopping  # pyright: ignore\n",
    "    tf.random.set_seed(SEED_TF)\n",
    "except Exception as e:\n",
    "    _missing.append(f\"tensorflow/keras ({e})\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# =========================\n",
    "# Parâmetros\n",
    "# =========================\n",
    "\n",
    "dry_run: bool = True\n",
    "\n",
    "# SSOT GOLD apenas\n",
    "GOLD_PATH = \"/home/wrm/BOLSA_2026/gold/IBOV_gold.parquet\"\n",
    "\n",
    "windows: List[int] = [5, 10, 15]\n",
    "horizons: List[int] = [1, 3, 5]\n",
    "\n",
    "# Validação temporal\n",
    "train_min_months: int = 18\n",
    "val_months: int = 3\n",
    "test_months: int = 6\n",
    "max_folds: int = 10\n",
    "\n",
    "# Priorização de CAI\n",
    "precision_floor: Dict[str, Optional[float]] = {\"D+1\": 0.82, \"D+3\": None, \"D+5\": None}\n",
    "coverage_min_rate: float = 0.10\n",
    "coverage_min_count: int = 8\n",
    "threshold_grid: List[float] = [i / 100.0 for i in range(10, 91)]  # 0.10 → 0.90\n",
    "N_min_preds_val: int = 10\n",
    "FLOOD_GUARD_MAX: float = 0.50  # pred_CAI_rate máximo em VAL/TESTE\n",
    "\n",
    "# XGBoost — sklearn API forçada (Delta 3.0.5)\n",
    "xgb_eval_metric = \"aucpr\"  # pode trocar para \"logloss\" se desejar estabilidade extra\n",
    "xgb_early_stopping_rounds: int = 50\n",
    "xgb_n_estimators: int = 2000\n",
    "\n",
    "# LSTM\n",
    "lstm_units: int = 48\n",
    "lstm_dropout: float = 0.2\n",
    "lstm_epochs: int = 50\n",
    "lstm_batch_size: int = 32\n",
    "lstm_patience: int = 5\n",
    "\n",
    "# =========================\n",
    "# Utilidades\n",
    "# =========================\n",
    "\n",
    "def now_ts() -> str:\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def summarize_df(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    rows, cols = df.shape\n",
    "    dmin = pd.to_datetime(df[\"__date__\"]).min(); dmax = pd.to_datetime(df[\"__date__\"]).max()\n",
    "    return dict(\n",
    "        row_count=str(rows),\n",
    "        date_min=str(getattr(dmin, 'date', lambda: '-')()) if pd.notnull(dmin) else \"-\",\n",
    "        date_max=str(getattr(dmax, 'date', lambda: '-')()) if pd.notnull(dmax) else \"-\",\n",
    "        columns=\", \".join(list(df.columns)[:25]) + (\" ...\" if df.shape[1] > 25 else \"\")\n",
    "    )\n",
    "\n",
    "def ensure_datetime(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # tenta usar coluna 'date' ou índice datetime\n",
    "    if \"date\" in df.columns:\n",
    "        out = df.copy()\n",
    "        out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n",
    "        out = out.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "        out[\"__date__\"] = out[\"date\"].values\n",
    "        return out\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        out = df.sort_index().copy()\n",
    "        out[\"__date__\"] = out.index\n",
    "        return out\n",
    "    raise ValueError(\"VALIDATION_ERROR: coluna 'date' ausente e índice não é DatetimeIndex.\")\n",
    "\n",
    "def month_add(d: pd.Timestamp, months: int) -> pd.Timestamp:\n",
    "    return d + pd.DateOffset(months=months)\n",
    "\n",
    "def build_walk_forward_splits(df: pd.DataFrame) -> List[Dict[str, pd.Timestamp]]:\n",
    "    dates = pd.to_datetime(df[\"__date__\"])  # type: ignore\n",
    "    start = dates.min().normalize(); end = dates.max().normalize()\n",
    "    train_end = month_add(start, train_min_months) - pd.DateOffset(days=1)\n",
    "    folds = []\n",
    "    test_start = train_end + pd.DateOffset(days=1)\n",
    "    test_end = month_add(test_start, test_months) - pd.DateOffset(days=1)\n",
    "    while test_start <= end and len(folds) < max_folds:\n",
    "        if test_end > end: test_end = end\n",
    "        val_end = test_start - pd.DateOffset(days=1)\n",
    "        val_start = month_add(val_end, -val_months) + pd.DateOffset(days=1)\n",
    "        tr_start = start; tr_end = val_start - pd.DateOffset(days=1)\n",
    "        if tr_start >= tr_end or val_start > val_end or test_start > test_end:\n",
    "            break\n",
    "        folds.append(dict(\n",
    "            train_start=tr_start, train_end=tr_end,\n",
    "            val_start=val_start, val_end=val_end,\n",
    "            test_start=test_start, test_end=test_end,\n",
    "        ))\n",
    "        test_start = test_end + pd.DateOffset(days=1)\n",
    "        test_end = month_add(test_start, test_months) - pd.DateOffset(days=1)\n",
    "    if not folds:\n",
    "        raise ValueError(\"VALIDATION_ERROR: não foi possível construir folds walk-forward.\")\n",
    "    return folds\n",
    "\n",
    "def subset(df: pd.DataFrame, a: pd.Timestamp, b: pd.Timestamp) -> pd.DataFrame:\n",
    "    return df.loc[(df[\"__date__\"] >= a) & (df[\"__date__\"] <= b)].copy()\n",
    "\n",
    "def detect_xgb_device() -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Retorna ('cuda','<GPU>') quando houver CUDA detectável; caso contrário ('cpu','-').\n",
    "    Usa nvidia-smi se disponível; caso contrário assume cpu.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        out = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name\", \"--format=csv,noheader\"],\n",
    "            stderr=subprocess.DEVNULL, stdin=subprocess.DEVNULL, timeout=2\n",
    "        )\n",
    "        names = out.decode(\"utf-8\", errors=\"ignore\").strip().splitlines()\n",
    "        if names and len(names[0].strip()) > 0:\n",
    "            return \"cuda\", names[0].strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"cpu\", \"-\"\n",
    "\n",
    "# =========================\n",
    "# Features\n",
    "# =========================\n",
    "\n",
    "def compute_log_ret(close: pd.Series) -> pd.Series:\n",
    "    return np.log(close / close.shift(1))\n",
    "\n",
    "def prepare_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Reqs: close, high, low\n",
    "    for c in (\"close\", \"high\", \"low\"):\n",
    "        if c not in out.columns:\n",
    "            raise ValueError(f\"VALIDATION_ERROR: coluna obrigatória ausente: {c}\")\n",
    "    out[\"ret1\"] = compute_log_ret(out[\"close\"])\n",
    "    # lags até 10\n",
    "    for lag in range(1, 11):\n",
    "        out[f\"ret1_lag_{lag}\"] = out[\"ret1\"].shift(lag)\n",
    "    # rolling mean/std para 5/10/15 e z-scores de ret1\n",
    "    for W in (5, 10, 15):\n",
    "        out[f\"ret_roll_mean_{W}\"] = out[\"ret1\"].rolling(W).mean()\n",
    "        out[f\"ret_roll_std_{W}\"] = out[\"ret1\"].rolling(W).std()\n",
    "        out[f\"zscore_ret_{W}\"] = (out[\"ret1\"] - out[f\"ret_roll_mean_{W}\"]) / out[f\"ret_roll_std_{W}\"]\n",
    "    # vol20d para farol\n",
    "    out[\"vol20d\"] = out[\"ret1\"].rolling(20).std()\n",
    "    # MA50 e pos_ma50\n",
    "    out[\"ma50\"] = out[\"close\"].rolling(50).mean()\n",
    "    out[\"pos_ma50\"] = ((out[\"close\"] > out[\"ma50\"]).astype(float)).where(out[\"ma50\"].notna(), np.nan)\n",
    "    # range compression (true range normalizado) 5/10\n",
    "    for W in (5, 10):\n",
    "        hi = out[\"high\"].rolling(W).max()\n",
    "        lo = out[\"low\"].rolling(W).min()\n",
    "        mid = out[\"close\"].rolling(W).mean()\n",
    "        out[f\"tr_norm_{W}\"] = (hi - lo) / (mid.replace(0, np.nan))\n",
    "    # forward returns\n",
    "    for h in horizons:\n",
    "        out[f\"ret_fwd_{h}\"] = (out[\"close\"].shift(-h) / out[\"close\"]) - 1.0\n",
    "        out[f\"y_h{h}_bin\"] = (pd.to_numeric(out[f\"ret_fwd_{h}\"], errors=\"coerce\") < 0).astype(\"Int8\")\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Modelos e Calibração\n",
    "# =========================\n",
    "\n",
    "def build_lstm_model(n_features: int, W: int) -> \"Sequential\":\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(W, n_features)))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(lstm_dropout))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def make_calibrator(y_val: np.ndarray, p_val: np.ndarray):\n",
    "    yv = np.asarray(y_val).astype(int)\n",
    "    pv = np.asarray(p_val).astype(float).reshape(-1, 1)\n",
    "    if len(np.unique(yv)) < 2 or len(yv) < 5:\n",
    "        return (lambda x: np.asarray(x, dtype=float)), \"none\"\n",
    "    # Platt\n",
    "    try:\n",
    "        lr = LogisticRegression(max_iter=1000, solver=\"lbfgs\")\n",
    "        lr.fit(pv, yv)\n",
    "        def f(x):\n",
    "            xv = np.asarray(x).astype(float).reshape(-1, 1)\n",
    "            return lr.predict_proba(xv)[:, 1]\n",
    "        return f, \"platt\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Isotonic fallback\n",
    "    try:\n",
    "        iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "        iso.fit(np.asarray(p_val).astype(float), yv)\n",
    "        def g(x):\n",
    "            return iso.predict(np.asarray(x).astype(float))\n",
    "        return g, \"isotonic\"\n",
    "    except Exception:\n",
    "        return (lambda x: np.asarray(x, dtype=float)), \"none\"\n",
    "\n",
    "# =========================\n",
    "# Métricas, sanity e seleção de threshold\n",
    "# =========================\n",
    "\n",
    "def cm_metrics(cm: np.ndarray) -> Tuple[float, float, float, float]:\n",
    "    TP = float(cm[0,0]); FP = float(cm[0,1]); FN = float(cm[1,0]); TN = float(cm[1,1])\n",
    "    prec = TP / max(1.0, (TP + FP))\n",
    "    rec = TP / max(1.0, (TP + FN))\n",
    "    acc = (TP + TN) / max(1.0, (TP + FP + FN + TN))\n",
    "    f1 = (2*prec*rec) / max(1e-12, (prec + rec)) if (prec + rec) > 0 else 0.0\n",
    "    return prec, rec, f1, acc\n",
    "\n",
    "def binary_eval(y_true: np.ndarray, y_score: np.ndarray, thr: float) -> Dict[str, Any]:\n",
    "    y_pred = (y_score >= thr).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "    prec_lib = float(precision_score(y_true, y_pred, zero_division=0))\n",
    "    rec_lib = float(recall_score(y_true, y_pred, zero_division=0))\n",
    "    f1_lib = float(f1_score(y_true, y_pred, zero_division=0))\n",
    "    acc_lib = float(accuracy_score(y_true, y_pred))\n",
    "    prec_cm, rec_cm, f1_cm, acc_cm = cm_metrics(cm)\n",
    "    sanity_ok = (abs(prec_lib - prec_cm) < 1e-6) and (abs(rec_lib - rec_cm) < 1e-6)\n",
    "    cover_rate = float((y_pred == 1).mean()) if len(y_pred) else 0.0\n",
    "    cover_count = int((y_pred == 1).sum()) if len(y_pred) else 0\n",
    "    return dict(\n",
    "        cm=cm, precision=prec_lib, recall=rec_lib, f1=f1_lib, acc=acc_lib,\n",
    "        precision_cm=prec_cm, recall_cm=rec_cm, f1_cm=f1_cm, acc_cm=acc_cm,\n",
    "        sanity_ok=sanity_ok, coverage_rate=cover_rate, coverage_count=cover_count\n",
    "    )\n",
    "\n",
    "def select_threshold_val(yv: np.ndarray, pv: np.ndarray, floor: float) -> Tuple[Optional[float], Dict[str, Any]]:\n",
    "    best = None\n",
    "    best_metrics = None\n",
    "    for thr in threshold_grid:\n",
    "        m = binary_eval(yv, pv, thr)\n",
    "        if (m[\"precision\"] >= floor and m[\"coverage_count\"] >= N_min_preds_val and m[\"coverage_rate\"] <= FLOOD_GUARD_MAX):\n",
    "            if best is None:\n",
    "                best, best_metrics = thr, m\n",
    "            else:\n",
    "                # Priorizar recall, desempate F1, depois ACC\n",
    "                cur = best_metrics\n",
    "                if (m[\"recall\"] > cur[\"recall\"]) or \\\n",
    "                   (m[\"recall\"] == cur[\"recall\"] and m[\"f1\"] > cur[\"f1\"]) or \\\n",
    "                   (m[\"recall\"] == cur[\"recall\"] and m[\"f1\"] == cur[\"f1\"] and m[\"acc\"] > cur[\"acc\"]):\n",
    "                    best, best_metrics = thr, m\n",
    "    return best, (best_metrics or {})\n",
    "\n",
    "# =========================\n",
    "# Execução principal\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    print(f\"[{now_ts()}] Início — CAI vs NÃO CAI — V1.2/V1.2.1 (Delta XGBoost 3.0.5) | Seeds: numpy={SEED_NUMPY}, tf={SEED_TF}, xgb={SEED_PY}\")\n",
    "\n",
    "    if _missing:\n",
    "        print(f\"CHECKLIST_FAILURE: dependências ausentes -> {', '.join(_missing)}\")\n",
    "        return\n",
    "\n",
    "    # TF: silenciar GPU e forçar CPU apenas para LSTM (sem afetar XGBoost)\n",
    "    try:\n",
    "        tf.config.set_visible_devices([], 'GPU')\n",
    "        TF_DEV_NOTE = \"TF device=CPU (fallback)\"\n",
    "    except Exception:\n",
    "        TF_DEV_NOTE = \"TF device=CPU (noop)\"\n",
    "\n",
    "    # XGBoost device detection\n",
    "    xgb_device, xgb_gpu_name = detect_xgb_device()\n",
    "    if xgb_device != \"cuda\":\n",
    "        xgb_device = \"cpu\"\n",
    "        xgb_gpu_name = \"-\"\n",
    "\n",
    "    # SSOT GOLD\n",
    "    if not os.path.exists(GOLD_PATH):\n",
    "        print(\"CHECKLIST_FAILURE: GOLD ausente no SSOT.\")\n",
    "        return\n",
    "    try:\n",
    "        df_raw = pd.read_parquet(GOLD_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"VALIDATION_ERROR: falha ao ler GOLD: {e}\")\n",
    "        return\n",
    "    df = ensure_datetime(df_raw)\n",
    "    df = prepare_features(df)\n",
    "\n",
    "    # PROVA SSOT\n",
    "    proof = summarize_df(df)\n",
    "    print(\"\\n[PROVA SSOT]\")\n",
    "    print(f\"- Caminho: {GOLD_PATH} (tier=GOLD)\")\n",
    "    print(f\"- Linhas: {proof['row_count']} | date_min: {proof['date_min']} | date_max: {proof['date_max']}\")\n",
    "    print(f\"- Colunas (amostra): {proof['columns']}\")\n",
    "\n",
    "    # Splits\n",
    "    splits = build_walk_forward_splits(df)\n",
    "    print(\"\\n[WALK-FORWARD — Folds]\")\n",
    "    for i, s in enumerate(splits, 1):\n",
    "        print(f\"Fold {i:02d} | train[{str(s['train_start'].date())} → {str(s['train_end'].date())}] | val[{str(s['val_start'].date())} → {str(s['val_end'].date())}] | test[{str(s['test_start'].date())} → {str(s['test_end'].date())}]\")\n",
    "\n",
    "    # Estruturas\n",
    "    preds_val: Dict[Tuple[str,int,int,int], Tuple[np.ndarray, np.ndarray, Dict[str, Any]]] = {}\n",
    "    preds_tst: Dict[Tuple[str,int,int,int], Tuple[np.ndarray, np.ndarray, np.ndarray]] = {}\n",
    "    per_fold_info: Dict[Tuple[str,int,int,int], Dict[str, Any]] = {}\n",
    "    error_counts: Dict[str, int] = {}\n",
    "    abort_xgb: bool = False\n",
    "    abort_lstm: bool = False\n",
    "    xgb_fit_kw_error: bool = False  # flag para pergunta final de múltiplos XGB\n",
    "\n",
    "    # Loop h, W, fold\n",
    "    for h in horizons:\n",
    "        ycol = f\"y_h{h}_bin\"\n",
    "        for W in windows:\n",
    "            # lista de features base para XGB\n",
    "            feat_cols = [\n",
    "                *[f\"ret1_lag_{lag}\" for lag in range(1, 11)],\n",
    "                *[f\"ret_roll_mean_{k}\" for k in (5,10,15)],\n",
    "                *[f\"ret_roll_std_{k}\" for k in (5,10,15)],\n",
    "                *[f\"zscore_ret_{k}\" for k in (5,10,15)],\n",
    "            ]\n",
    "            # faróis e compressores\n",
    "            for fi, s in enumerate(splits, 1):\n",
    "                tr = subset(df, s[\"train_start\"], s[\"train_end\"])  # inclui __date__\n",
    "                va = subset(df, s[\"val_start\"], s[\"val_end\"])\n",
    "                te = subset(df, s[\"test_start\"], s[\"test_end\"])\n",
    "                # construir faróis a partir do treino\n",
    "                vol_med = float(tr[\"vol20d\"].median()) if len(tr) else np.nan\n",
    "                tr[\"vol20d_high\"] = (tr[\"vol20d\"] >= vol_med).astype(int)\n",
    "                va[\"vol20d_high\"] = (va[\"vol20d\"] >= vol_med).astype(int)\n",
    "                te[\"vol20d_high\"] = (te[\"vol20d\"] >= vol_med).astype(int)\n",
    "                # range compression por mediana do treino\n",
    "                for k in (5, 10):\n",
    "                    med = float(tr[f\"tr_norm_{k}\"].median()) if len(tr) else np.nan\n",
    "                    tr[f\"range_compression_{k}\"] = (tr[f\"tr_norm_{k}\"] <= med).astype(int)\n",
    "                    va[f\"range_compression_{k}\"] = (va[f\"tr_norm_{k}\"] <= med).astype(int)\n",
    "                    te[f\"range_compression_{k}\"] = (te[f\"tr_norm_{k}\"] <= med).astype(int)\n",
    "                feat_all = feat_cols + [\"vol20d_high\",\"pos_ma50\",\"range_compression_5\",\"range_compression_10\"]\n",
    "                # drop NaNs (janelas)\n",
    "                trc = tr[[\"__date__\", ycol] + feat_all].dropna().copy()\n",
    "                vac = va[[\"__date__\", ycol] + feat_all].dropna().copy()\n",
    "                tec = te[[\"__date__\", ycol] + feat_all].dropna().copy()\n",
    "                if trc.empty or trc[ycol].isna().all():\n",
    "                    msg = f\"train_empty:h={h},W={W},fold={fi}\"\n",
    "                    error_counts[msg] = error_counts.get(msg, 0) + 1\n",
    "                    continue\n",
    "                # padronização fit no treino\n",
    "                scaler = StandardScaler().fit(trc[feat_all].values)\n",
    "                Xtr = scaler.transform(trc[feat_all].values); ytr = trc[ycol].astype(int).values\n",
    "                Xva = scaler.transform(vac[feat_all].values) if len(vac) else np.empty((0,len(feat_all)))\n",
    "                yva = vac[ycol].astype(int).values if len(vac) else np.empty((0,), dtype=int)\n",
    "                Xte = scaler.transform(tec[feat_all].values) if len(tec) else np.empty((0,len(feat_all)))\n",
    "                yte = tec[ycol].astype(int).values if len(tec) else np.empty((0,), dtype=int)\n",
    "                dates_te = tec[\"__date__\"].values.astype(\"datetime64[ns]\") if len(tec) else np.array([], dtype=\"datetime64[ns]\")\n",
    "\n",
    "                # scale_pos_weight\n",
    "                pos = max(1, int((ytr == 1).sum()))\n",
    "                neg = int((ytr == 0).sum())\n",
    "                spw = float(neg / pos) if (pos + neg) > 0 else 1.0\n",
    "\n",
    "                # ===== XGBoost (sklearn API forçada; Delta 3.0.5) =====\n",
    "                if not abort_xgb:\n",
    "                    try:\n",
    "                        xgb_params = dict(\n",
    "                            max_depth=5,\n",
    "                            learning_rate=0.05,\n",
    "                            n_estimators=xgb_n_estimators,\n",
    "                            subsample=0.9,\n",
    "                            colsample_bytree=0.9,\n",
    "                            objective=\"binary:logistic\",\n",
    "                            eval_metric=xgb_eval_metric,\n",
    "                            tree_method=\"hist\",\n",
    "                            device=xgb_device,\n",
    "                            random_state=SEED_PY,\n",
    "                            n_jobs=max(1, (os.cpu_count() or 2) - 1),\n",
    "                        )\n",
    "                        clf = XGBClassifier(**xgb_params, scale_pos_weight=spw)\n",
    "                        # early_stopping_rounds no fit; NUNCA usar callbacks\n",
    "                        clf.fit(\n",
    "                            Xtr, ytr,\n",
    "                            eval_set=[(Xtr, ytr), (Xva, yva)],\n",
    "                            early_stopping_rounds=xgb_early_stopping_rounds,\n",
    "                            verbose=False,\n",
    "                        )\n",
    "                        best_iter = getattr(clf, \"best_iteration\", None)\n",
    "                        if best_iter is not None:\n",
    "                            p_val_raw = clf.predict_proba(Xva, iteration_range=(0, int(best_iter)+1))[:,1] if len(Xva) else np.array([])\n",
    "                            p_tst_raw = clf.predict_proba(Xte, iteration_range=(0, int(best_iter)+1))[:,1] if len(Xte) else np.array([])\n",
    "                        else:\n",
    "                            p_val_raw = clf.predict_proba(Xva)[:,1] if len(Xva) else np.array([])\n",
    "                            p_tst_raw = clf.predict_proba(Xte)[:,1] if len(Xte) else np.array([])\n",
    "                        # calibração em VAL\n",
    "                        try:\n",
    "                            cal_fn, cal_m = make_calibrator(yva, p_val_raw)\n",
    "                        except Exception as e:\n",
    "                            msg = f\"CALIB_ERR:{type(e).__name__}:{str(e).strip()}\"\n",
    "                            error_counts[msg] = error_counts.get(msg, 0) + 1\n",
    "                            if error_counts[msg] >= 3:\n",
    "                                error_counts[\"repeat_error_stop:calibration\"] = error_counts.get(\"repeat_error_stop:calibration\", 0) + 1\n",
    "                            cal_fn, cal_m = (lambda x: np.asarray(x, dtype=float)), \"none\"\n",
    "                        p_val = cal_fn(p_val_raw)\n",
    "                        p_tst = cal_fn(p_tst_raw)\n",
    "                        preds_val[(\"XGB\", W, h, fi)] = (yva, np.asarray(p_val, dtype=float), {\"calibration\": cal_m, \"spw\": spw, \"device\": xgb_device})\n",
    "                        preds_tst[(\"XGB\", W, h, fi)] = (yte, np.asarray(p_tst, dtype=float), dates_te)\n",
    "                        per_fold_info[(\"XGB\", W, h, fi)] = {\"scaler\": \"standard\", \"best_iteration\": (int(best_iter) if best_iter is not None else None)}\n",
    "                    except TypeError as e:\n",
    "                        msg_txt = str(e).lower()\n",
    "                        # erros esperados de ambiente conflitado\n",
    "                        if (\"early_stopping_rounds\" in msg_txt and \"unexpected\" in msg_txt) or (\"callbacks\" in msg_txt and \"unexpected\" in msg_txt):\n",
    "                            error_counts[\"xgb_sklearn_fit\"] = error_counts.get(\"xgb_sklearn_fit\", 0) + 1\n",
    "                            xgb_fit_kw_error = True\n",
    "                            if error_counts[\"xgb_sklearn_fit\"] >= 3:\n",
    "                                error_counts[\"repeat_error_stop:xgb_sklearn_fit\"] = 3\n",
    "                                abort_xgb = True\n",
    "                        else:\n",
    "                            msg = f\"XGB_FIT_TYPE_ERR:{str(e).strip()}\"\n",
    "                            error_counts[msg] = error_counts.get(msg, 0) + 1\n",
    "                            if error_counts[msg] >= 3:\n",
    "                                abort_xgb = True\n",
    "                    except Exception as e:\n",
    "                        msg = f\"XGB_ERR:{type(e).__name__}:{str(e).strip()}\"\n",
    "                        error_counts[msg] = error_counts.get(msg, 0) + 1\n",
    "                        if error_counts[msg] >= 3:\n",
    "                            error_counts[\"repeat_error_stop:xgb_train\"] = 3\n",
    "                            abort_xgb = True\n",
    "\n",
    "                # ===== LSTM (CPU, silencioso) =====\n",
    "                if not abort_lstm:\n",
    "                    try:\n",
    "                        # montar painel para LSTM (ret1 + roll mean/std W)\n",
    "                        lstm_df = df[[\"__date__\",\"ret1\", f\"ret_roll_mean_{W}\", f\"ret_roll_std_{W}\", ycol]].dropna().copy()\n",
    "                        trl = subset(lstm_df, s[\"train_start\"], s[\"train_end\"])\n",
    "                        val = subset(lstm_df, s[\"val_start\"], s[\"val_end\"])\n",
    "                        tes = subset(lstm_df, s[\"test_start\"], s[\"test_end\"])\n",
    "                        if len(trl) >= (W + 5) and len(val) >= (W + 5) and len(tes) >= (W + 5):\n",
    "                            feat_l = [\"ret1\", f\"ret_roll_mean_{W}\", f\"ret_roll_std_{W}\"]\n",
    "                            sc = StandardScaler().fit(trl[feat_l].values)\n",
    "                            def to_seq(b: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "                                b2 = b.copy(); b2[feat_l] = sc.transform(b2[feat_l].values)\n",
    "                                X, y = [], []\n",
    "                                V = b2[feat_l].values; yv2 = b2[ycol].astype(int).values\n",
    "                                for i in range(W, len(b2)):\n",
    "                                    X.append(V[i-W:i,:]); y.append(yv2[i])\n",
    "                                dates = b2[\"__date__\"].values[W:].astype(\"datetime64[ns]\")\n",
    "                                return (np.stack(X,0) if X else np.empty((0,W,len(feat_l)))), (np.array(y, int) if y else np.empty((0,), int)), dates\n",
    "                            Xtr, ytr_l, _ = to_seq(trl)\n",
    "                            Xva, yva_l, _ = to_seq(val)\n",
    "                            Xte, yte_l, dte_l = to_seq(tes)\n",
    "                            if Xtr.shape[0] and Xva.shape[0] and Xte.shape[0]:\n",
    "                                tf.keras.backend.clear_session()\n",
    "                                model = build_lstm_model(n_features=len(feat_l), W=W)\n",
    "                                es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=lstm_patience, restore_best_weights=True, verbose=0)\n",
    "                                model.fit(Xtr, ytr_l, validation_data=(Xva, yva_l), epochs=lstm_epochs, batch_size=lstm_batch_size, callbacks=[es], verbose=0)\n",
    "                                p_val_raw = model.predict(Xva, verbose=0, batch_size=lstm_batch_size).reshape(-1)\n",
    "                                p_tst_raw = model.predict(Xte, verbose=0, batch_size=lstm_batch_size).reshape(-1)\n",
    "                                try:\n",
    "                                    cal_fn, cal_m = make_calibrator(yva_l, p_val_raw)\n",
    "                                except Exception as e:\n",
    "                                    msg = f\"CALIB_ERR:{type(e).__name__}:{str(e).strip()}\"\n",
    "                                    error_counts[msg] = error_counts.get(msg, 0) + 1\n",
    "                                    if error_counts[msg] >= 3:\n",
    "                                        error_counts[\"repeat_error_stop:calibration\"] = error_counts.get(\"repeat_error_stop:calibration\", 0) + 1\n",
    "                                    cal_fn, cal_m = (lambda x: np.asarray(x, dtype=float)), \"none\"\n",
    "                                preds_val[(\"LSTM\", W, h, fi)] = (yva_l, np.asarray(cal_fn(p_val_raw), dtype=float), {\"calibration\": cal_m})\n",
    "                                preds_tst[(\"LSTM\", W, h, fi)] = (yte_l, np.asarray(cal_fn(p_tst_raw), dtype=float), dte_l)\n",
    "                                per_fold_info[(\"LSTM\", W, h, fi)] = {\"scaler\": \"standard\"}\n",
    "                    except Exception as e:\n",
    "                        msg = f\"LSTM_ERR:{type(e).__name__}:{str(e).strip()}\"\n",
    "                        error_counts[msg] = error_counts.get(msg, 0) + 1\n",
    "                        if error_counts[msg] >= 3:\n",
    "                            error_counts[\"repeat_error_stop:lstm\"] = 3\n",
    "                            abort_lstm = True\n",
    "\n",
    "    # Relatar avisos de repetição e perguntas\n",
    "    repeat_msgs = [f\"{k} (x{v})\" for k,v in error_counts.items() if v >= 3 and k.startswith(\"repeat_error_stop\")]\n",
    "    if repeat_msgs:\n",
    "        print(\"\\n[ERROS REPETIDOS — intervenção requerida]\")\n",
    "        for m in repeat_msgs:\n",
    "            print(f\"- {m}\")\n",
    "\n",
    "    # Pisos D+3/D+5 via VAL com N_min e clip\n",
    "    for h in [3, 5]:\n",
    "        best_prec = 0.0\n",
    "        for key, (yv, pv, meta) in preds_val.items():\n",
    "            _, _, hh, _ = key\n",
    "            if hh != h or len(yv) == 0:\n",
    "                continue\n",
    "            # varrer grade e pegar melhor precisão respeitando N_min\n",
    "            for thr in threshold_grid:\n",
    "                m = binary_eval(yv, pv, thr)\n",
    "                if m[\"coverage_count\"] >= N_min_preds_val:\n",
    "                    if m[\"precision\"] > best_prec:\n",
    "                        best_prec = m[\"precision\"]\n",
    "        tag = f\"D+{h}\"\n",
    "        if precision_floor.get(tag) is None:\n",
    "            pf = round(best_prec, 2)\n",
    "            precision_floor[tag] = float(np.clip(pf, 0.70, 0.85))\n",
    "    print(\"\\n[PISOS DE PRECISÃO — usados]\")\n",
    "    print(f\"- D+1: {precision_floor['D+1']:.2f} (fixo)\")\n",
    "    print(f\"- D+3: {precision_floor['D+3']:.2f}\")\n",
    "    print(f\"- D+5: {precision_floor['D+5']:.2f}\")\n",
    "    print(f\"- N_min_preds_val: {N_min_preds_val}; flood_guard ≤ {int(FLOOD_GUARD_MAX*100)}%\")\n",
    "\n",
    "    # Seleção de thresholds por fold (VAL) e avaliação no TESTE\n",
    "    fold_rows: List[Dict[str, Any]] = []\n",
    "    for key in sorted(preds_val.keys()):\n",
    "        model, W, h, fi = key\n",
    "        yv, pv, meta = preds_val[key]\n",
    "        yt, pt, dt = preds_tst.get(key, (np.array([]), np.array([]), np.array([])))\n",
    "        if len(yv) == 0 or len(yt) == 0:\n",
    "            continue\n",
    "        floor = precision_floor[f\"D+{h}\"] or 0.0\n",
    "        thr, mval = select_threshold_val(yv, pv, floor)\n",
    "        reasons = []\n",
    "        if thr is None:\n",
    "            reasons.append(\"piso/Nmin/flood_val\")\n",
    "            # Inelegível — sem relax; ainda calculamos métricas de teste com um thr neutro para diagnóstico\n",
    "            thr_test = 0.5\n",
    "            mtest = binary_eval(yt, pt, thr_test)\n",
    "            sanity_ok = mtest.get(\"sanity_ok\", True)\n",
    "            if not sanity_ok:\n",
    "                reasons.append(\"metric_swap_detected\")\n",
    "            cov_ok = (mtest[\"coverage_rate\"] >= coverage_min_rate) or (mtest[\"coverage_count\"] >= coverage_min_count)\n",
    "            flood_ok = False  # sem threshold válido em VAL → considerar falha de flood guard operacional\n",
    "            piso_ok = (mtest[\"precision\"] >= floor)\n",
    "            fold_eligible = False\n",
    "            fold_rows.append(dict(\n",
    "                model=model, window=W, horizon=h, fold=fi, thr=float(thr_test),\n",
    "                prec_val=float(\"nan\"), rec_val=float(\"nan\"), rate_val=float(\"nan\"),\n",
    "                prec_test=mtest[\"precision\"], rec_test=mtest[\"recall\"], f1_test=mtest[\"f1\"], acc_test=mtest[\"acc\"],\n",
    "                rate_test=mtest[\"coverage_rate\"], n_pred_test=mtest[\"coverage_count\"],\n",
    "                cm_TP=int(mtest[\"cm\"][0,0]), cm_FP=int(mtest[\"cm\"][0,1]), cm_FN=int(mtest[\"cm\"][1,0]), cm_TN=int(mtest[\"cm\"][1,1]),\n",
    "                fold_eligible=fold_eligible, reasons=\",\".join(reasons),\n",
    "            ))\n",
    "            continue\n",
    "\n",
    "        mtest = binary_eval(yt, pt, thr)\n",
    "        # sanity assert (VAL e TESTE)\n",
    "        sanity_ok = mval.get(\"sanity_ok\", True) and mtest.get(\"sanity_ok\", True)\n",
    "        if not sanity_ok:\n",
    "            reasons.append(\"metric_swap_detected\")\n",
    "        # cobertura mínima & flood guard em TESTE e VAL\n",
    "        cov_ok = (mtest[\"coverage_rate\"] >= coverage_min_rate) or (mtest[\"coverage_count\"] >= coverage_min_count)\n",
    "        flood_ok = (mval.get(\"coverage_rate\", 0.0) <= FLOOD_GUARD_MAX) and (mtest[\"coverage_rate\"] <= FLOOD_GUARD_MAX)\n",
    "        piso_ok = (mtest[\"precision\"] >= floor)\n",
    "        fold_eligible = bool(piso_ok and cov_ok and flood_ok and sanity_ok)\n",
    "        fold_rows.append(dict(\n",
    "            model=model, window=W, horizon=h, fold=fi, thr=float(thr),\n",
    "            prec_val=mval.get(\"precision\", np.nan), rec_val=mval.get(\"recall\", np.nan), rate_val=mval.get(\"coverage_rate\", np.nan),\n",
    "            prec_test=mtest[\"precision\"], rec_test=mtest[\"recall\"], f1_test=mtest[\"f1\"], acc_test=mtest[\"acc\"],\n",
    "            rate_test=mtest[\"coverage_rate\"], n_pred_test=mtest[\"coverage_count\"],\n",
    "            cm_TP=int(mtest[\"cm\"][0,0]), cm_FP=int(mtest[\"cm\"][0,1]), cm_FN=int(mtest[\"cm\"][1,0]), cm_TN=int(mtest[\"cm\"][1,1]),\n",
    "            fold_eligible=fold_eligible, reasons=\",\".join(reasons),\n",
    "        ))\n",
    "\n",
    "    if not fold_rows:\n",
    "        print(\"CHECKLIST_FAILURE: nenhuma combinação produziu resultados.\")\n",
    "        return\n",
    "\n",
    "    folds_df = pd.DataFrame(fold_rows)\n",
    "\n",
    "    # Agregar por combinação/horizonte somando CMs e recomputando métricas\n",
    "    agg_rows: List[Dict[str, Any]] = []\n",
    "    thr_medians: Dict[Tuple[str,int,int], float] = {}\n",
    "    for (h, m, W), grp in folds_df.groupby([\"horizon\",\"model\",\"window\"], as_index=False):\n",
    "        TP = int(grp[\"cm_TP\"].sum()); FP = int(grp[\"cm_FP\"].sum()); FN = int(grp[\"cm_FN\"].sum()); TN = int(grp[\"cm_TN\"].sum())\n",
    "        prec, rec, f1, acc = cm_metrics(np.array([[TP, FP],[FN, TN]], dtype=float))\n",
    "        n_pred = int(grp[\"n_pred_test\"].sum()); n_total = int((TP+FP+FN+TN))\n",
    "        rate = float(n_pred / max(1, n_total)) if n_total > 0 else 0.0\n",
    "        floor = precision_floor[f\"D+{h}\"] or 0.0\n",
    "        # mediana de thresholds apenas entre folds elegíveis\n",
    "        elig_thr = grp.loc[grp[\"fold_eligible\"]==True, \"thr\"].values\n",
    "        thr_med = float(np.median(elig_thr)) if len(elig_thr) else float(\"nan\")\n",
    "        thr_medians[(m, W, h)] = thr_med\n",
    "        # checagens\n",
    "        cov_ok = (rate >= coverage_min_rate) or (n_pred >= coverage_min_count)\n",
    "        # média de VAL flood guard entre folds do par (modelo, W, h)\n",
    "        rate_val_mean = float(grp[\"rate_val\"].mean()) if \"rate_val\" in grp.columns and len(grp) else float(\"nan\")\n",
    "        flood_ok = (rate_val_mean <= FLOOD_GUARD_MAX if np.isfinite(rate_val_mean) else False) and (rate <= FLOOD_GUARD_MAX)\n",
    "        sanity_ok = not (grp[\"reasons\"].str.contains(\"metric_swap_detected\").fillna(False)).any()\n",
    "        piso_ok = (prec >= floor)\n",
    "        eligible = bool(piso_ok and cov_ok and flood_ok and sanity_ok)\n",
    "        reason_parts = []\n",
    "        if not piso_ok: reason_parts.append(\"piso\")\n",
    "        if not cov_ok: reason_parts.append(\"cobertura\")\n",
    "        if not flood_ok: reason_parts.append(\"inundacao\")\n",
    "        if not sanity_ok: reason_parts.append(\"sanity\")\n",
    "        agg_rows.append(dict(horizon=h, model=m, window=W, TP=TP, FP=FP, FN=FN, TN=TN,\n",
    "                             precisao_CAI=prec, recall_CAI=rec, F1_CAI=f1, acc=acc,\n",
    "                             pred_CAI_rate=rate, num_pred_CAI=n_pred,\n",
    "                             eligible=eligible, reason=\",\".join(reason_parts), folds=int(grp[\"fold\"].nunique()),\n",
    "                             threshold_median=thr_med, rate_val_mean=rate_val_mean))\n",
    "    agg_df = pd.DataFrame(agg_rows).sort_values([\"horizon\",\"model\",\"window\"]) if agg_rows else pd.DataFrame()\n",
    "\n",
    "    # Top-3 e melhores por horizonte\n",
    "    best_by_h: Dict[int, Dict[str, Any]] = {}\n",
    "    for h in horizons:\n",
    "        sub = agg_df[agg_df[\"horizon\"]==h].copy()\n",
    "        elig = sub[sub[\"eligible\"] == True].copy()\n",
    "        print(f\"\\nRESUMO — D+{h} (TESTE agregado) — modelo × janela\")\n",
    "        if sub.empty:\n",
    "            print(\"–\")\n",
    "        else:\n",
    "            print(sub[[\"model\",\"window\",\"precisao_CAI\",\"recall_CAI\",\"F1_CAI\",\"acc\",\"pred_CAI_rate\",\"eligible\",\"reason\",\"folds\",\"threshold_median\"]].to_string(index=False))\n",
    "        print(f\"\\nTOP-3 — D+{h} (TESTE)\")\n",
    "        if elig.empty:\n",
    "            print(\"–\")\n",
    "        else:\n",
    "            elig_sorted = elig.sort_values([\"recall_CAI\",\"F1_CAI\",\"acc\"], ascending=[False,False,False])\n",
    "            top3 = elig_sorted.head(3).reset_index(drop=True)\n",
    "            print(top3[[\"model\",\"window\",\"precisao_CAI\",\"recall_CAI\",\"F1_CAI\",\"acc\",\"pred_CAI_rate\",\"folds\",\"threshold_median\"]].to_string(index=False))\n",
    "            best = elig_sorted.iloc[0]\n",
    "            best_by_h[h] = best.to_dict()\n",
    "            # Matriz de confusão agregada do melhor\n",
    "            print(f\"\\nMATRIZ DE CONFUSÃO — melhor combinação D+{h} (modelo={best['model']}, janela={int(best['window'])}) [CAI=1, N_CAI=0]\")\n",
    "            header = [\"\", \"pred_CAI\", \"pred_NAO_CAI\"]\n",
    "            print(\"{:<14s}{:>10s}{:>14s}\".format(*header))\n",
    "            print(\"{:<14s}{:>10d}{:>14d}\".format(\"true_CAI\", int(best.get(\"TP\",0)), int(best.get(\"FP\",0))))\n",
    "            print(\"{:<14s}{:>10d}{:>14d}\".format(\"true_NAO_CAI\", int(best.get(\"FN\",0)), int(best.get(\"TN\",0))))\n",
    "\n",
    "    # Threshold operacional (medianas)\n",
    "    threshold_operacional: Dict[int, float] = {}\n",
    "    print(\"\\nTHRESHOLD OPERACIONAL (mediana entre folds elegíveis)\")\n",
    "    for h in horizons:\n",
    "        val = float(best_by_h[h][\"threshold_median\"]) if h in best_by_h and np.isfinite(best_by_h[h][\"threshold_median\"]) else float(\"nan\")\n",
    "        threshold_operacional[h] = val\n",
    "        print(f\"- D+{h}: {val if np.isfinite(val) else '–'}\")\n",
    "\n",
    "    # Sequência final — somente se houver elegíveis\n",
    "    final_seq = []\n",
    "    all_operable = all(h in best_by_h for h in horizons)\n",
    "    if all_operable:\n",
    "        try:\n",
    "            last_fold = max(int(k[3]) for k in preds_tst.keys()) if preds_tst else None\n",
    "        except Exception:\n",
    "            last_fold = None\n",
    "        if last_fold is not None:\n",
    "            for h in horizons:\n",
    "                b = best_by_h[h]\n",
    "                m, W = str(b[\"model\"]), int(b[\"window\"])\n",
    "                thr_med = float(b.get(\"threshold_median\", float(\"nan\")))\n",
    "                yt, pt, dt = preds_tst.get((m, W, h, last_fold), (np.array([]), np.array([]), np.array([])))\n",
    "                if len(pt) == 0 or not np.isfinite(thr_med):\n",
    "                    final_seq.append(\"—\")\n",
    "                else:\n",
    "                    yhat = (pt >= thr_med).astype(int)\n",
    "                    final_seq.append(\"CAI\" if int(yhat[-1]) == 1 else \"NÃO CAI\")\n",
    "        else:\n",
    "            all_operable = False\n",
    "    print(\"\\nSEQUÊNCIA FINAL (último bloco de TESTE):\")\n",
    "    print(f\"- (D+1, D+3, D+5) = {', '.join(final_seq) if all_operable and final_seq else '—'}\")\n",
    "\n",
    "    # Baselines — TESTE (médias por horizonte)\n",
    "    baseline_rows = []\n",
    "    for h in horizons:\n",
    "        seen = set()\n",
    "        for key, (yt, pt, dt) in preds_tst.items():\n",
    "            m, W, hh, fi = key\n",
    "            if hh != h or fi in seen or len(yt) == 0:\n",
    "                continue\n",
    "            seen.add(fi)\n",
    "            n = len(yt)\n",
    "            # Sempre NÃO CAI\n",
    "            pred0 = np.zeros(n, dtype=int)\n",
    "            cm0 = confusion_matrix(yt, pred0, labels=[1,0])\n",
    "            p0, r0, f10, a0 = cm_metrics(cm0)\n",
    "            baseline_rows.append(dict(horizon=h, baseline=\"Sempre_NAO_CAI\", precision=p0, f1=f10))\n",
    "            # ProporcaoTreino>0.5 via VAL proxy\n",
    "            yv = None\n",
    "            for k2, (yvv, pvv, meta) in preds_val.items():\n",
    "                mm, WW, hhh, fii = k2\n",
    "                if hhh == h and fii == fi and len(yvv) > 0:\n",
    "                    yv = yvv\n",
    "                    break\n",
    "            if yv is not None:\n",
    "                pred1 = np.ones(n, dtype=int) if float((yv == 1).mean()) > 0.5 else np.zeros(n, dtype=int)\n",
    "                cm1 = confusion_matrix(yt, pred1, labels=[1,0])\n",
    "                p1, r1, f11, a1 = cm_metrics(cm1)\n",
    "                baseline_rows.append(dict(horizon=h, baseline=\"ProporcaoTreino>0.5\", precision=p1, f1=f11))\n",
    "            # Sinal de ontem (ret1[t-1] < 0 -> CAI)\n",
    "            ret1_map = pd.Series(df.set_index(\"__date__\")[\"ret1\"])\n",
    "            pred2 = []\n",
    "            for d in dt:\n",
    "                prev = pd.to_datetime(d) - pd.Timedelta(days=1)\n",
    "                v = ret1_map.get(prev, np.nan)\n",
    "                pred2.append(1 if (pd.notna(v) and v < 0) else 0)\n",
    "            pred2 = np.array(pred2, int)\n",
    "            cm2 = confusion_matrix(yt, pred2, labels=[1,0])\n",
    "            p2, r2, f12, a2 = cm_metrics(cm2)\n",
    "            baseline_rows.append(dict(horizon=h, baseline=\"SinalOntem\", precision=p2, f1=f12))\n",
    "            # Momentum_3d (soma log ret últimos 3 < 0)\n",
    "            mom3 = df[\"ret1\"].rolling(3).sum().shift(1)\n",
    "            mom_map = pd.Series(mom3.values, index=df[\"__date__\"])\n",
    "            pred3 = [1 if (pd.notna(mom_map.get(pd.to_datetime(d), np.nan)) and mom_map.get(pd.to_datetime(d)) < 0) else 0 for d in dt]\n",
    "            pred3 = np.array(pred3, int)\n",
    "            cm3 = confusion_matrix(yt, pred3, labels=[1,0])\n",
    "            p3, r3, f13, a3 = cm_metrics(cm3)\n",
    "            baseline_rows.append(dict(horizon=h, baseline=\"Momentum_3d\", precision=p3, f1=f13))\n",
    "    base_df = pd.DataFrame(baseline_rows)\n",
    "\n",
    "    # Painel V1.2.1 — Uma página\n",
    "    print(\"\\n[Painel — Checklist Operacional (V1.2.1)]\")\n",
    "    lines = []\n",
    "\n",
    "    # Bloco de Diagnóstico (3 linhas; antes de HORIZONTE D+1)\n",
    "    py_ver = f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\"\n",
    "    device_line = f\"cuda({xgb_gpu_name})\" if xgb_device == \"cuda\" and xgb_gpu_name != \"-\" else \"cpu\"\n",
    "    lines.append(f\"Python: {py_ver} | xgboost: {XGB_VERSION} | device: {device_line}\")\n",
    "    lines.append(f\"SSOT: {GOLD_PATH} | Folds: {max_folds} | Dry-run: {dry_run}\")\n",
    "    lines.append(f\"Seeds: np={SEED_NUMPY} | tf={SEED_TF} | xgb={SEED_PY}\")\n",
    "\n",
    "    apto_map: Dict[int, str] = {}\n",
    "    for h in horizons:\n",
    "        lines.append(f\"## HORIZONTE D+{h}\")\n",
    "        floor = precision_floor[f\"D+{h}\"] or 0.0\n",
    "        thr_op = best_by_h[h][\"threshold_median\"] if h in best_by_h else \"—\"\n",
    "        lines.append(f\"Piso de Precisão(CAI): {floor:.2f}\")\n",
    "        lines.append(f\"Threshold Operacional (mediana): {thr_op if isinstance(thr_op, str) or (isinstance(thr_op,float) and not np.isfinite(thr_op)) else f'{thr_op:.2f}'}\\n\")\n",
    "        if h in best_by_h:\n",
    "            b = best_by_h[h]\n",
    "            lines.append(f\"Vencedor: {b['model']} — janela={int(b['window'])}\")\n",
    "            lines.append(f\"Precisão(CAI): {b['precisao_CAI']:.2f}   Recall(CAI): {b['recall_CAI']:.2f}   F1(CAI): {b['F1_CAI']:.2f}   ACC: {b['acc']:.2f}\")\n",
    "            # taxas: usar médias aproximadas a partir de agg (VAL média dos folds) e TESTE do agregado\n",
    "            rate_val = float(agg_df[(agg_df['horizon']==h) & (agg_df['model']==b['model']) & (agg_df['window']==b['window'])]['rate_val_mean'].iloc[0]) if h in best_by_h else float(\"nan\")\n",
    "            rate_test = float(b['pred_CAI_rate'])\n",
    "            def pct(v: float) -> str:\n",
    "                return f\"{(v*100):.1f}%\" if np.isfinite(v) else \"—\"\n",
    "            lines.append(f\"Pred_CAI_rate (VAL/Teste): {pct(rate_val)} / {pct(rate_test)}\")\n",
    "            lines.append(f\"Confusão (Teste): TP={int(b.get('TP',0))}  FP={int(b.get('FP',0))}  FN={int(b.get('FN',0))}  TN={int(b.get('TN',0))}\\n\")\n",
    "            # Checks\n",
    "            piso_ok = (b['precisao_CAI'] >= floor)\n",
    "            cov_ok = (b['pred_CAI_rate'] >= coverage_min_rate) or (int(b['num_pred_CAI']) >= coverage_min_count)\n",
    "            flood_ok = (rate_val <= FLOOD_GUARD_MAX) and (rate_test <= FLOOD_GUARD_MAX)\n",
    "            sanity_ok = not (folds_df[(folds_df[\"horizon\"]==h)&(folds_df[\"model\"]==b['model'])&(folds_df[\"window\"]==b['window'])][\"reasons\"].str.contains(\"metric_swap_detected\")).any()\n",
    "            # Baselines precision comparison\n",
    "            bd = base_df[base_df[\"horizon\"]==h]\n",
    "            best_beats_all = True\n",
    "            if not bd.empty:\n",
    "                for _, row in bd.iterrows():\n",
    "                    if b['precisao_CAI'] < float(row['precision']) - 1e-12:\n",
    "                        best_beats_all = False\n",
    "                        break\n",
    "            lines.append(\"Checks:\")\n",
    "            lines.append(f\"{'✓' if piso_ok else '✗'} Piso de Precisão(CAI)\")\n",
    "            lines.append(f\"{'✓' if cov_ok else '✗'} Cobertura mínima\")\n",
    "            lines.append(f\"{'✓' if flood_ok else '✗'} Freio de inundação (VAL/Teste)\")\n",
    "            lines.append(f\"{'✓' if sanity_ok else '✗'} Sanity de métricas\")\n",
    "            lines.append(f\"{'✓' if best_beats_all else '!'} Baselines (precisão)\")\n",
    "            # Baselines resumo\n",
    "            lines.append(\"\\nBaselines — Precisão(CAI) / F1:\")\n",
    "            if bd.empty:\n",
    "                lines.append(\"• –\")\n",
    "            else:\n",
    "                order = [\"Sempre_NAO_CAI\",\"ProporcaoTreino>0.5\",\"SinalOntem\",\"Momentum_3d\"]\n",
    "                for base in order:\n",
    "                    row = bd[bd[\"baseline\"]==base]\n",
    "                    if not row.empty:\n",
    "                        pr = float(row.iloc[0][\"precision\"]); f1v = float(row.iloc[0][\"f1\"])\n",
    "                        lines.append(f\"• {base}: {pr:.2f} / {f1v:.2f}\")\n",
    "            # Decisão\n",
    "            apto = bool(piso_ok and cov_ok and flood_ok and sanity_ok and best_beats_all)\n",
    "            apto_map[h] = \"SIM\" if apto else \"NÃO\"\n",
    "            if not apto:\n",
    "                motivo = []\n",
    "                if not piso_ok: motivo.append(\"piso\")\n",
    "                if not cov_ok: motivo.append(\"cobertura\")\n",
    "                if not flood_ok: motivo.append(\"inundacao\")\n",
    "                if not sanity_ok: motivo.append(\"sanity\")\n",
    "                if not best_beats_all: motivo.append(\"baselines\")\n",
    "                lines.append(f\"\\nApto a operar?  NÃO\")\n",
    "                lines.append(f\"Motivo (se NÃO): {','.join(motivo)}\\n\")\n",
    "            else:\n",
    "                lines.append(f\"\\nApto a operar?  SIM\\n\")\n",
    "        else:\n",
    "            apto_map[h] = \"NÃO\"\n",
    "            lines.append(\"Vencedor: —\")\n",
    "            lines.append(\"Precisão(CAI): —   Recall(CAI): —   F1(CAI): —   ACC: —\")\n",
    "            lines.append(\"Pred_CAI_rate (VAL/Teste): — / —\")\n",
    "            lines.append(\"Confusão (Teste): TP=—  FP=—  FN=—  TN=—\\n\")\n",
    "            lines.append(\"Checks:\")\n",
    "            lines.append(\"✗ Piso de Precisão(CAI)\")\n",
    "            lines.append(\"✗ Cobertura mínima\")\n",
    "            lines.append(\"✗ Freio de inundação (VAL/Teste)\")\n",
    "            lines.append(\"✗ Sanity de métricas\")\n",
    "            lines.append(\"! Baselines (precisão)\")\n",
    "            lines.append(\"\\nBaselines — Precisão(CAI) / F1:\\n• –\")\n",
    "            lines.append(\"\\nApto a operar?  NÃO\\n\")\n",
    "\n",
    "    # Rodapé\n",
    "    seq_str = \", \".join(final_seq) if all_operable and final_seq else \"—\"\n",
    "    lines.append(f\"Resumo: D+1={apto_map.get(1,'NÃO')} | D+3={apto_map.get(3,'NÃO')} | D+5={apto_map.get(5,'NÃO')}   |  Sequência final (último TESTE): {seq_str}\")\n",
    "    alerts_added = False\n",
    "    if repeat_msgs or xgb_fit_kw_error:\n",
    "        lines.append(\"Alertas:\")\n",
    "        alerts_added = True\n",
    "        for m in repeat_msgs:\n",
    "            lines.append(f\"• {m}\")\n",
    "        if xgb_fit_kw_error:\n",
    "            lines.append(\"• repeat_error_stop:xgb_sklearn_fit — Ambiente tem múltiplos XGBoost? Limpar venv e reinstalar 3.0.5? [ ]Sim [ ]Não\")\n",
    "    if not alerts_added:\n",
    "        lines.append(\"Alertas: —\")\n",
    "    lines.append(\"SSOT: /home/wrm/BOLSA_2026/gold/IBOV_gold.parquet | Folds: 10 | Seeds: {}/{}/{} | {}\".format(SEED_NUMPY, SEED_TF, SEED_PY, TF_DEV_NOTE))\n",
    "    lines.append(\"Precision floors: D+1={:.2f}; D+3={:.2f}; D+5={:.2f} | N_min_preds_val=10 | flood_guard≤{}%\".format(\n",
    "        precision_floor['D+1'], precision_floor['D+3'], precision_floor['D+5'], int(FLOOD_GUARD_MAX*100)\n",
    "    ))\n",
    "\n",
    "    print(\"\\n\" + \"\\n\".join(lines))\n",
    "\n",
    "    # Checklist final\n",
    "    print(\"\\nCHECKLIST\")\n",
    "    print(f\"- xgboost_version detectada: {XGB_VERSION}\")\n",
    "    print(f\"- XGBoost API: sklearn.fit(early_stopping_rounds={xgb_early_stopping_rounds}) | device={xgb_device}{'('+xgb_gpu_name+')' if xgb_device=='cuda' else ''}\")\n",
    "    print(f\"- SSOT: {GOLD_PATH} (tier=GOLD)\")\n",
    "    print(f\"- Horizontes: {horizons} | Janelas: {windows} | Folds: {len(splits)}\")\n",
    "    print(f\"- precision_floor: D+1={precision_floor['D+1']:.2f}, D+3={precision_floor['D+3']:.2f}, D+5={precision_floor['D+5']:.2f}\")\n",
    "    if xgb_fit_kw_error:\n",
    "        print(\"- Pergunta: Ambiente tem múltiplos XGBoost? Limpar venv e reinstalar 3.0.5? [ ]Sim [ ]Não\")\n",
    "    print(f\"- dry_run=True (nenhum arquivo salvo)\")\n",
    "    print(f\"\\n[{now_ts()}] Fim — CAI vs NÃO CAI (dry_run={dry_run})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project: BOLSA_2026)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
